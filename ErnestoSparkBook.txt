CHAPTER 1: INTRODUCTION TO APACHE SPARK


Theory


This chapter is intended to provide a comprehensive introduction to Apache Spark which is the center of focus throughout this book. We will also go through Scala programming language in the upcoming chapters to interact with Spark. But before we begin with Spark, let’s have look at a brief introduction of Hadoop and compare it with Spark.
An Overview of BigData
Quick Introduction to Hadoop
Apache Hadoop is an open source distributed framework that allows storage and processing of large data (BigData) sets across cluster of commodity machines.  Hadoop overcomes the traditional limitations of storing and computing of data by distributing the data over cluster of commodity machines making it scalable and cost-effective.
The idea of Hadoop was originated when Google released a white paper about Google File System (GFS) - a computing model built by Google which was designed to provide efficient, reliable access to data using large clusters of commodity hardware. The model was then adopted by Doug Cutting and Mike Cafarella for their search engine called “Nutch”. Hadoop was then developed to support distribution for the Nutch search engine project by Doug Cutting and Mike Cafarella. Well, what does the name Hadoop mean? There is no significance for the name and it is not an acronym either. Hadoop is the name that Doug Cutting’s son gave to his yellow stuffed elephant. The name is very unique, easy to remember and sometimes funny. Not only does Hadoop have such name with no significance but also its sub-projects tend to have such names which are based on names of animals like Pig and for the same reason. They are unique, not used anywhere else and are easy to remember.
Why Hadoop?
Companies today have been realizing that there is lot of information in unstructured documents spread across the network. A lot of data is available in the form of spreadsheets, text files, e-mails, logs, PDF’s and other data formats that contain valuable information which can help discover new trends, designing new products, improving existing products, knowing customers better and what not. Data is increasing at an alarming rate beyond limits like never before and there are no signs of slowing down, at least in the near future. To deal with such data, we need a reliable and low cost tool to meaningfully process it. Therefore, we use Hadoop. Hadoop helps us process all this BigData which is present in variety of formats reliably, in a very less time in a flexible and cost effective way.
Let us see why Hadoop is so popular and what it has in store for you.
* Scalable: Hadoop is scalable, meaning; you can just start from a single node server and eventually increase more nodes as you need more storage and more computing power.
* Fault-Tolerant: Hadoop helps prevent loss of data. All the data which is stored in Hadoop Distributed File System is broken into blocks and stored with a default replication factor of 3. While processing data, if a node goes off, the process does not stop but continues as the data still exists in other nodes.
* Flexible: Hadoop does not require schema. Hadoop can process unstructured, semi-structured and structured data from any kind of source or even from multiple sources.
* Cost effective: Hadoop does not require expensive high end computing hardware. Hadoop works well with a cluster of commodity machines by parallel computing.


Quick Introduction to Hadoop Distributed File System


Hadoop Distributed File System (HDFS) is a File System which extends over a cluster of commodity machines rather than a single high end machine. HDFS is a distributed large scale storage component and is highly scalable. HDFS can accept node failures without losing data. HDFS is widely known for its reliability. Let us now check out why HDFS stands out of crowd when it comes to Distributed file systems. 


Reliable Data Storage
	HDFS is very much reliable when it comes to data storage. Whatever the data stored in HDFS is replicated by a default replication factor of 3. That means, even if a machine fails, the data will be still available in two other machines. 
	Cost Effective
	HDFS can be deployed on cluster of commodity hardware and can save you a lot of bucks. High end expensive hardware is not required by HDFS to function.
	Big Datasets
	HDFS is capable of storing Petabytes of data over a cluster of machines where a file can range from Gigabytes to Terabytes of size. HDFS is not designed to store huge number of small sized files as the file system meta data is stored in memory of NameNode.
	Streaming Data Access
	HDFS provides streaming access to data. HDFS is best suited for batch processing of data and not suitable for interactive processing. HDFS is not designed for applications which require low latency access to data such as OnLine Transaction Processing (OLTP).
	Simple Coherency Model
	HDFS is designed to write once and read many times access model for files. Appending the content to files is supported at the end but cannot by updated at arbitrary point and it is also not possible to have multiple writers. Files can only be written by a single writer.
	

Block Placement in HDFS
Hadoop is designed such a way that the first block replica is placed on the same node as of client and the second replica is placed on a different rack to that of first replica. Third replica is placed on a random node on the same rack as of second replica. If the replication factor is more, random nodes in the cluster are selected to place the replicas. If a client running outside the cluster stores a file, a random node (That isn’t busy) is picked to place the first replica. This way, if a node fails, the data is still available on other nodes of the cluster and if a rack fails, again, the data is still intact.
HDFS Architecture
HDFS is a Master and Slave architecture, in which the Master node controls and assigns jobs to all its slave nodes. The following terminologies are used to describe the Master and Slave nodes.


The Master Nodes in HDFS are:
* NameNode
* Secondary NameNode


The Slave Nodes in HDFS are:
* Data Nodes


These nodes are the core serving roles in HDFS architecture. Let us now look in detail about the roles of each Node and understand them better.


NameNode
	NameNode is a HDFS daemon which controls all the Data Nodes and handles all the File System operations such as creating a directory, creating a file or reading and writing a file. The NameNode is responsible for managing the File System namespace image. It holds the image in memory, representing how the File System looks like. It also maintains the meta data for all the blocks of files in the File System and also tracks the replication value, so it knows the locations of blocks stored on Data Nodes with in the cluster. But the meta data is not stored on to the disk and is every time recreated when it starts. NameNode stores all this information persistently on local disk in the form of namespace image and edit log. The NameNode is the single point of failure in the Hadoop cluster. If the NameNode fails, entire cluster fails.
	Data Nodes
	Data Nodes are the slave machines controlled by the NameNode, that actually does all the block operations. Data Nodes store and retrieve blocks when asked to do so by NameNode and periodically inform NameNode with the lists of blocks they store by sending heartbeats. Data Nodes replicate the data physically when instructed by the NameNode on where and how to replicate. 
	Secondary NameNode
	Secondary NameNode, as its name implies, is not exactly the Secondary NameNode. The secondary NameNode is not a high availability solution and does not automatically takeover the responsibilities of NameNode on failure. Its main role is to create checkpoint and take the backup of NameNode periodically. It is like a backup solution to the NameNode. The hardware specifications of secondary NameNode should be similar to that of NameNode. In case of NameNode’s failure, the secondary NameNode can be manually configured to work as a primary NameNode. This is not a high availability solution.
	

Introduction to MapReduce
MapReduce is a programming model for processing large amounts of data stored in distributed file systems such as HDFS. MapReduce is low level programming and thus programs are written in low level languages such as Java, Python, Ruby etc. Let us look at the architecture of MapReduce.
Architecture of MapReduce
MapReduce, similar to HDFS is master-slave architecture but instead of NameNode in HDFS, we have JobTracker and instead of DataNodes in HDFS, we have Task Trackers in MapReduce. Unlike NameNode and Data Nodes, the JobTracker and Task Trackers are not physical hardware components but Java programs running on their own JVM’s inside the machines.


The detail description of MapReduce daemons is as follows:
JobTracker
	JobTracker is the MapReduce daemon which is master to the Task Trackers. The role of JobTracker is to receive job requests from Hadoop clients and assign work to task trackers on Data Nodes. The JobTracker queries the location of data on Data Nodes and tries to assign task trackers on Data Nodes where the data is present locally, so as to achieve data locality. If the node where the data present locally is busy, Job Tracker assigns tasks to task tracker within the same rack.  If the task tracker fails for some reason, the JobTracker will assign the same task on another task tracker as the data is replicated across the cluster on other nodes too.
JobTracker is the single point of failure similar to that of NameNode. If JobTracker fails, all the task trackers fail and there will be no tasks running. So it is wiser to spend more for the machine which runs JobTracker so as to decrease the chances of failure. 
	Task Trackers
	Task Trackers are slaves to JobTracker which do the actual work. TaskTrackers accept tasks from JobTracker and perform the tasks. The task trackers send status or progress of the tasks to JobTracker in the form of heartbeats so that the JobTracker can know that the task trackers are performing as they should and they have not failed. The task trackers also send heartbeat messages to JobTracker about the free slots available with them for processing of tasks.
The failure of task tracker is not as serious as that of the JobTracker as the JobTracker can always assign the failed task to another task tracker.
	

Processing Data with MapReduce
MapReduce consists of two major phases through which the data is processed. The two Major MapReduce phases are:


* Map phase
* Reduce Phase


Both the phases have key-value pairs as input and output. The data types for the key-value pairs can be chosen by the developer and the developer has to specify a map function and a reducer function, which is the logic for processing the data in MapReduce. The map function serves as logic to map task and the reduce function serves as logic to reduce task.
Let us now look at the flow of data and the various stages the data is processed in MapReduce.
* The input to map task is in the form of a split. A split is a fixed chunk of data based on the inputFormat and should not be confused with HDFS block. Blocks belong to HDFS and splits belong to MapReduce. Block is the smallest size of data stored in HDFS where as a split is an input to map task. Optimal split size will be equal to the block size. A map task can process one input split at a time.


*  Map tasks processes the splits using the map function in parallel and produces an output. The output from map tasks is not stored in HDFS but on local disk because the map output is intermediate result and there is no need to save it on HDFS with replications. The map output is only saved in local disk until the reducer has produced the final result. If we have specified no reducers for the job, the map output will be the ultimate result and will be stored in HDFS with replications.


* The output from all the map tasks is merged, sorted and partitioned. Merging is the process where the data from all the map tasks is merged together. Sorting is the process where the map tasks output is sorted based on the key. Partitioning is process where the data is divided based on keys so that the values from all the keys should go to one reducer. Partitioning is useful when there are multiple reducers used.  This map tasks output will then be fed to reducer as input.


* The reducer then processes the map output using the reducer function and produces the desired result. Unfortunately, reducers cannot take the advantage of data locality and will be fetched across the network. Also the number of reducers is not set automatically unlike mappers as the number of mappers is automatically set based on number of splits. The output produced by reducers is the end result and will be stored in HDFS with replications.


Let us consider an example with few words and understand the concept of MapReduce in pictorial representation.
Consider a file which consists of search terms for a website with each search term in a separate line as shown below.


Input file    






For example, let us assume that each line is an input to different map tasks. Please note that, the above assumption is for better understanding of the concept and in reality each map task processes much more data. The input file is broken into splits so as to feed to the map task. The file is split line by line as each line corresponds to a search term will be sent to the mappers. The splits of the file are as shown below:


Input Splits    


  



  



  



  







These input splits are further broken down into individual words and then submitted to mapper in the form of key value pairs. The key will be the word and value will be 1 as shown below.
  

Mapper  
  







  





  





  











At this point, all the intermediate data from map tasks output goes through the shuffle and sort phase. All the relevant words are brought together in our example as shown below.


  

Shuffle & Sort  


  





  







  









  







  







When the intermediate data is processed through the shuffle and sort phase, it is time for reducer to produce the end result using the logic in reducer function. After shuffle and sort phase, the partitioner makes sure that the key value pairs with same keys go to same reducer if there are multiple reducers set.


The reducer will aggregate the input and produce the following output.








  

Reducer  


  



  



  



  



  







The output from the reducers is concatenated to have a single file which contains the end result. The end result is as shown below.


  

Result  














The result shows the occurrences of each word in a given file.


Finally let us conclude the quick introduction on HDFS by looking at 3V’s of Hadoop which summarizes what Hadoop is capable of.
3V’s of Hadoop
Hadoop can be better described with 3V’s. The 3V’s of Hadoop are as follows
* Volume: Hadoop is designed to process large amounts of data ranging from hundreds of Gigabytes to Petabytes. There are lots of Petabyte datasets available today and Exabyte datasets are not a distant dream. Hadoop excels while processing large amounts of data rather than small data.
* Velocity: Hadoop is designed to ingest data at higher speeds from multiple sources. Hadoop uses the distributed framework for parallel processing which in turn decreases the time taken to complete a job. Hadoop brings computation to data rather than bringing data to computation which makes network bandwidth a bottle neck.
* Variety: Hadoop can process data in structured, semi-structured and unstructured forms. There are no restrictions on schema. Unlike relational database management systems, Hadoop has the schema on read capability. There is no schema required while writing to HDFS. The schema can be parsed at read time.
The traditional computing models lack these features but this is where Hadoop excels providing you with more power to explore your data.
Now that we have had a quick introduction to Hadoop, let us shift our focus on the main topic of our discussion, Apache Spark. 
Introduction to Spark
What is Spark?
Apache Spark is an open source, fast and unified parallel large-scale data processing engine. It provides a framework for programming with distributed processing of large datasets at high speed. Spark supports most popular programming languages such as Java, Python, Scala and R. Spark is scalable, meaning, it can run on a single desktop machine or a laptop to a cluster of thousands of machines. Spark provides a set of inbuilt libraries which can be accessed to perform data analysis over a large dataset. However, if your requirement doesn’t get satisfied with the inbuilt libraries, you can write one or explore countless external libraries from open source communities on the internet.
Why Spark?
Why use Spark when we have Hadoop? Well, Spark excels as a unified platform for processing huge data at very high speeds for various data processing requirements (will be discussed later in this chapter). Also, Spark is an in-memory processing framework. Spark is considered as a successor of Apache Hadoop. Let us briefly discuss the advantages of Spark over Hadoop.


With the Hadoop ecosystem, we had various frameworks for various data processing requirements. As a developer, you would use MapReduce framework to analyze your data using any of the languages such as Java, C++, Python etc, but a data warehouse engineer who is a SQL expert, has to learn any of the aforementioned programming languages. To overcome this problem, a new framework which runs on the top of Hadoop called “Hive” was introduced. This was also a problem for ETL processing and so “Pig” was introduced. Similarly tools like “Giraph” and “Mahout” were introduced for Graphs processing and Machine Learning respectively. Moreover, Hadoop is only used for batch processing and there was no way to process data in real time. So, for this a new framework called “Storm” was integrated with Hadoop to work with streaming data. With so many frameworks, organizations had a tough time to maintain all the frameworks and track issues pertaining to them. Fortunately, all this would change with advent of Spark. As mentioned, Spark is a unified platform which provides all these frameworks as one package with four major components. 


Now, what actually does In-memory processing mean? Aren’t all the applications processed in memory only? Well, yes all the applications are processed in-memory and written back to disk when processing is done, but Spark can process data in-memory and also retain the data within the memory or write to disk. Let us understand this with a figure by comparing Spark with MapReduce.


















1(a) Data Processing with MapReduce
  
  
  
  







  
  
  
  
  
  
  
  

Read       Write           Read                 Write           Read           Write        Read        Write
  
  
  
  













In MapReduce, the data present in HDFS or any other Distributed file system is read by a MapReduce application and is processed in memory and then written back to disk after the job is complete. If the processed data is again needed for further processing, the data is again read from disk by a MapReduce application, processed in memory and then written back to disk. This process continues as per the requirement, as seen in the figure 1(a). The processes of reading and writing data from and to the disk increase the IO latency and so the overall job duration is increased. This is optimized in Spark as shown in the figure 1(b).


1(b) Data Processing with Spark


  
  
  
  

  
  
  

  
  



        Read  
  
  
  

                                                     Write








In Spark, the data is read from the disk, processed in-memory but instead of spilling it back to disk, Spark can retain the data within the memory for further processing. So, if the processed data is again required for further processing, the data is already present in the memory and the Spark application processes the data eliminating the IO latency and therefore the overall time to process the job is significantly reduced. With this, the processing speed when compared to MapReduce has been increased upto 100 times faster. The processed data from a Spark application can either be retained in memory or can be stored to the disk as per the requirement as shown in the figure 1(b).


The reasons, such as a unified platform for various data processing requirements and High Speed In-Memory processing, have gained worldwide popularity throughout the industry with almost all the major organizations using Spark for their data processing requirements.


Components of Spark
Now that we know why Spark is being used, let us dive in more and learn what Spark is made up of. Let us look at each of the major Spark’s components individually and know them in detail. The following figure 1(c) shows the components of Spark. 


1(c) Components of Spark
  
  
  
  









  





  
  
  
  



  
  
  
  











Let us look at brief explanation regarding these components so that we can better understand the Spark components.
Spark Core
	Spark Core, as the name suggests is the core component of Spark which has all the basic functionalities for processing large datasets. Some of its functionalities include managing memory, scheduling jobs, fault tolerance, using in-memory computation, referring datasets stored in storage systems etc. Spark Core includes a programming abstraction (API) called Resilient Distributed Datasets also known as RDDs, which is responsible for partitioning data across nodes on a cluster. With the help of these RDDs, the data can be transformed, collected and reduce things together. These RDD APIs can be referred by using any of the programming languages such as Scala, Python, Java and R as shown in the figure 1(c). We shall learn more about RDDs in the upcoming chapters. 
	Spark SQL








	The Spark SQL component provides the developer with an SQL like interface to work with huge structured data which is distributed over a cluster of nodes. Spark SQL works well with structured and semi structured data. Spark SQL can also work with data sources such as Apache Hive tables, Avro, JDBC, ORC, JSON and Parquet file formats. Spark SQL also allows developers to combine RDD APIs along with Spark SQL code in a single application. We shall learn more about Spark SQL in the upcoming chapters.
	Spark Streaming
	Spark Streaming component of spark deals with processing of real time data also known as Streaming data. The streaming data can be from fleet of web servers, sensors, IOT devices or any other sources which generate data. This enables Spark to ingest data as it is generated in realtime and perform data manipulation on that data. There are three major phases of Spark Streaming. They are Gathering, Processing and Data Storage. Spark Streaming is also fault tolerant and scalable. This book does not cover Spark Streaming.
	Spark MLlib
	Spark MLlib is short for Machine Learning libraries which provides Machine Learning for huge datasets. MLlib contains various Machine Learning algorithms such as Regression, Clustering, Classification and Collaborative Filtering. MLlib also contains lower level primitives such as generic gradient descent optimization algorithm. MLlib also uses the linear algebra package called Breeze for numerical computing. This book does not cover Spark MLlib.
	GraphX
	GraphX deals with processing of Graphs in very efficient and distributed manner. GraphX extends the RDD APIs which allows a developer to create a directed multigraph with properties attached to each vertex and edge. This book does not cover GraphX.
	Cluster Managers
	Spark is all about processing massive amounts of datasets by distributing them over a number of nodes and scaling the cluster as required. To efficiently perform this task a cluster manager is required and Spark has its own cluster manager called Standalone Scheduler. Spark can also be deployed using Hadoop YARN, Apache Mesos or Kubernetes as a cluster manager to schedule jobs and manage the resources of the cluster. We shall look into more about cluster managers in upcoming chapters.
	

Spark Data Storage
Spark supports major file systems such as HDFS, Amazon S3, Azure Blob etc. Spark supports the local file system for storing the data as well. However, using a distributed file system such as HDFS can leverage the power of Spark by distributing the datasets throughout the cluster. Spark is also capable of dealing with various file formats such as text, ORC, parquet etc which we shall be covering in detail in the upcoming chapters.
Various Spark Versions
Apache Spark is an open source Apache Software Foundation Project which follows semantic versioning guidelines with a few deviations. All the Spark releases are versioned as [MAJOR].[FEATURE].[MAINTENANCE] for example Apache Spark 1.2.1 Which means Spark has a major version of 1 with feature version as 2 and Maintenance version as 3. Please check the link in references section for more about versioning in Spark.
The major versions of Spark are considered as Apache Spark 1.0 and Apache Spark 2.0. However, we shall be using Apache Spark 2.4.1 throughout the book as this is the latest version of Apache Spark at the time of writing this book.






LAB EXERCISE


"There are no activities required for this lab"


































SUMMARY
Apache Spark is an open source, fast and unified parallel large-scale data processing engine. Spark provides a single platform for various data processing tasks such as BigData analytics, Streaming, Machine Learning and Graph processing. Spark is upto 100 times faster than MapReduce due to its In-Memory processing and lazy execution of tasks.
Apache Spark has its own in-built cluster manager called “Standalone Scheduler” or can be deployed on “Hadoop YARN”, “Apache Mesos” or “Kubernette”.






























REFERENCES


* http://spark.apache.org/
* http://spark.apache.org/docs/latest/
* http://hadoop.apache.org/
* http://spark.apache.org/versioning-policy.html




















































CHAPTER 2: PROGRAMMING WITH SCALA
Theory


Before we dig deeper into Spark, let us take a detour and look into the programming language called Scala, using which we shall be writing all our Spark code in. This chapter covers most of the Scala basics to get started with Spark programmatically. That includes knowing the Scala syntax, calling functions in Scala, using few data structures etc. Scala is the most popular language used to write Spark code but Spark also supports other programming languages such as Java, Python etc.


What is Scala?
Scala is short for Scalable Language. It is called scalable language because it is scalable as per the user requirement. Scala is a Functional as well as Object Oriented Open Source Programming language which runs on top of a Java Virtual Machine (JVM). Scala is, as said above, the most preferred programming language for Spark, as Spark itself is built with Scala. Since Scala runs on top of a JVM, we can access Java libraries within the Scala code and utilize the functionalities of Java without being limited by Scala itself.
The compiler used to compile the Scala code is scalac. The Scala compiler compiles the Scala code to a byte code which is understood by a JVM to process the code. This is similar to how Java compiles its Java code using javac compiler to a byte code to process it on a JVM.  
Why Scala?
Let us now briefly look at the advantages of Scala and know why it is a best fit with Spark.
Functional
	Scala is a functional programming language. Every function in Scala is a value. The functions in the functional programming are implemented in such a way that there are no side effects. Meaning, the functions only do what they are supposed to do. They take  arguments, process the logic and return results. Scala supports higher order functions which takes a function within a function. The syntax for functions in Scala is lightweight. Scala supports anonymous functions, function currying and nesting of functions. Scala has in-built support for pattern matching model algebraic types used in many functional programming languages. Scala’s case classes have classes that can be instantiated so that they can automatically generate several methods. Scala also provides Singleton objects (which is a class that has only one instance) to group functions that aren’t members of a class.
	Object-Oriented
	Scala is also a pure object-oriented programming language where every value is an object and every operation is a method call. Object in Scala is instance of a class. Type and behaviour of the object are described by classes and Traits. Traits are similar to that of interfaces in Java. However, traits can also have method implementations as well as fields.
	Immutability
	Variables in Scala can be declared as immutable using the ‘val’ keyword. Meaning, once a variable is declared, it cannot be modified. Collections in Scala are immutable by default, unlike Java, which are mutable by default.
	Type Inference
	Scala is smart enough to infer the data type. Meaning, we need not explicitly declare the data type of a variable while declaring or returning it in a function. Scala can determine the data type of the return type of a function based on the last expression within the function.
	Pattern Matching
	Pattern matching is a core feature of Scala. Pattern matching is a process of checking for a specific pattern or a value in given sequence of data. Pattern matching is similar to that of Switch in C and Java. In Scala, pattern matching is achieved by using the match method in a root class, and then a number of cases as an argument using the case keyword to perform a pattern match.
	Interoperable
	Scala supports interoperability with Java. We can embed Java code within the Scala code and run its code without any issues. Scala code is compiled to a bytecode to run it on a Java Virtual Machine.
	Data Types in Scala
Like every other language, Scala also has data types and are very similar to that of Java. The following are the data types in Scala.
Data Type
	Size
	Int
	4 Bytes (32 bits)
	Long
	8 Bytes
	Float
	4 Bytes
	Double
	8 Bytes
	Char
	1 bit
	Boolean
	1 bit
	String
	Dynamic
	Unit (Similar to void in Java)
	--
	Functions in Scala
Scala is a functional programming language and so functions play a crucial role. The syntax to define a function is as follows:


def function_name(parameters: type): return_type = {


// expressions


}


* The function in Scala is declared by using the def keyword.


* Next, the name of the function should be specified in lower camel case.


* Following the name of the function is the parameters or arguments and their type for the function separated by a colon. The parameters and their types should be enclosed in parentheses. There can be multiple parameters and each parameter should be separated by a comma.


* The return type of the function can be specified after specifying the parameters. However, the return type is optional to specify, as Scala can infer the return type by looking at the last line of expression in function body. If the function doesn’t return anything, the keyword unit is used as return type. This is similar to that of void in Java.


* Finally, the return type is followed by an equals operator and then the body of the function is defined, enclosed in curly braces.


We shall look at functions in the lab exercises.
Collections in Scala
Collections are a group of variable number of heterogeneous data items or elements stored as a single object of similar type. The collections can be manipulated according to the requirement. All the operations that we usually perform on data such as searching, sorting, insertion, manipulations, deletion, etc. can be performed by collection.


There are two types of collections in Scala. They are


* Immutable collections
* Mutable collections


As the name suggests, the Immutable collections in Scala cannot be changed or modified once created, while the mutable collections can be modified after they are created.


Let us now look at a brief description of most common collections.


List
	List is the collection of elements with similar data types represented by a linked list data structure. A List can contain duplicate elements. Each element in the list can be accessed based on the position (index) starting from 0.
	Set
	Set is the collection of elements with similar data types but does not contain duplicate elements. Order of the elements inserted in a set is not maintained and therefore accessing the elements based on their position (index) is not possible in a Set. Hashing technique is used internally to store the values in the collection.
	Map
	Map is a collection of key-value pairs where keys are unique and a value is retrieved based on the referenced key. The values can also be accessed by the index of the key.
	Array
	Array is a collection of similar type of elements. Array in Scala is similar to that of Java. Array is a mutable collection in Scala and its index starts with 0. Scala also supports multidimensional arrays.
	Vector
	A Vector is an improvised version of a List which addresses the inefficiency for random access on lists. Vector shines where a list does not. A vector collection can have duplicates and the order of insertion of the elements is maintained.
	Tuples
	Tuple is a collection of items which can contain different types of elements. Unlike other collections, tuples’ index starts with 1. Tuples are immutable and cannot be modified once they are set. Tuples are frequently used in Spark.
	

Coding Scala
The Scala code can be written and executed in two ways. They are
* Scala Shell (REPL)
* Scala IDE


Scala Shell is probably the easiest way to getting started with writing the Scala code. Scala Shell is an interactive prompt to write Scala expressions and programs. The Scala Shell is sometimes also referred as REPL which is short for read–eval–print loop.
Another way to write Scala code efficiently is to use a Scala plugin with any of the popular IDEs (Integrated Development Environment) such as IntelliJ, Eclipse, Netbeans etc. We shall be using the IntelliJ IDE in all the upcoming chapters.
Conclusion
This concludes the theory part of Programming with Scala. Let us now proceed to labs to get our hands dirty and start programming with Scala.
Please note that this chapter is not designed to make you a Scala expert. This is only designed as an introduction to Scala covering basic topics and to get you started with Apache Spark. Please refer to the References section to learn more about Scala. 
































AIM


The aim of the following lab exercises is to install Scala, and perform various exercises by writing Scala code so that we can get our hands on Scala.
The labs for this chapter include the following exercises.
* Downloading and Installing JDK
* Downloading and Installing Scala
* Getting started with Scala basics
* Learning the Loops concept
* Getting familiar with Functions
* Getting Familiar with Collections


We need the following packages to perform the lab exercise: 
* Java Development Kit
* Scala
























LAB EXERCISE 1: PROGRAMMING WITH SCALA
  

Loops


Functions
Collections


	  


1. Download and install JDK
2. Download and install Scala
3. Scala Basics
4. Loops
5. Functions
6. Collections












  
  

Loops


Functions
Collections
Task 1: Download and Install JDK
Step 1: From the terminal, run the following commands to install JDK (Java Development Kit).


$ sudo apt-get update


This will update the package index. You might be asked to enter your password after you run the command above. 


Step 2: Once you run the above command, run the following command to actually download and install JDK.


$ sudo apt-get install default-jdk


  









  
  

Loops


Functions
Collections


The prompt will ask you to hit ‘Y’ after running the above command as shown in the screenshot. Hit ‘Y’ from your keyboard to continue with the installation and finally hit the Enter key. This will download and install JDK on your machine.


  



The installation process might take some time depending on your internet connection. Please allow it to download and install completely. You should see the following message when the installation is complete.


  



Step 3: Run the following command to check if Java has been installed successfully. The terminal should show the Java version as shown in the screenshot.
  
  

Loops


Functions
Collections




$ java –version


  



Task 1 is complete!


Task 2: Download and Install Scala
Now that we have installed Java, we are ready to install Scala and start writing some Scala code!


Step 1: Run the following command from the terminal to install Scala.


$ sudo apt-get install scala


  



The prompt will ask you to hit ‘Y’ after running the above command as shown in the screenshot. Hit ‘Y’ from your keyboard to continue with the installation and finally hit the Enter key.


Step 2: Verify your Scala installation version by running the following command.
  
  
  

Loops


Functions
Collections


$ scala -version


  



Step 3: After the installation is completed successfully, type scala in your terminal and you will be prompted with a Scala prompt.


$ scala


  



This completes the Scala installation. The scala prompt is the interactive shell where you can write and run Scala code. This interactive shell is also known as REPL.


Step 4: You can now start writing Scala code! Let’s start by printing the classic “Hello world!” from the shell. To do this simply type the following code and hit enter on your keyboard.


scala> println(“Hello World!”)


  



  
  
  

Loops


Functions
Collections


As you can see from the screenshot, the output is shown below immediately as soon as you hit enter.


Step 5: To quit the Scala REPL, you use the following command.


scala> :q


  



You are now back to the terminal prompt.


Task 2 is complete!
Task 3: Scala Basics
Let us now get familiar and start writing some Scala code to get acquainted with Scala syntax and basic programming constructs.


Step 1: Fire up your terminal and go into the Scala console by entering scala and hitting enter. Once you see the scala prompt, enter the following piece of code and hit enter


scala> val name: String = “Learning Voyage”




  
  
  
  

Loops


Functions
Collections


  



The above line of code is used to declare an immutable variable named name of type String which has a value of Learning Voyage. The keyword val is used to declare a variable which is immutable. When val is used, the created variable can no longer be changed or modified. Scala encourages to use immutability whenever possible. This will help you track your code easily and the values do not get modified accidentally when referring them programmatically. Unlike other languages, the name of the variable is declared first and then the data type is declared separated by a colon (:) in Scala. 


You can now use the variable name and use it inside the println function as shown below. This will print the value (String) associated with the variable (which is Learning Voyage in this case) to the console.


scala> println(name)


  



You can check if the declared variable is immutable by trying to append a new String to the name variable which we created previously as shown below.
  
  
  
  

Loops


Functions
Collections
scala> name = name + “ Inc”


  



As you can see from the screenshot above, it throws an error saying reassignment to val, which means you cannot reassign or modify an immutable variable.


Step 2: There might be scenarios where you would want to modify a variable. For that you can create a variable using var keyword instead of the val keyword as shown below.


scala> var newName: String = “Learning”


scala> newName = newName + “ Voyage”


scala> println(newName)


  

  
  
  
  

Loops


Functions
Collections


As you can see in the screenshot, we have concatenated a new string to the same variable and printed out the new string to the console using mutability. Mutability should be only used when it is absolutely required for your application.


Step 3: We can apply a workaround to use transformations on immutable objects to create new immutable objects and achieve the same result as we got using the mutable objects. 


scala> val name: String = “Learning”


scala> val newName: String = name + “ Voyage”


scala> println(newName)


  



As you can see from the screenshot above, we have applied the transformations on the immutable objects and achieve the same result as using the mutable objects.


Step 4: Let us now look into more data types as discussed in Data Types in Scala section and see how we can create them.


  
  
  
  

Loops


Functions
Collections


scala> val num: Int = 255


The above piece of code creates an immutable variable num of type Integer with a value of 255. Similarly, we can create immutable variables of all the other data types as shown below.


scala> val longNum: Long = 89416414


scala> val decimal: Double = 85.5545


scala> val decimalf: Float = 54.24f


scala> val letter: Char = ‘f’


Please note that there are only single quotes for Char type while there are double quotes for a String type.


scala> val lieDetector: Boolean = true






























  
  
  
  

Loops


Functions
Collections


  



Please note that the Data type name starts with a upper case letter unlike the other programming languages. However, you do not even have to specify the data type. Scala is smart enough to infer the type based on the value. You need not explicitly specify the data type while declaring a variable as shown below.


scala> val num = 256


  



  
  
  
  

Loops


Functions
Collections


As you can see from the screenshot above, Scala has automatically inferred the type of the variable as Int.


You can also do this for a String or any other data type as shown below.


scala> val name = “Learning Voyage”


scala> val decimal = 25.3545


  



Step 5: Let us now look at various ways we can print to the console. Using all the above variables we created in the previous step, let us concatenate them all in one string using ‘+’ symbol as shown below.


scala> println(“Printing to console using concatenation: ” + name + num + longNum + decimal + decimalf + letter + lieDetector)


  



It works but the output is not formatted correctly because we have not used spaces to separate the variables. We can add a white space as a string after each variable but it becomes a lengthy process if we have so many variables.


  
  
  
  

Loops


Functions
Collections


So, to overcome this we can substitute the variables within a string using an s prefix in the print statement before the double quotes as shown below. Each variable has a $ prefix.


scala> println(s“Printing to console using variable substitution: $name $num $longNum $decimal $decimalf $letter $lieDetector”)


  



Not only can we substitute the variables but we can also substitute expressions within the print statements enclosed in curly braces.


scala> println(s“Four divided by two is ${4/2}”)


  



Scala also supports the printf statements similar to that of Java. All we have to do is use an f prefix in the printf statement. The example of the same is as shown below.


scala> printf(f“Printing the value of a double with 2 decimal places $decimal%.2f”)


  



Task 3 is complete!
  
  
  
  
  

Loops


Functions
Collections


Task 4: Loops
Loops are an essential part of any programming language and it is no different with Scala. Let us now look at the loops concept and write some code to get familiar with them.


Step 1: Let us start the loops concept with the if loop. Fire up the Scala console if you haven’t already and type in the following code.


scala> val numOfKids = 3


scala> if (numOfKids > 2) println (“They are Phoebe Buffay’s kids.”) else println (“Parent unknown!”)


  



As you can see from the screenshot, the console only prints out the statement which is true based on the condition.


You can also write the if loop in the REPL in multiple lines using the paste mode as shown below. From the Scala prompt enter the following command and hit enter.


scala> :paste


This will take you to the paste mode with a prompt to enter your code as shown in the screenshot.


  
  
  
  
  

Loops


Functions
Collections


  



You can now enter Scala code in multiple lines. Once you are done with your code press Ctrl + D to come out of the paste mode and execute the code.


scala> :paste
//Entering paste mode (ctrl-D to finish)


val numOfKids = 3
if (numOfKids > 2) {
println(“They are Phoebe Buffay’s kids.”)
} else {
println(“Parent unknown!”)
}


  

  
  
  
  
  

Loops


Functions
Collections


The code is executed as soon as you have exited from the paste mode and result is displayed as shown in the screenshot above.


Step 2: Let us now look at for loops. Enter in to the paste mode and execute the following code.


scala> :paste
//Entering paste mode (ctrl-D to finish)


for (i <- 1 to 5) {
        val sum = i + i
        println(sum)
}


The syntax for the for loop is a bit unusual. We start the for loop with the for keyword and assign a range of 1 to 5 both inclusive to the variable i. The <- symbol is the range operator in Scala. Basically, it means that range of values between 1 and 5 are being assigned to the variable i  as a list of 1, 2, 3, 4, 5 and the loop is iterated through those values. Next we declare a variable called sum and add each value 1 through 5 with itself . Finally, we print the sum of values using the print line statement. The result is displayed with each value in a new line as shown in the screenshot below.




















  
  
  
  
  

Loops


Functions
Collections


  



If you do not want the last iteration to be included, you can use the keyword until instead of to. For example,


scala> :paste
//Entering paste mode (ctrl-D to finish)


for ( i <- 1 until 5) {
        val sum = i + i
        println(sum)
}














  
  
  
  
  

Loops


Functions
Collections


  

We can also use an if statement within the for loop as shown below.


scala> :paste
//Entering paste mode (ctrl-D to finish)


val friends = List(“Chandler”, “Monica”, “Rachel”, “Ross”, “Joey”, “Phoebe”)
for(friend <- friends if friend == “Chandler”){
println(s“The king of sarcasm is $friend”)
}


  

  
  
  
  
  

Loops


Functions
Collections


In the above example, we are looping through the list of collection called friends, with an if condition. We filter out all the items except for one element and substitute the variable in the print statement. Please see that we are using double equals operator to compare two strings.


Step 3: Let us now look at while and do while loops. The while construct is similar to that of other programming languages. However, in functional programming, the use of while loops is discouraged. 


Enter into the paste mode and execute the following code.




scala> :paste
//Entering paste mode (ctrl-D to finish)


var friends = 0


val names = List(“Chandler”, “Monica”, “Rachel”, “Phoebe”, “Ross”, “Joey”)


println(“The names of friends are:”)


while (friends < 6){


println(s“${names(friends)}”)


friends += 1


}






  
  
  
  
  

Loops


Functions
Collections


  



In the code above, we have first declared an Integer variable with a value of 6 and then a list of names of type String. Next, we print out a header so that the output makes sense and then write the While loop. The loop starts with a keyword while and then the condition inside the parentheses. The condition we set here is to continue the loop until value of friends is less than 6. Next, we use String interpolation to substitute the variables within the print statement. Please see that we have used curly braces, as we have substituted a variable named friends as a value to the variable  names. So that every time the loop runs, we are accessing each element of the list by its index starting from 0. Finally we increment the variable friends with 1.


The while loop runs every time the condition is satisfied and only comes out of the loop when the condition is false.


Now, enter into the paste mode again and execute the following code to perform a do while loop.
  
  
  
  
  

Loops


Functions
Collections


scala> :paste
//Entering paste mode (ctrl-D to finish)


var i = 0


do{


i += 1


println(i)


} while (i < 5)


  



  
  
  
  
  

Loops


Functions
Collections


The difference between while and do while loops is that any expression within the do clause runs at least once irrespective of the condition in while clause.


Step 4: Let us now look at pattern matching. Pattern matching is, as said before, a core feature in Scala. Pattern matching is similar to that of switch in other languages.


Enter into the paste mode and execute the following code.


scala> :paste
//Entering paste mode (ctrl-D to finish)


val job = “Transponster”


job match {
case “Masseuse” => println(“That’s Phoebe”)
case “Chef” => println(“That’s Monica”)
case “Executive” => println(“That’s Rachel”)
case “Transponster” => println(“That’s Chandler”)
case “Actor” => println (“That’s Joey”)
case “Paleontologist” => println(“That’s Ross”)
case _ => println(“Unknown job role”)
}
















  
  
  
  
  

Loops


Functions
Collections


  



In the code above, we have created a new variable called job which has a value of Transponster. We then use the match keyword to match the job variable with a list of cases. As you can see from the screenshot, the value of job is correctly matched and the output is printed. The last case statement which has an underscore (_) is a wild card operator. It is used so that, if none of the cases match, the default case is executed in the loop. Also, notice that there are no break clauses in Scala, similar to that of Java. Scala has in-built fall through mechanisms and so there are no break statements required.


Task 4 is complete!








  
  
  
  
  
  

Loops


Functions
Collections
Task 5: Functions
Task 1: Let us first create a function that does not have any parameters or a return type. Enter into the paste mode in Scala console and execute the following code.


scala> :paste
//Entering paste mode (ctrl-D to finish)


def hello = {
println(“Hello there!”)
}


Now, exit out of the paste mode and simply call this function by its name.


hello()


  





  
  
  
  
  
  

Loops


Functions
Collections


As you can see from the screenshot above, Scala has automatically inferred the return type as unit which means no return type. Unit is similar to that of Void in Java.


Step 2: Let us now create a function which takes parameters and has a return type.


scala> :paste
//Entering paste mode (ctrl-D to finish)


def married(name: String, times: Int): String = {
return name + “ has married ” + times + “ times”
}


Now, exit out of the paste mode and simply call this function by its name.


married(“Ross”, 3)


  



  
  
  
  
  
  

Loops


Functions
Collections


Please note that the return type (which is String, in this case) and also the keyword return are optional. Scala can determine the return type based on the last expression in the function body as shown below.


  



Step 3: Let us now look at Higher Order Functions. Higher Order functions are the functions which take other functions as parameters. Let us understand this with an example.


In the scala console, enter the following code.


scala> def squared (num: Int) : Int = {
num * num
}


You can call the function square to see if it works. For example,


Scala> squared(5)
  
  
  
  
  
  

Loops


Functions
Collections




  



Now, let us pass this function as a parameter to another function.


scala> def highSquared(num: Int, func: Int => Int): Int = {
        func(num)
        }


Here, we are defining a function named highSquared which takes two parameters and returns an Int. One of them is an integer named num and the another one is function named func which takes a parameter of type Int and returns an Int. In the function body, The function func takes the value of num and returns its value. Let us call this function.


scala> val result = highSquared(4, squared)


scala> println(result)














  
  
  
  
  
  

Loops


Functions
Collections


  



Step 4: We can also use the function literals in the parameters instead of using the name of the function as shown below.


scala> highSquared(6, x => x * x)


  



In the example above, we have used a function literal instead of referring name of another function. It basically does the same thing which we have seen in the previous step but in a different way. This is often used in Spark and it is very important that you understand what is going on here.


Task 5 is complete!








  
  
  
  
  
  
  

Loops


Functions
Collections
Task 6: Collections
Let us now look at few collections in Scala.


Step 1: Let us create a list and apply various transformations or operations over it.


scala> val shows: List[String] = List(“F.R.I.E.N.D.S”, “The Big Bang Theory”, “Game of Thrones”, “Breaking Bad”, “The Mentalist”)


Mentioning the type of list is optional as Scala can infer the type automatically. We can simply type “val shows = ” and continue with the list.


Let us now print the list.


scala> println(s“Some of the popular TV shows are: $shows”)


  



We can also access the individual items in the list using their index.


scala>println(s“The TV show at position 0 is ${shows(0)}”)


scala>println(s“The TV show at position 1 is ${shows(1)}”)


scala>println(s“The TV show at position 4 is ${shows(4)}”)
  
  
  
  
  
  
  

Loops


Functions
Collections


  



Let us now see how we can access the first and the rest of elements using head and tail.


scala>println(shows.head)


scala>println(shows.tail)


  



Let us now use foreach to print each element of the list.


scala> shows.foreach(println)














  
  
  
  
  
  
  

Loops


Functions
Collections


  



Let us now use the map function to convert each and every show to lower case.


scala> shows.map(show => show.toLowerCase)


  



These are a few transformations we can apply over a list. There are so many more transformations and operations you can apply on. From the Scala console, simply type the list of the name and the dot operator as shown below and press tab key on your keyboard. The console should display you a huge list of transformations you can apply on your list.


scala> shows.<press tab key>












  
  
  
  
  
  
  

Loops


Functions
Collections


  



If you are not sure what a function does, you can type its name after the dot operator and press tab twice. The console will show you what the function expects you to pass in. For example,


scala> shows.reduce<press tab key twice>


  





  
  
  
  
  
  
  

Loops


Functions
Collections


Play around with different transformations. Search over the internet if you do not understand any of them.


Step 2: Let us now look at map collection. Let us first create a map of elements.


scala> val couples = Map(“Chandler” -> “Monica”, “Ross” -> “Rachel”, “Phoebe” -> “Mike”)


Now that we have a map collection, let us try to access the value by the key.


scala> println(couples(“Chandler”)


  



As you can see from the screenshot above, we were able to access the value based on it’s key.


But if we try to access a value for a non existing key, an exception is thrown as shown in the screenshot. 


scala> println(couples(“Joey”)












  
  
  
  
  
  
  

Loops


Functions
Collections


  



To overcome this problem, we use the getOrElse method and specify a default value when the key does not exist.


scala> val unknown = util.Try(couples(“Joey”)) getOrElse “Not Known”




scala> println(unknown)


  



Play around with Map as you did with lists and explore all the transformations and operations you can perform on the map objects.


Step 3: Let us now create a tuple and see how we can access its elements.


scala> val showInfo = (1994, “Friends”, 8.8, 2011, “Game Of Thrones”, 9.4, 2010, “Sherlock”, 9.1)






  
  
  
  
  
  
  

Loops


Functions
Collections


  



As you can see from the screenshot above, a tuple can contain different types of elements. Also, you need not explicitly specify the data types for the elements. Scala can infer data types automatically.


Let us now access the elements of the tuple based on its index. Remember, the index of a tuple starts with 1 and NOT with 0.


scala> println(showInfo._1)


scala> println(showInfo._5)


  



We can also access the elements of a tuple and print it out to the console as shown below.


scala> println(s“${showInfo._5} is the highest rated show with “${showInfo._6} rating.”)
________________


  
  
  
  
  
  
  

Loops


Functions
Collections




  



As always, play around with tuples and practice as much as possible.


Task 6 is complete!


Note: This chapter does not cover everything from Scala. There are many other topics. Please check the references section to learn more about Scala.










































LAB CHALLENGE


* Write Scala code to perform basic mathematical operations such as additions, subtractions, multiplication and division.
* Write a Scala program to print all the data types using various print techniques.
* Write a Scala program to list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23.
* Write a Scala program to list Fibonacci sequence whose values do not exceed two million.




























SUMMARY
Scala is a Functional as well as Object Oriented Open Source Programming language which runs on top of a Java Virtual Machine (JVM). In this chapter we have seen why we need Scala and different features of Scala such as Functional Programming, Collections etc.
In the labs, we have installed JDK and Scala to our machines and learned Scala basics, loops, functions and collections.






























REFERENCES


* https://www.scala-lang.org/
* http://allaboutscala.com/
































CHAPTER 3 : HANDS ON SPARK
Theory
It is now time to get our hands on Spark. In the previous chapters we have covered the Introduction to Spark and learned the basics of Scala programming Language. In this chapter, let us take a deep dive into the internals of Spark and see how Spark works. Later in the labs, we shall be installing Spark on our machines and run Spark code.
Introduction to RDD
Resilient Distributed Dataset also known as RDD is the basic data structure of Spark, which is immutable and fault tolerant collection of elements that can be computed and stored in parallel over a cluster of machines. Let us look at each word individually and try to understand it in detail.


Resilient: The RDDs are fault tolerant to any data loss. Any loss in data due to hardware failure or data corruption can be recovered using the RDD lineage graph or DAG. 


Distributed: The RDDs can be distributed over a cluster of machines in memory.


Dataset: The RDDs can be created with any of the datasets such as a text file, JSON, CSV, Database file via JDBC etc.


An RDD can be created in two ways. We can use the parallelize keyword on an existing collection or referencing data in an external storage system such as HDFS, shared storage system, HBASE or any other data source offering a Hadoop InputFormat.


There are two operations which can be applied on an RDD. They are Transformations and Actions. Transformations is the operation where an RDD is transformed into one or more RDDs while actions are the operations where a result is returned after computing an RDD. 


We shall look at RDD in more detail in the next chapter. Let us now look at the architecture of Spark.


Architecture of Spark
As we know from chapter 1, Spark is a unified parallel large-scale data processing engine. To achieve parallelism for processing huge data efficiently and with fault tolerance, Spark framework was built. But how exactly does Spark work?


Let us understand the basics of data processing so that we would know the importance of processing data parallelly. Consider processing data in your personal computer. You have a text file of size ranging from a few MBs to few hundred MBs. You can easily load that file to your computer and do various operations such as searching, sorting or filtering without any issue. Now, consider a plethora of text files each ranging from few hundreds of MBs to a few hundreds of GBs or even a few hundreds of TBs. Let alone processing such data, it becomes harder to save those files in the first place. This becomes a bottleneck for your computer. You now have two options. You can either increase the specifications of your computer or process a percentage of files in multiple computers. The first option is not feasible as you cannot keep on upgrading hardware to a single computer. The second option looks feasible. But how will you coordinate the processing on each computer to be efficient and accurate? How do you ensure that the data processed on one computer is not processed on another? How do you collect the results and merge them accurately? What happens if a computer stops working due to a hardware problem?


Apache Spark is the answer for all those questions. Spark takes care of managing the group of computers (nodes) called cluster and coordinating the tasks on those nodes to process data.  Spark uses a cluster manager to manage a cluster. As seen in chapter 1, there are various cluster managers such as Spark’s standalone cluster manager, YARN, Mesos or Kubernette. The cluster manager takes care of managing and coordinating the Spark applications.


As we know, Spark is a master-slave architecture. There is one master node and several worker nodes. The master node is simply called the Driver Program as it contains the program or the application which we develop to process the data. The Driver Program consists of an object called Spark Session, which provides access to all the underlying hardware for the Spark job. You can think of Spark Session as a gateway to access all the Spark functionalities similar to a database connection. 


  

  

  

  



  



  

  
  
  
  

  
  
  
  
  





 






3(a) Spark Architecture


Prior to Spark 2.0, Spark Context was the entry point or a gateway for a Spark application. RDD was one of the main APIs of Spark which was accessed through Spark Context. Similarly, to access other APIs other contexts were required. Developers had to create different contexts such as SQL Context for Spark SQL, Hive Context for Hive queries and Streaming Context for Spark Streaming. So, a level of abstraction called Spark Session was introduced which includes all the APIs mentioned above in a single object without the necessity to create multiple contexts. We shall look at these APIs in detail in the upcoming chapters. 


The driver executes commands in the driver program while the cluster manager manages the resources on the cluster. As seen earlier, the cluster manager can be a Spark’s standalone cluster manager, YARN, Mesos or Kubernette.


The slaves are the worker nodes which are known as Executors in the Spark. As the name suggests, executors are the workhorses of Spark and are responsible to perform the tasks assigned to them by the Driver program. There can be any number of executors in a Spark cluster depending on the requirement. Since Spark is scalable, you can add more executors as your data increases. 


The Spark session is responsible to break the job and distribute the work to the executors while the cluster manager allocates resources for the Spark job on the executors. Once the executors finish processing the job, the cluster manager collects the processed data and reports it back to Spark.


Let us now look at a job workflow in Spark.
Job Workflow in Spark
The following is a workflow of a job in Spark.


* The job is submitted by a client to the Spark cluster. The driver takes the code and converts the code to a Lineage Graph. This is the logical execution plan.


* In this step, the Lineage Graph is converted to a physical execution plan or DAG where a bunch of tasks are created to run on the executors. All the optimizations to run the job are performed at this step.


* The driver then contacts the cluster manager to allocate resources to run these tasks. The cluster manager launches executors on behalf of the driver. The driver then starts the tasks to the allotted executors based on the data placement. The executors send heartbeat to the driver while executing the tasks.


* The driver monitors the progress of each task running on the executors. If a task fails on the executor, it is again restarted on another executor.


* Once the executors complete their tasks, the result is sent back to the driver through the cluster manager.


We have seen new terms in this chapter such as Directed Acyclic Graph (DAG) and the RDD lineage graph. Do not worry about them, as we will be looking at them in detail in the next chapter and they will make more sense then.


Let us now move to labs and install Spark and Intellij IDE to write Spark code.










































AIM


The aim of the following lab exercises is to install Spark and Intellij IDE. We shall also perform various exercises by writing Spark code so that we can get our hands on Spark.
The labs for this chapter include the following exercises.
* Downloading and Installing Spark
* Installing Spark on Multi-Node Cluster
* Creating RDDs from Spark-Shell
* Basic RDD Operations
* Downloading and Installing IntelliJ IDEA
* Configuring IntelliJ IDEA


We need the following packages to perform the lab exercise: 
* Java Development Kit
* Scala
* Spark
























LAB EXERCISE 2: HANDS ON SPARK
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


	  


1. Download and install Spark
2. Installing Spark on Multi-Node Cluster
3. Creating RDDs from Spark-Shell
4. Basic RDD Operations
5. Download and Install IntelliJ IDEA
6. Configuring IntelliJ IDEA










  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
Task 1: Download and Install Spark
Note: Java Development Kit (JDK) is a prerequisite to install Spark. Make sure you have installed it if you have not performed the Lab Exercise 1. Please follow the steps in task 1 of Lab Exercise 1 and then come back here to Install Spark.


Step 1: Verify if your machine has Java installed by executing the following command as shown below.


$ java –version


  



You will see the version displayed in the terminal as shown in the screenshot above. If you do not see the Java version displayed, your machine does not have Java installed. Please install JDK before continuing to next step.


Step 2: Let us install Spark in Standalone mode. Navigate to the download URL below, select the latest stable version for Spark (which is Spark 2.4.2 at the time of writing this book) and select the package type which is Pre-built for Apache Hadoop 2.7 and later. After selecting click the link as shown in the screenshot below to download Spark.


Download URL : http://spark.apache.org/downloads.html










  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
  



After clicking the download link, you will be taken to a page with mirror site to download Spark. Click the mirror link as shown below and your download should start. The download may take a while depending upon your internet connection.


  



Step 3: The download will be saved to the Downloads directory by default. Execute the following command to change the directory to Downloads folder.


$ cd Downloads


  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
Once you are in the Downloads directory, you may optionally check if Spark has been downloaded using the ls command.


$ ls


  



Now that you are sure that you have the Spark tar file, untar the Spark tar file to /usr/share directory using the command below.


$ sudo tar –xvf spark-2.4.2-bin-hadoop2.7.tgz –C /usr/share


  



The file will start to untar to /usr/share directory as shown in the screenshot above. You can verify the same by executing the following command below.


$ cd /usr/share


$ ls sp*


  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
  



As we can see from the screenshot above, the Spark directory is listed.


Let us make a softlink to the Spark directory so that we don’t have to refer Spark with entire name as above. This will also be useful for the future updates. Execute the following command.


$ ln –s spark-2.4.2-bin-hadoop2.7 spark


Run the following command again to check if we were able to create the softlink successfully.


  



Step 4: Let us now set up the environment variables for Spark. Execute the following command to do so.


$ sudo vi ~/.bashrc


  



The file should open as shown below.
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ




  



Now press i key to edit the file and append the following environment variable at the end of the file.


SPARK_HOME=/usr/share/spark
Export PATH=$SPARK_HOME/bin:$PATH


















  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



After you have finished appending the text above, hit the Esc on your keyboard to stop editing and then press Shift - Z - Z to exit out of the editor by saving the changes. (Please see that you need to press Z twice while holding Shift key.)


Now reload the modified .bashrc file using the following command.


$ source ~/.bashrc


  



Step 5: Let us now test the Spark installation by accessing the Spark Shell. To do so, execute the following command.


$ spark-shell


Once you run the above command, a screen similar to the screenshot shows confirming the successful installation of Spark.
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Task 1 is complete!
Task 2: Installing Spark on Multi-Node Cluster
In the previous task we have installed Spark on a single node. Let us now look at the steps for installing Spark on Multi-Node cluster. This task requires installing Spark on multiple nodes. You can use a cloud based platform such as Amazon AWS. For this task we shall be setting up a cluster of 3 nodes: 1 master and 2 slaves.


Step 1: Let us first install Spark on the master node. Edit the hosts file by executing the following command and adding the IP address of master and slave nodes.


$ sudo vi /etc/hosts


You will be shown an editor as shown below. 


  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Append the IP address of the master and the slave nodes to the hosts file.


<Master IP>        master
<Slave01 IP>        Slave01
<Slave02 IP>        Slave02


Please replace the <Master IP>, <Slave-1 IP> and <Slave-2 IP> with their respective IP addresses as shown below.


  



Save and close the file to come back to the console.


Step 2: Install JDK and Scala by following the steps in previous exercise. Only proceed to the next step after installing JDP and Scala.


Step 3: Let us now configure SSH so that the nodes can connect with each other. Execute the following command to Install Open SSH Server-Client.
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
$ sudo apt-get install openssh-server openssh-client 


  



Generate the key pairs by executing the command below.
$ ssh-keygen -t rsa -P ""


Next, set up passwordless SSH by copying the public key .ssh/id_rsa.pub from master to the .ssh/authorized_keys of Slave nodes.


Once copied, you should be able to SSH to slave nodes without being prompted for password.


Step 4: In this step, we shall download and install Spark. Please follow the steps in the previous task till step 4 to install Spark before continuing with this step. Once you are able to set up the environment variables, execute the following command to edit the spark-env.sh file. The file is present in $SPARK_HOME/conf/ directory.


$ cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh


  



$ sudo vi $SPARK_HOME/conf/spark-env.sh


Once you are in the editor, append the following parameters to the file.


export JAVA_HOME= /usr/lib/jvm/java-11-openjdk-amd64/bin
SPARK_WORKER_CORES=16
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
Please note that your Java path might be different. Please run the following command to know the Java directory path.


$ update-alternatives --list java


Step 5: Finally, create a file named slaves in the $SPARK_HOME/conf/ directory.


$ sudo vi $SPARK_HOME/conf/slaves


Now, add the following entries in the slaves file.


Slave01
Slave02


This completes the Spark installation on master node. We now have to install it on the slave nodes.


Step 6: Please repeat the steps 1 and 2 of this task on each slave node. Once you are done with the steps 1 and 2, create a tar file of Spark installation by executing the following commands.


$ tar -zcvf spark.tar.gz /usr/share/spark


The tar file will be saved to home directory when you run the command above. Copy the tar file from the master node to all the slaves on your cluster. 


After you have copied the tar file to slaves, untar the Spark tar file on all the slaves of the cluster by using the following command.


$ tar –xvf /usr/share/spark.tar.gz


  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


With this, you have successfully installed and configured Spark. You are now ready to start running Spark in distributed mode.


Step 7: You can now start the Spark daemons by running the following command from the master node.


$ /usr/share/spark/sbin/start-all.sh


You can also stop the Spark daemons by running the following command from the master node.


$ /usr/share/spark/sbin/stop-all.sh


You may also choose to start and stop master and slave nodes by running the following commands.


$ /usr/share/spark/sbin/start-master.sh
$ /usr/share/spark/sbin/start-slaves.sh


$ /usr/share/spark/sbin/stop-master.sh
$ /usr/share/spark/sbin/stop-slaves.sh


This task was created for information purposes only and you do not have to use the multi node cluster for the rest of the exercises.


Task 2 is complete!
Task 3: Creating RDDs from Spark-Shell
Now that we have Spark installed in our machines, let us begin coding with Spark. As a first step in our journey with Spark coding, let us look at the different ways to create an RDD. 
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


Step 1: Let us create a RDD using the parallelize keyword. Fire up the spark-shell from the terminal and create a list as shown below.


$ spark-shell


scala> val friends = List(“Chandler”, “Rachel”, “Phoebe”, “Joey”, “Ross”)


Now, let us use the parallelize keyword and create an RDD for the list we have created above.


scala> val friendsRDD = sc.parallelize(friends) 


  



You have created your first RDD successfully. Please note that we are using  sc which is the object of Spark Context. The sc object is automatically created when we launch the Spark Shell as shown in the screenshot below. This is the reason we are able to access sc and use Spark Context.


















  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Step 2: Let us now create an RDD using a file from the file system. We shall be using the textFile API to create an RDD from the file system. First, download the file ratings.csv from the URL below and save it to the home/chapter_3 folder. (Please create a folder named chapter_3 in the home folder.)


ratings.csv - http://bit.ly/2L8IEBS


Each line of this file represents one rating of one movie by one user, and has the following format: userId, movieId, rating, timestamp


scala> val ratings = sc.textFile(“chapter_3/ratings.csv”)


  



Step 3: We can now create a new RDD from the existing RDD. For example, let us count the number of ratings in the ratings RDD we created in the previous step.
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


scala> val count_ratings = ratings.count


  

As you can see from the screenshot above, the count of the total records (ratings) present in the RDD has been returned as a new RDD called count_ratings.


Step 4: We can also create an RDD from data present in Hadoop Distributed File System (HDFS) using the same textFile API. But instead of local path, we have to provide a HDFS path.


scala> val ratings = sc.textFile(“hdfs://dev_server:9000/file.txt”)


The RDD will be created using the data from HDFS and then you can continue to apply transformations and actions.


Task 3 is complete!
Task 4: Basic RDD operations


Step 1: Let us start learning the basic RDD operations in Spark by creating an RDD from a collection as done in the previous task.


scala> val letters = List(f, a, g, f, c, a, b, n, d, b)


scala> val lettersRDD = sc.parallelize(letters)


  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Step 2: You can access the first element of the RDD using the first method as shown below.


scala> lettersRDD.first


  



As you can see from the screenshot above, the first element in the RDD has been returned.


You can use the take(n) method to read the n elements from your RDD, where n is number of elements starting from the first element you want to read.


scala> lettersRDD.take(4)


  



  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


But if you want to view all the elements in the RDD, you have to use collect method as shown below. Please note that using collect on a large dataset is not recommended, as collect will bring all the data of an RDD to the Driver program and load it in its memory. 


scala> lettersRDD.collect


  



Step 3: Let us now use filter function using the contains method and filter out an RDD which satisfies the filter criteria. Create a new list as shown below and then filter out a string.


scala> val friends = (“Monica”, “Chandler”, “Ross”, “Phoebe”, “Rachel”, “Joey”)


scala> val friendsRDD = sc.parallelize(friends)


scala> val chandler = friendsRDD.filter(name=> name.contains(“Chandler”))


scala> chandler.collect














  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



The filter function we used above is a higher order function which takes another function as parameter and returns an RDD of type String. The name => name.contains(“chandler”) is similar to a function in Scala as shown below.


scala> def find(name: List[String]): Boolean = {
        name.contains(“Chandler”)
        }


Let’s call the function with the parameter friends which is a List of type String.


scala> find(friends)


  





  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


The code below 


friendsRDD.filter(name=> name.contains(“Chandler”))


is same as


def find(name: List[String]): Boolean = {
        name.contains(“Chandler”)
}
friendsRDD.filter(find)


Step 4: Let’s now use a map function on the friendsRDD and output a tuple with the first character in each element and the name itself.


scala> val pairs = friendsRDD.map(name => (name.charAt(0), name))


With this the pairs RDD gets created. Now let us use the foreach keyword to print each element of the pairs RDD.


scala> pairs.foreach(println)


  





  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


As you can see from the screenshot above, we have used the map function to create a tuple with first character of the name of each element and name itself in the friendsRDD. The first character is obtained by the function called charAt which takes the number to access the position of a character from a String.


If you think this is a bit complicated to understand, let us look at another example with a simple map function. Create a List of few numbers and create an RDD from that list as shown below.


scala> val num = List(1, 2, 3, 4)
scala> val numRDD = sc.parallelize(num)


Now let us write a map function which takes the numRDD and gives a squaredRDD as shown below.


scala> val squaredRDD = numRDD.map(x => x * x)


scala> squaredRDD.foreach(println)


  



  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
numRDD.map(x => x * x)
is same as


def square(x: Int): Int = {
        x * x
}
numRDD.map(square)


Step 5: For the numRDD we created in the previous step, let us use the reduce function to add all the numbers.
scala> val sumRDD = numRDD.reduce((a, b) => (a + b))
  

Similarly, we can also use the reduce function to multiply all the numbers in numRDD.


scala> val mulRDD = numRDD.reduce((a, b) => (a * b))
  



These are a few basic RDD operations. We shall look at more of these operations in detail in our next chapter which is entirely dedicated to RDDs. 


Task 4 is complete!


  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
Task 5: Download and Install IntelliJ IDEA
We have been so far writing our code in the Spark Shell. Spark Shell is great for developing applications with a small set of data. But in the real time, you will be using an integrated development environment also known as IDE for developing Software applications. 


We shall be using IntelliJ IDEA to write and run Spark-Scala code throughout the lab exercises. In this task we shall download and install IntelliJ IDEA and then install Scala plugin so that we can write Scala code.


Step 1:  Navigate to the following URL from your web browser and click on the “Download” button for Community edition as shown in the screenshot below.


http://bit.ly/2V1HFYO
  

The file should begin to download automatically to the Downloads folder. The download might take a while depending upon your internet connection.
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
Step 2: Once the download is complete, open the terminal and run the following commands to untar. We shall be extracting the tar ball to the /opt directory.


$ sudo tar –xzf Downloads/ideaIC-2019.1.1.tar.gz –C /opt


  



Please note that your version of IntelliJ might be different. Please replace the version correctly.


Step 3: Now run the following command to install IntelliJ.


$/opt/idea-IC-191.6707.61/bin/idea.sh


Please note that your path or the version might be different.


You should now see a prompt asking to import settings. Simply click “OK”.


  



Step 4: You should now be prompted with a Privacy Policy window. Click on the check box to accept the policy and click on “Continue” button as shown in the screenshot.


  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



In the next prompt, click on “Don’t Send”.


  



  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


Step 5: You will now be prompted to select a theme. Please select a theme you are comfortable with and click on the “Skip remaining and Set Defaults” button as shown in the screenshot.


  



You should now see the welcome screen as shown below.












  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



This concludes the installation of IntelliJ IDEA. But to run Spark Scala code, we need to install Scala plugin.


Step 6: Click on the “Configure” button as shown in the screenshot and click on “Plugins” in the dropdown menu.
















  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Step 7: The marketplace for plugins will be opened. Click on the “Install” button for “Scala” plugin as shown in the screenshot. If you do not see “Scala” right away, search for “Scala” in the search bar above and then click the “Install” button.
















  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



This action should begin the download. Once it is downloaded, you will be asked to Restart IDE. Please click on “Restart IDE” button. Click “Restart” in the conformation pop up.


  

  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


The IDE will now restart and show the welcome screen again. With this, you have successfully installed IntelliJ IDEA with Scala plugin.


Task 5 is complete!
Task 6: Configuring Intellij IDEA


Step 1: Click on the “Create New Project” button on the welcome screen.


  

  
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ
Step 2: You will then be taken to “New Project” screen. Click on Scala in the left panel, select “SBT” and then click “Next” as shown in the screenshot below.
  



Step 3: After clicking “Next” in the previous step, you will be taken to prompt to enter project name. Enter project name as “Spark”. Make sure the JDK, SBT and Scala versions are selected automatically as shown in the screenshot. Also, check the “Sources” checkbox for both SBT and Scala if not already checked. Finally click the Finish button.


  
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Step 4: You will now be taken to the IDE interface. Click on the “Project” as shown in the screenshot if not already open.














  
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



Expand the Spark project by clicking on the small triangle to the left of it, if not already expanded and double click on build.sbt file as shown in the screenshot.


  



Step 5: Now go to the Maven Repository for Spark using the URL below.
  
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


http://bit.ly/2UQvnNU


Click on the Spark Project Core link as shown in the screenshot.


  



Next, select the version of Spark you have installed. We have installed Spark 2.4.2 in this book and hence we will be clicking on 2.4.2 link for Spark. Please select the correct version as per your installation.


  



Now, select SBT tab and copy all the lines of code for SBT and paste it in the build.sbt file.


  
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



  



Go back to the Maven Repository page and copy paste the Spark Project SQL libraries as well in the build.sbt file. Finally, click on the “Import Changes” to finish configuration.


You can add new libraries if required by following the same procedure.






















  
  
  
  
  
  
  

RDD op.


IntelliJ D/l & Ins
Configure IntelliJ


  



You are now ready to write your first Spark program! 


Task 6 is complete!




















SUMMARY


Resilient Distributed Dataset also known as RDD is the basic data structure of Spark, which is immutable and fault tolerant collection of elements that can be computed in parallel over a cluster of machines.
Spark is a master-slave architecture. Spark consists of a Driver Program as the master and executors as slaves. A cluster manager is used to manage resources across the cluster.
In the labs, we have installed Spark to our machines and learned RDD basic operations. We have also installed and configured IntelliJ IDEA as IDE for our Spark code.
























REFERENCES


* https://spark.apache.org/






























CHAPTER 4 : INTERNALS OF SPARK
Theory
In the previous chapter, we have had a brief introduction to the Resilient Distributed Datasets (RDD). Let us now deep dive and learn more about RDDs which are the fundamental data structures of Spark as they provide the core abstraction. Also, let us look in detail what goes on under the hood when the Spark job is submitted.


Let us recall what we have learned in the previous chapter. What is an RDD? RDD is a collection of records (elements) which are distributed across the memory of nodes of a cluster and is tolerant to data loss. RDDs are immutable and cannot be modified once they are declared. RDDs are evaluated by Spark lazily. Below are the characteristics of RDD explained in detail.
Characteristics of RDD


Immutability
	As seen earlier, RDDs are immutable and cannot be changed once declared. However, an RDD can be transformed to a new RDD by applying operations.
	In-Memory
	RDDs are computed In-Memory where the data is retained within the memory instead of spilling the data to disk. The methods cache() and persist() are used to retain the data in memory. Please check “Why Spark” in chapter 1 for detailed information.
	Lazy
	RDDs are evaluated lazily. When a transformation is applied to an RDD, the transformations are not evaluated immediately but are only evaluated when an action is applied to the transformations.
	Tolerant
	RDDs are tolerant to data loss. Any data loss can be rebuilt using lineage graph. 
	Partitions
	RDDs are broken down into logical chunks called partitions and are distributed across the nodes in a cluster. Partitions help achieve parallelism.
	

RDD Operations
There are two types of operations that can be applied on RDDs. They are Transformations and Actions. Let us now look at them in detail.
RDD Transformations
Transformations are the operations where a new RDD is created from an existing RDD by applying a function. The existing RDD is not modified as it is immutable. So, a transformation is applied on an existing RDD and a new RDD is created. Transformations are lazy. A transformation applied to an RDD is not evaluated until an action is called. Each transformed RDD points a pointer to a parent RDD. All these pointers along with the RDD dependencies are logged in a graph called Lineage Graph. The Lineage Graph then creates a Logical Execution Plan. Whenever we perform a transformation or a series of transformations, they are not applied on the data right away but they are logged in the Lineage Graph as mentioned above. We shall look at Lineage Graph in the next section. The examples of transformation function are map, flatMap, filter, reduceByKey, join, union, sample etc.


There are two types of transformations which can be applied on an RDD. They are:


Narrow Transformations: The transformations where the data required to perform a transformation exists in a single partition and no data is shuffled from other partitions are called Narrow Transformations. The examples of Narrow Transformations are map, filter etc.


Wide Transformations: The transformations where the data required to perform a transformation has to be shuffled across various partitions are known as Wide Transformations.  The examples of Wide Transformations are groupByKey, reduceByKey etc.
RDD Actions
Actions are the operations where the actual processing of data happens. Actions are either used to display or store the final result to a location. The transformed RDDs are evaluated once an action is called, triggering the execution using lineage graph. The examples of actions include collect, count, countByValue, take, top etc.


The transformations and actions help improve efficiency by lazy evaluation.
Lineage Graph
From the previous section, we have learned that the transformations are applied to RDDs to create new RDDs which are not evaluated immediately, but lazily. When a transformation function is applied to an existing RDD, a new RDD is created with a pointer to the parent RDD. These pointers are logged to a Lineage Graph along with the dependencies of all the RDDs. Let us consider an example to understand this better.


* A new RDD is created using the textFile API. Let us call this RDD1.
* A flatmap function is applied on RDD1, creating a new RDD called RDD2.
* A filter function is applied on RDD2, creating a new RDD called RDD3.
* Finally, the result (RDD3) is stored using the saveAsTextFile API which is an action.


The first three operations above are the transformations which are not evaluated immediately, but lazily and the last operation is an action. The action triggers the evaluation of RDDs and the result is stored to the disk. The Lineage Graph for the operations above will look as shown below.




  

val RDD1 = sc.textFile(“file.csv”)
  

  

val RDD2 = RDD1.flatmap(line => line.split(“,”))
  

  

val RDD3 = RDD2.filter(x => x._2 == “new”)
  

  

RDD3.saveAsTextFile(“results/output”)




4(a) Lineage Graph


The Lineage Graph helps in recovering from data loss in partitions of an RDD. The Lineage Graph has to simply perform the transformations again for the lost partition of an RDD. Consider the example above. If RDD3 is lost, Spark can simply apply the transformation on RDD2 and recover it. This feature of Spark helps in being fault tolerant without having to replicate data on multiple nodes as HDFS. This is how Spark is tolerant to data loss.
Directed Acyclic Graph
Directed Acyclic Graph also known as DAG is a graph with vertices and edges connecting each other in a non circular manner. In simple words, DAG is a graph with dots and lines pointing to other dots in one direction where the lines do not form circles to go back to the same dots. An example of a DAG is a Tree Graph. Look at the fig 4(a) below to better understand DAG.






  



4(b) Directed Acyclic Graph




As you can see from the fig 4(a), each node (dots or circle with a number) points to another node in one direction (directed) and none of the nodes have pointers back to the same node (acyclic).


What does DAG have to do in the context of Spark? Well, Spark uses DAG to optimize and efficiently run Spark jobs. As we have learned, DAG is a graph with vertices and edges connecting each other in a non circular manner. In Spark, RDDs are the vertices and the operations which we apply on the RDDs are the edges. As we have seen in the Lineage Graph above, each operation is directed to another. In other words, each vertex directs an edge to another vertex. 


But how is a DAG different from Lineage Graph? A Lineage Graph is a logical execution plan while a DAG is physical execution plan. DAG consists of different stages so as to optimize the execution plan by reducing the data shuffling between the nodes. In Spark, the DAG Scheduler is responsible to implement the stages of DAG. The following steps show how the DAG Scheduler implements.


* Spark creates the Lineage Graph which is the logical execution plan when the code is submitted.


* The Lineage Graph is submitted to the DAG scheduler by Spark to prepare the physical execution plan.


* The DAG Scheduler splits the graph in stages. The stages are grouped together based on the transformations. All the narrow transformations are pipe-lined together as first stage and all the wide transformations which require shuffling are grouped as next stage.


* The stages are then passed on  to the Task Scheduler which launches tasks by contacting the cluster manager.


* The tasks are executed by the executors to produce final result.


Let us understand this concept with an example. Consider a sample Spark code where we load a file to create an RDD using textFile API and apply a map function to transform that RDD. Then we create a new RDD using parallelize. We now have two RDDs. We join them using the join function and filter the data using the filter function. Finally, the RDD is stored to the disk using the saveAsTextFile API.




  
  



  
  



  
  

  



  





  





  



  

  



  



  









4(c) Stages in DAG 


* The first two operations are narrow transformations which include reading the file using textFile API and map function. These operations do not require the data to be shuffled. So, this is considered as Stage 0. 


* The next stage is Stage 1 where we use parallelize to create a new RDD. 


* After creating the RDD, the next operations are wide transformations which include join function to join two RDDs. We are basically joining the two RDDs we created from textFile and parallelize. After joining we also perform a  filter  operation and save the file using saveAsTextFile API. These operations require the data to be shuffled across the cluster from various nodes. This stage will be called Stage 2.


Stage 0 and Stage 1 are executed in parallel as they are not dependent on each others outcome. Stage 1 can be processed without having to wait for the outcome of Stage 0. The data required to fulfill the operations in Stage 0 and Stage 1 is not shuffled from the other nodes and so the tasks within these stages are ran parallely. However, Stage 2 is dependent on outcome of both Stage 0 and Stage 1 and hence it is only executed after both these stages have provided the result. The data is shuffled from other nodes across the cluster to fulfill the join operation. Finally, the operations in sequence after join are executed one after the other in the same stage as they are dependent of the outcome from the operations prior to them and provide the final result.


This concludes theory for this chapter. 














































AIM


The aim of the following lab exercises is to start writing Spark code in Intellij and execute the code within IntelliJ and also look at the Spark web interface.
The labs for this chapter include the following exercises.
* Creating a new package in IntelliJ IDEA
* Spark Program – Loading Data
* Spark Program – Performing Operations
* Spark Program – Saving Data
* Spark Program – Lineage Graph
* Spark Web Interface


We need the following packages to perform the lab exercise: 
* Java Development Kit
* Scala
* Spark
























LAB EXERCISE 3: SPARK PROGRAM
  

Saving Data


Lineage Graph
Web Interface


	  


1. Creating a new package in IntelliJ IDEA
2. Spark Program – Loading Data
3. Spark Program – Performing Operations
4. Spark Program – Saving Data
5. Spark Program – Lineage Graph
6. Spark Web Interface












  
  

Saving Data


Lineage Graph
Web Interface
Task 1: Creating a new package in IntelliJ IDEA


Step 1: Open Intellij IDEA, which was installed in the previous exercise. If you cannot find it, click on the Apps menu on your desktop as shown in the screenshot.


  



Now, click on the IntelliJ IDEA icon as shown in the screenshot. You can also search for IntelliJ IDEA if you are unable to find it in the Apps menu. Once IntelliJ IDEA is loaded, right click on the icon which is present in the left task bar and click on “Add to Favorites”. This way, you can quickly access when you need it the next time instead of repeating all this process over and over again.




  
  

Saving Data


Lineage Graph
Web Interface


  



You should see an interface as shown in the screenshot below.






















  
  

Saving Data


Lineage Graph
Web Interface


  



Step 2: Expand the src folder by clicking on the caret. You should now see a main folder. Expand it as well and you should see a folder called scala.


  



  
  

Saving Data


Lineage Graph
Web Interface


Step 3: Now, right-click on scala folder and hover over New and click on Package as shown in the screenshot below.


  



You will be asked to enter a name for your package. You may enter any name as you like. We have named our package as training. After entering your name, click on OK button to finish creating a package. You should now see a package as below the scala folder as shown in the screenshot.


  



Task 1 is complete!
  
  
  

Saving Data


Lineage Graph
Web Interface
Task 2: Spark Program – Loading Data


Step 1: Let us now write Spark code in IntelliJ IDEA to count each word in a given file. Right-click the package which you have created in previous step and hover over New and then click on Scala Class.


  



Step 2: You will be prompted with a pop up asking you to enter the name of your Scala class. Please enter wordCount and then click on the dropdown for Kind and select Object as shown in the screenshot below.
  

  
  
  

Saving Data


Lineage Graph
Web Interface


Step 3: Click OK and you should see a new tab open on the right side and also with the Scala object in left panel as shown in the screenshot below.


  



You should see that the name of the package and the object should be prepopulated in the IDE editor.


Step 4: Now that we have editor to start writing the Spark code, the first thing we need to do is to include the import statements from Spark libraries. This program requires the following imports.


import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._


The first two import statements are used to import the Spark packages and last import statement is used to set logging level for our Spark application.


  
  
  

Saving Data


Lineage Graph
Web Interface


  



Please make sure you enter the import statements above the object line as shown in the screenshot above.


Step 5: Once we have the required imports, we need to write the main function similar to that of Java. This is the starting point for the compiler to execute our program. 


def main(args: Array[String]): Unit = {


The main function takes an argument args which is an Array of String type and does not return anything. Unit represents no return value similar to void in Java. It is optional to mention the return type.
















  
  
  

Saving Data


Lineage Graph
Web Interface


  



Step 6: Let us declare the level of logging to only log the error and fatal messages.


Logger.getLogger("Org").setLevel(Level.ERROR)


  

  
  
  

Saving Data


Lineage Graph
Web Interface


This step is not mandatory and you may skip this if you want all the logs.


Step 7: Let us now create a SparkContext object so that we can access all the spark functionality.


val sc = new SparkContext("local[*]", "WordCount")
We are creating an immutable variable called sc which contains the SparkContext object. Inside the SparkContext object the first parameter tells Spark if we would want the program to be executed in local or distributed mode. In our case, since we are working locally, we will be using local[*]. The [*] tells Spark to use all the CPU cores available locally in our machine. The next parameter is just the name of our app which is WordCount.


  



Please note that IntelliJ IDEA automatically prepends the names of the parameters based on what we enter.


Step 8: We now have a SparkContext object created. We can now use this object and load data using the textFile API as we have done in the Spark Shell. 
  
  
  

Saving Data


Lineage Graph
Web Interface


Download a text file from the URL below and save it in the path IdeaProjects/Spark/chapter_4/treasure_island.txt. Please create new directories as required. The IdeaProjects folder is present in your Home folder.


treasure_island.txt - http://bit.ly/2LBFLtt


Once you have the file downloaded and saved in the desired location, write the following line of code to load the file to create an RDD.


val data = sc.textFile(“chapter_4/treasure_island.txt”)


With this we have successfully created an RDD using the text file.


  



This completes the process of creating a SparkContext object and creating the first RDD by loading the data using the textFile API.


Task 2 is complete!
  
  
  
  

Saving Data


Lineage Graph
Web Interface
Task 3: Spark Program – Performing Operations


In the previous task, we have successfully created an RDD. Now let us use the RDD and apply operations to achieve our goal of counting number of words in a file.


Step 1: We have an RDD which contains text in lines. Let us split the lines to words using the flatMap function. The flatMap function is used to remove a level of nesting. The flatMap function is basically a combination map function and flatten function where the map function is applied first and then flatten function is applied.


val words = data.flatMap(line => line.split(" "))


The above piece of code splits each line into a separate word. The logic we apply to split the line is by a white space character. The flatMap function takes the data RDD and splits each line of word by a space character.


As a lab challenge, apply a map function to see how the output looks like instead of flatMap function. After you see the output from map function, apply flatten function to compare the result of flatMap function and the result of map and flatten function.


















  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Step 2: At this point, we have each word in a row. In order to count the occurrences of each word, we have to map each word to a key-value pair where the key is the word itself and the value will be number 1.


val wordskv = words.map(word => (word, 1))
























  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Here we use a map function to create a key value pair for each word where the key is the word and value is the literal number 1. With this operation we will end up having a tuple of word and the literal number 1.


Step 3: Now that we have tuples, all we need to do is add the values (literal number 1) for the same key. To achieve this, we use the reduceByKey function. As the name suggests, the reduceByKey function takes a key and applies operations to the values of that key.


val count = wordskv.reduceByKey((x,y) => x + y)


The above line of code takes the wordskv RDD and applies reduceByKey function to perform a sum of all the values for a key. This way we will end up with tuple where the first element is the word and the second element is the number of occurrences for that word.


  
  
  
  

Saving Data


Lineage Graph
Web Interface


The reduceByKey function is similar to reduce function which we have seen in the previous chapter. The difference is that the reduceByKey function performs reduce operation on values for a given key in a tuple while reduce function is applied for all the elements in the collection. 


  



Step 4: With the previous step all the transformations have been completed. Let us now perform an action to print out the result to console.


count.collect.foreach(println)
We can now simply use collect to collect the final RDD which is count and use foreach with println to print out each record in the RDD to the console. This will actually trigger the program to evaluate. All the transformations before this action are only logged in the Lineage graph to achieve lazy evaluation.








  
  
  
  

Saving Data


Lineage Graph
Web Interface
  



This completes our first ever Spark program. All we need to do now is to run it.


Step 5: To run this program within the IDE, simply click on the play button and Run program as shown in the screenshot. The program will then be compiled and executed.






















  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Once you click on the play button the IDE will start executing the program by first compiling and then display the result in the bottom. The execution might take some time based upon the hardware configuration of your machine. Once the program has been executed successfully you should see exit code: 0 as shown in the screenshot below. If it shows exit code: 1, you might have made a mistake somewhere in the program. Please go back, correct it and then run the program again.


  



To view the result, scroll up the console until you see text in white as shown in the screenshot. This is the output result showing each word in the text file with number of its occurrences.
  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Task 3 is complete!
Task 4: Spark Program – Saving Data


The output displayed in the console is useful while developing but in the real time we would want the output to be saved. 


Step 1: We shall be using the saveAsTextFile API to save the output of our program to the disk. Simply replace the collect statement from the previous task with this line of code.


count.saveAsTextFile(“chapter_4/word_count/output”)


We shall be saving the output to the following path IdeaProjects/Spark/chapter_4/word_count/output. You need not create the directories word_count and output. They will be automatically created. In fact the compiler will throw an error if the output directory is already present.




  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Step 2: Now run the program as you did in the previous task and check the output directory. You should see two files: part-00000 and a _SUCCESS file. The output is saved in part-00000 file.


  



Open the part-00000 file in a text editor and you should see the result as shown in the screenshot below.








  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Task 4 is complete!
Task 5: Spark Program – Lineage Graph


Let us now check the Lineage Graph for our Word Count program.


Step 1: To check the Lineage Graph for our Word Count program, we should use the toDebugString method. To do so, simply replace the saveAsTextFile line from previous task with the code below.


count.toDebugString.foreach(print)


Please note that we have used print inside foreach and not println.








  
  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Step 2: Run the program as you did before and you should see the output as shown below.


  



As you can see from the screenshot above, the toDebugString method displays the Lineage Graph. The indentations in the last four lines specify the shuffle boundary. Meaning, there was no shuffle of data for these operations: map, flatmap, teftFile. While the reduceByKey operation involves shuffling of data.




  
  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


The number inside the parenthesis is to denote the number of parallel tasks. In our case it is only 1. The higher number denotes a high level of parallelism.


Task 5 is complete!


Task 6: Spark Web Interface


We can access the Spark web interface to monitor the execution of Spark applications through a web browser. The web interface can be accessed by navigating to the following URL.


http://[driverHostname]:4040


The driverhostname is usually an IP address in the realtime environment and 4040 is the Spark’s port by default. 


Step 1: Open a terminal and start the spark-shell by entering the following command.


$ spark-shell


The Spark shell should show you that the web interface is available at the following URL as shown below.














  
  
  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Your driverHostname might be different. If a port is being used by another application, Spark will increase the port by 1 until an open port is found. For example, if 4040 is already taken, it will increase the port number to 4041.


Step 2: Now open your web browser and navigate to the web interface URL displayed in your Spark Shell. You should see the Spark web interface as shown in the screenshot below.


  

  
  
  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


Since there is no job running, you won’t be able to see any metrics.


Step 3: Let us run a job. Create a List of few numbers and create an RDD from that list as shown below.


scala> val num = List(1, 2, 3, 4)
scala> val numRDD = sc.parallelize(num)


Now let us write a map function which takes the numRDD and gives a squaredRDD as shown below.


scala> val squaredRDD = numRDD.map(x => x * x)


scala> squaredRDD.foreach(println)


After you see the output in the console, navigate back to the browser and refresh the Spark web interface. You should see a completed job as shown in the screenshot below.


  

Step 4: You can click on the collect link below the Description column and you will be taken to stages. Click on the collect link again to check more information as shown in the screenshot below.




  
  
  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


  



Step 5: Click on the DAG Visualization link to view the DAG.


  



  
  
  
  
  
  
  

Saving Data


Lineage Graph
Web Interface


Step 6: Click on the Executors link in the navigation bar to monitor the executors.


  



Task 6 is complete!
































SUMMARY


In this chapter we have looked at the Characterstics of RDD in detail. We have seen the types of operations we can apply on RDD. Also, we have seen the Spark architecture in detail by discussing the Lineage Graph and DAG.
In the labs, we wrote our first Spark program in Intellij IDEA and executed it. We have also looked at the Spark web Interface.
































REFERENCES


* https://spark.apache.org/
































CHAPTER 5 : RDD KEY-VALUE PAIRS & CACHING
Theory
In the previous chapter, we have seen what happens when a Spark job is executed internally and ran our first Spark job through IntelliJ IDEA. In this chapter, let us learn about RDDs of key-value pairs. 
Paired RDD
An RDD containing key value pairs is known as Paired RDD. Paired RDDs are the most common used data structure in Spark to perform aggregations. Paired RDDs are also beneficial during joins where two RDDs are joined based on the same key. We have already created a Paired RDD in Task 3 of previous lab exercise. We have used a map function to map each word to a key-value pair where the key is the word itself and the value is number 1. We have then applied reduceByKey function which can only be applied on a Paired RDD.


The simplest way to create a Paired RDD is by using the map function as seen in the previous lab exercise. All we need to do is map an RDD as a tuple and Spark will automatically consider the tuple as a key value RDD or paired RDD. Next, we can apply transformations to the key value pair RDD as per the requirement. A paired RDD can have a complicated data structure associated with it as well. You can use nesting within the tuple for a key and value. All it needs to have is a key and a value at the top level to be qualified as a Paired RDD.
Paired RDD Transformations
The functions which are used for a standard RDD can also be used for Paired RDD. However, since Paired RDDs are in the form of tuples, we need to apply functions which deal with a key value pair rather than individual elements. The following are the few transformation functions which can be applied on the paired RDDs. 


reduceByKey
	reduceByKey function is used to combine all the values of a given key using an associative function. The syntax for reduceByKey is rdd.reduceByKey((x,y) => x + y). The x and y in the syntax above do not represent a key value pair. Instead they represent two values of a given key on which the commutative and associative function is applied at a time. The result of the first two values of the key is then combined with the third value for the key and so on for all the values of a given key. The reduceByKey function is a wide transformation as it required shuffling of data across the nodes of a cluster.
	sortByKey
	sortByKey function is used to sort all the values of a given key. The syntax for sortByKey is rdd.sortByKey(). 
	groupByKey
	groupByKey function is similar to that of reduceByKey. It is used to group all the values of a given key in the form of an iterator. The syntax for groupByKey is rdd.groupBykey(). The groupByKey function is also a wide transformation as it required shuffling of data across the nodes of a cluster.
	mapValues
	mapValues function is used to transform all the values and keep the key as-is without changing. mapValues function is only applied on the values. Therefore, mapValues function is used to transform the values only and the key is not transformed. The syntax for mapValues function is rdd.mapValues().
	flatMapValues
	flatMapvalues function is similar to that of mapValues function but it also applies the flatMap function over the mapValues function. flatMapValues is combination of mapValues function and flatMap function. The flatMap function is useful to break down the collections into elements of the collection after mapValues is applied.  
	keys
	keys function is used to return a new RDD of all the keys from the paired RDD. The syntax for keys is rdd.keys.
	values
	values function is used to return a new RDD of all the values from the paired RDD. The syntax for values is rdd.values.
	

	

	

	

	Two Paired RDD Transformations 
Similar to that of single paired RDD transformations we can also perform transformations on two paired RDDs. An example of such two paired RDD transformation is join. The following are functions which can be performed with two paired RDDs.


join
	join is similar to that of SQL style join which is used to join two paired RDDs based on a common key. This join action performs an inner join on two paired RDDs. The syntax of join is rdd1.join(rdd2).
	rightOuterJoin
	Given two Paired RDDs, rdd1 and rdd2, the rightOuterJoin is used to join these two RDDs where the key is present in rdd1. The syntax of rightOuterjoin is rdd1.rightOuterjoin(rdd2).
	leftOuterJoin
	Given two Paired RDDs, rdd1 and rdd2, the leftOuterJoin is used to join these two RDDs where the key is present in rdd2. The syntax of leftOuterjoin is rdd1.leftOuterjoin(rdd2).
	subtractByKey
	The subtractByKey function is used to produce a new RDD by removing the elements which have the same key on both the paired RDDs. The syntax of subtractByKey is rdd1.subtractByKey(rdd2).
	cogroup
	The cogroup function is used to group data from both the paired RDDs which have the same key. The cogroup function is similar to that of Full Outer Join in SQL but instead of flattened result of each line per record, we get an iterable interface with cogroup. We can then either tokenize the records or perform other operations as per the requirement. The syntax for cogroup is rdd1.cogroup(rdd2).
	

Paired RDD Actions
In addition to transformations, we also have actions which can be applied on paired RDDs. Actions, as seen earlier, trigger the evaluation of a Spark job. All the regular actions which are applied on RDDs can be applied on the paired RDDs.  Let us look at the actions for paired RDDs.


countByKey
	The countByKey function gives back an RDD with  the count of values for each key. This function is very useful when you just need to count the values for each key in the paired RDD. The syntax for countByKey is rdd1.countByKey().
	collectAsMap
	The collectAsMap function is similar to collect function, but collectAsMap goes a step ahead to return the results for paired RDD as a Map collection. Since this will be a map collection, all the duplicate keys will be removed and the result will contain paired RDD with unique keys. Please be informed that when you use collectAsMap function similar to collect, all the data will be shuffled to the driver. Only use this when you know you have enough driver memory to accommodate all the data. The syntax for collectAsMap is rdd.collectAsMap().
	lookup(key)
	The lookup(key) function is used to look up values corresponding to a key. This provides an efficient way to get values based on the key. The syntax for lookup(key) is rdd.lookup(key).
	

RDD Caching and Persistence
RDD Caching and RDD Persistence play very important role in processing data with Spark. With caching and persistence, we will be able to store the RDD in-memory so that we do not have to recompute or evaluate the same RDD again, if required. This is an optimization technique which helps to complete jobs more quickly by saving the evaluation of RDD in memory. We have seen this characteristic of RDD in the previous chapter. 


Let us understand this better with an example. The default behavior is that an RDD is computed every time an action is called on the RDD. Look at the following piece of code below.


scala> val data = sc.textFile(“/some/path/records.txt”)


The above line simply loads a text file using the textFile API and stores it to an RDD called data.


scala> data.take(5)


The above line uses the take() function to return first five elements of the RDD. This is an action which triggers the evaluation. The RDD data is now computed by loading it from the file system and then the action is performed. 


Now, let’s say we need to count the elements in our RDD. 


scala> data.count()


We are now running a new action, which is causing the RDD to  compute again by loading it from the file system and then the action count is performed. As you can see, we are evaluating the RDD twice. This takes lot of time if the data is very big. To overcome this problem we have cache() and persist() method which can be cache or persist the RDD in memory or on the disk. 


The difference between cache() and persist() methods is that the cache() uses the default storage level of StorageLevel.MEMORY_ONLY, while the persist() method can have the combination of various storage levels as seen below.
Persistence Storage Levels


MEMORY_ONLY
	This is the default storage level. The RDD when cached is stored in memory only. If the RDD doesn’t fit in the memory, few partitions which do not fit are computed on the fly when an action is called. The RDDs are stored as deserialized Java objects.
	MEMORY_AND_DISK
	This storage level uses disk to store few partitions of RDD if they do not fit in the memory. So, instead of recomputing the RDD partitions which do not fit in memory, they are spilled to disk. The RDDs are stored as deserialized Java objects.
	MEMORY_ONLY_SER
	This storage level is same as MEMORY_ONLY but RDDs are stored as serialized Java objects. Serialization is more space efficient when compared to deserialized objects but is CPU intensive operation.
	MEMORY_AND_DISK_SER
	This storage level is same as MEMORY_AND_DISK but RDDs are stored as serialized Java objects in memory. The partitions which don’t fit in memory are spilled to disk.
	DISK_ONLY
	In this storage level, the RDDs are stored to the disk only and not in memory. This requires low space when compared to persisting in memory but is CPU intensive.
	MEMORY_ONLY_2, 
MEMORY_AND_DISK_2,
MEMORY_ONLY_SER_2,
MEMORY_AND_DISK_SER_2,
DISK_ONLY_2
	All these levels are same as above but store the RDD partitions with replication factor of 2. Meaning each partition is stored on two nodes of a cluster with replication.
	

Let’s go back to our example and see how we can use cache() and persist() methods.


scala> val data = sc.textFile(“/some/path/records.txt”)


Once we load the file using the TextFile API, we can now cache or persist the data RDD. Before we can cache or persist we have to import the following.


scala> import org.apache.spark.storage.StorageLevel


And then use the cache() method, if we need the default implementation of storage only.


scala> data.cache()


However, if we want to use the various storage levels as explained above, we have to use the persist() method and specify the desired storage level. So the code will look like:


scala> data.persist(StorageLevel.MEMORY_AND_DISK_SER)


At this point of time, we have simply specified out storage level for persistence. The actual persistence happens when the action on the RDD is called.


scala> data.take(5)


After this action is completed, the RDD is stored in the memory and any partitions that do not fit in memory are spilled to disk. When we trigger another action as below, the RDD will not be computed again as it is already computed and persisted. This will reduce the total time taken to complete the job without having to compute the same RDD over and over again.


scala> data.count()


It is also possible to remove the persisted RDDs manually. You simply have to use the unpersist() function to the RDD you want to unpersist.


scala> data.unpersist()


However, if you choose not to remove the persisted RDDs manually, Spark automatically removes the partitions based on Least Recently Used (LRU) cache policy, when there is too much data cached in memory. So, you need not worry about breaking a job when memory is full.


This concludes theory for this chapter.








AIM


The aim of the following lab exercises is to start writing Spark code in Intellij to learn about Paired RDDs.
The labs for this chapter include the following exercises.
* Creating a Tuple
* Creating a Paired RDD
* Performing Operations on Paired RDD
* Performing more Operations on Paired RDD
* Performing Joins on Paired RDD
* Performing Actions on Paired RDD


We need the following packages to perform the lab exercise: 
* Java Development Kit
* Scala
* Spark




















LAB EXERCISE 4: PAIRED RDD – HANDS ON
  

More Op.


Pair RDD Joins
Pair RDD Actions


	  


1. Creating a Tuple
2. Creating a Paired RDD
3. Performing Operations on Paired RDD
4. Performing more Operations on Paired RDD
5. Performing Joins on Paired RDD
6. Performing Actions on Paired RDD














  
  

More Op.


Pair RDD Joins
Pair RDD Actions
Let us start this exercise by executing a program which computes the average rating given by the user for a list of movies.
Task 1: Creating a Tuple


Step 1: Download the ratings.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


ratings.csv - http://bit.ly/2QmnAH9


Please save this file in IdeaProjects/Spark/chapter_5 folder. (Please create a folder named chapter_5 in the home folder.)


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter avgRatings as the name and click on the dropdown for Kind and select Object. Enter the imports as shown below.


import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._


  





  
  

More Op.


Pair RDD Joins
Pair RDD Actions
Step 3: Let us now write a function to parse the records and extract the fields we are interested in. For our program, we are only interested in userId and ratings fields. The function will split a line of input into a tuple of (userId, ratings).


Let us name the function parseRecords.


def parseRecords (rows: String): (Int, Float)={
This function takes each line of input as an argument and returns a tuple of an integer and a float.


  



Step 4: Now, using the split function, we split each field in the record by a comma as we know that each field in our record is separated by comma.


val records = rows.split(",")


  



  
  

More Op.


Pair RDD Joins
Pair RDD Actions
Step 5: Now that we have splitted the records, all we have to do is extract the required fields and convert the userId to integer and ratings to float types.


val userId = records(0).toInt
val ratings = records(2).toFloat


The records variable contains of 4 fields. We can simply access them based on the index starting from 0. So, we simply extract the userId which is the first field to the variable userId and ratings which is the third field to the variable ratings.


  



Step 6: Finally, we simply have to return the userId and ratings to complete our function.


(userId, ratings)


  

  
  

More Op.


Pair RDD Joins
Pair RDD Actions


This creates a tuple of userId and ratings. We can use this tuple to create our paired RDD in the next task.


Task 1 is complete!


Task 2: Creating a Paired RDD


Let us now write the main function for our program where we create our paired RDD and perform operations over the paired RDD.


Step 1: Write the following main function and error log level as shown below.


def main(args: Array[String]): Unit = {

 Logger.getLogger("Org").setLevel(Level.ERROR)


  





  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions
Step 2: Create a SparkContext object as seen in the previous exercise. Enter the master  as local to use all the cores and the name of the app as Average ratings by users.


val sc = new SparkContext("local[*]", "Friends By First Name")


Now that we have the SparkContext object created, let us load our file using the textFile API.


val data = sc.textFile(“chapter_5/ratings.csv”)


  

We now have an RDD loaded.


Step 3: Finally let us create our pairedRDD. To do this, we have to pass the parseRecords function as an argument to the map higher order function, so that all the records in the data RDD are parsed as per the logic in our parseRecords function. The following line of code does that work.


val RDDPair = data.map(parseRecords)


  



  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


That’s it. We now have our paired RDD. You can optionally take a look at the RDDPair by simply printing it out to the console using the code below.


 RDDPair.collect.foreach(println)


PS: Using collect is not recommended if your data is very big. When collect is used, all the data is shuffled to the driver node and if there is not enough memory available in the driver node, the job will throw a memory exception error.


Once you write this line of code, run the program by clicking the play icon and then clicking on the Run option as shown below. Make sure you have the appropriate closing flower braces at the end of the code.


  



You should have the output in the console with the key-value pairs as shown in the screenshot below.


  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



The first element in the tuple is the key (userId) and the second element in the tuple is a value (ratings).


Task 2 is complete!
Task 3: Performing Operations on Paired RDD


Before proceeding with this task, please comment out or remove the following statement from the previous task.


RDDPair.collect.foreach(println)


You can use comments in Scala using the characters ‘//’ preceding the comment as shown below. These kind of comments are called single line comments.












  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



You can also use multi-line comments by using characters ‘/*’ and ‘*/’ around the comment.


/*
This is a multi-line comment.
Which can span multiple lines.
Like this.
*/


Step 1: At this point, we have a paired RDD with userId as key and ratings as value. We now have to compute the sum of user ratings and divide them by the number of ratings so that we can get the average rating by an user.


To achieve this, we must first compute the number of ratings by a user using the mapValues function. 


val mappedRatings = RDDPair.mapValues(x => (x,1))


This transformation converts each rating value to a tuple of (ratings, 1). So we will be having our Paired RDD as (userId, (ratings, 1)).








  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



You may optionally print out the mappedRatings to the console as in the previous task, to check how the result is displayed.


Step 2: Next, we shall be using reduceByKey function to sum up all the ratings and all the instances for each userID, by adding together all the rating values and 1's respectively.


val totalRatings = mappedRatings.reduceByKey( (x,y) => (x._1 + y._1, x._2 + y._2))


The result will be in the form of (userId, (totalRatings, totalInstances))


  







  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


Step 3: Finally, we can now compute the average of the ratings by userId again by using the mapValues function. The average is calculated by dividing totalRatings by totalInstances.


val avgRatings = totalRatings.mapValues(x => x._1/x._2)


  



We now have the average rating by each user.


Step 4: We can now use the collect function to collect the final result from the RDD and display it on the console using the following line of code. This action triggers DAG and the job is executed.


avgRatings.collect.foreach(println)


  



  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


Step 5: Run the program by clicking on the play icon and Run option as shown in the screenshot below.


  



Once the job is finished, check the output in the console as shown in the screenshot below.


  



Step 6: We can also sort the result on either column using the sortBy function as shown below.


val sorted = avgRatings.sortBy(x => x._2)
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


The above line is used to sort the second field which is the value (Average rating) in the ascending order by default. The sorted result is as shown below.


  



However, if you want to sort it in descending order, you can simply use the dash (-) symbol as shown below.


val sorted = avgRatings.sortBy(x => -x._2)


You will have the results sorted in descending order when you run the program as shown below.














  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



You can also sort the result by the key by referring the first element in the sortBy function or simply using the sortByKey function key as shown below.


val sorted = avgRatings.sortByKey()


  

  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


Finally, you can sort the results in descending order when sortByKey is used by passing argument as false for the function.


val sorted = avgRatings.sortByKey(false)


Task 3 is complete!
Task 4: Performing more Operations on Paired RDD


In the previous tasks, we have only performed a couple operations on the Paired RDDs. Let us now look at even more operations that we can perform on Paired RDDs.


Step 1: Download the tags.csv file from the URL below. This file contains four columns: userId, movieID, tag and timestamp.


tags.csv - http://bit.ly/2YTVGFk


Please save this file in IdeaProjects/Spark/chapter_5 folder. (Please create a folder named chapter_5 in the home folder.)


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter tags as the name and click on the dropdown for Kind and select Object. Enter the required imports as shown in the screenshot below.












  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



Step 3: Write the recordsParser function as in the previous task. For this task, let us extract the movieID and tag fields. The recordsParser function is as shown below.


def parseRecords (rows: String): (Int, String)={
val records = rows.split(",")
val movieID = records(1).toInt
val tags = records(2).toString
(movieID, tags)
}






















  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



Step 4: Create a paired RDD as in Task 2 by writing the main function, setting error log level (optional), creating a SparkContext object and loading the file using the textFile API.


def main(args: Array[String]): Unit = {

 Logger.getLogger("Org").setLevel(Level.ERROR)


val sc = new SparkContext("local[*]", "Paired RDD Operations")


val data = sc.textFile(“chapter_5/tags.csv”)


Now create an RDD pair by parsing the data RDD using the recordsParser function.


val RDDPair = data.map(parseRecords)
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


We now have our paired RDD. Let us use some operations in the next step on our paired RDD.


  



Step 5: Now that we have our paired RDD, let us group all the tags by movieID using the groupByKey function.


val grouped = RDDPair.groupByKey()


  



Let us now print out the result of grouped RDD to the console.


  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions




grouped.collect.foreach(println)


The output is as shown in the screenshot below with all the tags for a movie  grouped together.


  



You may optionally convert the values from compactBuffer to a list by simply mapping the output and converting them to a List as shown below.


val flattened = grouped.map(x => (x._1, x._2.toList))


flattened.collect.foreach(println)






  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



Step 6: We can also extract the keys and values to separate RDDs as shown below.


val RDDKeys = flattened.keys


RDDKeys.collect.foreach(println)


  

  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


Similarly, we can extract the values using the code below.


val RDDValues = flattened.values


RDDValues.collect.foreach(println)


  



Task 4 is complete!




Task 5: Performing Joins on Paired RDDs


So far we have been working on single paired RDDs. In this task let us look at two paired RDDs by performing joins.


Step 1: Download the ratings.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


ratings.csv - http://bit.ly/2QmnAH9


Please save this file in IdeaProjects/Spark/chapter_5 folder if not saved already in the previous tasks.


Step 2: Download the movies.csv file from the URL below. This file contains three columns: movieID, movieName and genre.


movies.csv - http://bit.ly/2EJj0Os


Please save this file in IdeaProjects/Spark/chapter_5 folder. 


We shall join these datasets based on the movieID.


Step 3: Create a new object in the IDE and name it joins. Import all the required import statements as shown below. Next, let us declare a case class with fields according to the columns in both the files along with their data types.


import org.apache.spark._
import org.apache.log4j._

object joins {


case class ratings(userId: Int, movieID: Int, rating: Float, timestamp: String)


case class movies(movieID: Int, movieName: String, genre:String)








  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



Step 4: Now let us load both the files using textFile API and also split the fields by comma delimiter. We shall be using the map function to split the fields for every record in the RDDs.


def main(args: Array[String]): Unit = {

 Logger.getLogger("Org").setLevel(Level.ERROR)

val sc = new SparkContext("local[*]", "Joins")


val rating = sc.textFile("chapter_5/ratings.csv").map(x => x.split(','))
val movie = sc.textFile("chapter_5/movies.csv").map(x => x.split(','))


By splitting the fields, we can refer them individually while creating a paired RDD.


  



  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


Step 5: Now that we have both the datasets loaded and splitted by comma delimiter, let us create a tuple of two elements (Paired RDD) so that we can perform a join on both the RDDs based on key. The key here will be the movieID.


To create a tuple, we use the map function with the first element (key) of the tuple as movieID, which is second field in the rating RDD and first field in movie RDD. The second element (value) of the tuple will be the entire records from both the RDDs.


val rating_record = rating.map(x => (x(1).toInt, ratings(x(0).toInt, x(1).toInt, x(2).toFloat, x(3).toString)))

val movie_record = movie.map(x => (x(0).toInt, movies(x(0).toInt, x(1).toString, x(2).toString)))


  



Step 6: The next step is to simply perform the join as shown below.


val joined = rating_record.join(movie_record)


Now we can simply collect the results and print them to the console.


joined.collect.foreach(println)
  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



The output with the joined results can be seen as shown in the screenshot below.


You can then similarly perform the rest of the joins such as left outer join and right outer join.


It is recommended to perform joins using dataframes rather than RDDs as it can have the benefit of catalyst optimizer when performed using dataframes. We shall look at dataframes in our upcoming chapters.


Task 5 is complete!


Task 6: Performing Actions on Paired RDDs


Let us now perform actions on the paired RDDs and also look at caching and persisting RDDs. Also, let us continue the program from previous task and apply actions over them.


  
  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


Step 1: We shall be using the joined RDD for all the actions we perform in this task. Since we shall be using this RDD more than once, let us persist it before performing these actions so that the application does not process all the transformations over and over again whenever an action is called.


joined.persist(StorageLevel.MEMORY_AND_DISK_SER)


You might see an error about the missing import. If so, please add the following import to the list of imports.


import org.apache.spark.storage.StorageLevel


You may use your desired level of persist storage level above.


Step 2: Let us first use the countByKey function to check the number of ratings per movie on our joined RDD.


val count = joined.countByKey()


Let us print the count to the console using the println function and run the program. You should see the result as shown below with count of reviews for each movie shown as a Map collection.


println(count)
















  
  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


  



Step 3: Let us now use the collectAsMap function on the joined RDD.


val mappedCol = joined.collectAsMap()
println(mappedCol)


The result is returned as a Map collection with all the duplicate keys removed.


  



Step 4: Finally, let us use the lookup(key) function to lookup value for a key in our joined RDD.


  
  
  
  
  
  
  

More Op.


Pair RDD Joins
Pair RDD Actions


val look = joined.lookup(25)
println(mappedCol)
  



The result is shown as an ArrayBuffer for all the values of the key.


Task 6 is complete!


































LAB CHALLENGE


* As a continuation to Task 5, perform the other joins (Right Outer and Left Outer joins) which we have learned in the theory. 






















































SUMMARY


In this chapter we have looked at the RDD Key value pairs in detail. We have seen the types of operations we can apply on Paired RDDs. Also, we have seen the caching and persistence.
In the labs, we have created a paired RDD and applied transformations and operations on the paired RDD.


























REFERENCES


* https://spark.apache.org/
































CHAPTER 6 : SHARED VARIABLES
Theory


Spark provides two abstractions as low level APIs. We have been looking at RDDs, which are the low level Application Programming Interface (API) for Spark so far. In this chapter, we shall be looking at the Distributed Shared Variables which are also another low level abstraction for Spark. Let us learn what are these Distributed Shared Variables and why should we use them.
What are Shared Variables?
The Shared Variables in Spark are the variables which are used to perform operations in parallel. When a job is run in Spark, the functions of that job are run in parallel as tasks on different executors of the cluster. The variables used in the functions are sent to each task everytime the variable is required. Each task gets a copy of the variable in serialized form and changes made to these variables are not propagated back to the driver or to other tasks in executors processing them. This default behavior works well when the requirement doesn’t have to manipulate the variables or if the variables are shipped only once. However, this is not the case all the time and is inefficient to ship a copy of variable and deserialize it everytime it is required during the job, especially if the value is considerably large. 
Why Shared Variables?
As explained in the previous section, in order to overcome the inefficiency of shipping the variables every time to executors, and also to overcome the inability of propagating the updates of variables back to the driver or executors, we use Distributed Shared Variables.


There are two types of Distributed Shared Variables supported by Spark.


* Broadcast Variables
* Accumulators


Let us look at each of them in detail and try to understand them better with an example in the lab exercises as well.


Broadcast Variables
Broadcast variables are the shared variables, which allow Spark to send large values efficiently in an immutable (read-only) state to all the worker nodes. These variables can be used one or more times during Spark operations. The broadcast variables are sent to the worker nodes only once and are then cached to the worker nodes’ memory in deserialized form. These variables are very useful when the Spark job consists of multiple stages and multiple tasks in those stages require the same variable. Broadcast Variables overcome the inefficiency of shipping the variables every time to executors.


If not for broadcast variables, the required variable should be shipped to executors running the tasks everytime it is referenced, cache it in the executor’s memory and then deserialize it. All this accounts to an overhead and hampers the job performance. Instead of shipping the variable every time it is required during the multiple stages, it can be simply accessed any number of times without any overhead as it is available locally when Broadcast Variables are used. An example for use case of broadcast is a look up table or a feature vector in machine learning algorithm.


Using Broadcast Variables doesn’t mean that the data is not transmitted across the network. But unlike transmitting the data everytime the variable is referenced in the “regular” variables, the data is only transmitted once, saving network bandwidth and executor resources.
Optimizing Broadcast Variables
It is important to optimize Broadcast Variables with compact and fast data serialization when large values are used to broadcast. Not doing so will result in network bottlenecks, if the value takes too much time to serialize and transmit over the network. Using the correct serialization library will help improve the job performance in an efficient manner.


The default serialization library, Java Serialization, provides fast and compact serialization for almost everything except arrays of primitive types. You can use a different serialization library to optimize your broadcast using spark.serializer property, or you can implement your own user defined serialization as per the requirement.


We shall be looking at an example program using Broadcast variables in the lab exercise for this chapter.
Accumulators
Another kind of the distributed shared variables are Accumulators. Accumulators are most commonly used in error handling, i.e. to implement custom counters to count the events that occur during the job execution. Let us learn about Accumulators in detail.


As the name suggests, Accumulators are used to accumulate values to implement counters and aggregations. Since accumulation involves updating the values, Accumulators are mutable (writable), unlike Broadcast variables, can be updated by tasks running on different nodes and can be read by the driver program. Accumulators overcome the inability of propagating the updates of variables back to the driver. Spark has support for numeric type Accumulators by default, but you can also develop Accumulators for other types as well.


We can create an accumulator variable using the accumulator method provided by the SparkContext object. The accumulator method takes two arguments. The first argument is the initial value for the accumulator, and the second argument is the name you can specify for the accumulator which is displayed in Spark web UI. The second argument is optional and the method is also valid with only one argument, i.e., the initial value for your accumulator. The initial value can be incremented by tasks locally using the add method or += operator. The driver then globally increments all the accumulated values from tasks. The driver uses the value method to read the accumulated value.


Accumulators are efficient because no data is shuffled across the network. All the data is processed locally in the executor nodes by tasks. However, it is not recommended to use Accumulators when the locally accumulated data is large and does not fit the memory of the driver. The regular RDD transformations should be used in such cases.
Points to remember when Accumulators are used
* Accumulators should be used within the action operations only. If Accumulators are used within transformations, there is no guarantee that the final results are accurate. Since transformations are executed lazily whenever an action is called, the Accumulators get processed more than once and so the result would be incorrect.


* Accumulators are evaluated in the driver node since it is not associated with an RDD. The rule of thumb is that any code associated with RDD is evaluated in executors and the code which is not associated with an RDD is evaluated in the driver node.


We shall be looking at Accumulators in respect to Spark version 1.x and Spark version 2.x in the lab exercises to understand the concept better. We shall also look at creating a custom Accumulator.
Scala Monadic Collections
Now that we are looking at error handling in Spark, let us discuss the Monadic Collections in Scala which are used for error handling.


There are three Monadic collections in Scala which are used in error handling in functional programming, similar to try catch blocks in Java exception handling. However, in functional programming, we have to return something while try catch blocks in Java do not return anything. So, Monadic collections help you follow the rules of functional programming while implementing error handling.


Unlike the traditional collections such as List, Map, Set etc which can contain multiple objects, Monadic collections only contain two objects and can only return one object at a time. Let us look at these collections in detail.
Either Monadic Collection
The Either collection contains two objects called Left and Right object. The way how it works is, we can implement a condition in Either and return Left object in case of errors if the condition in Either is not satisfied, and return Right object in case of the condition in Either is satisfied. Left for errors and Right for no errors is just a convention. You can use vice-versa as well. We shall look at this collection in our lab exercise.
Option Monadic Collection
The Option Monadic Collection helps us deal with the NullPointerException. As explained earlier, Option Monadic Collection also contains two objects called Some and None. Let us create a case class which stores employee information.


scala> val welcome: Option[String] = Some(“Welcome to Learning Voyage”)


You can now retrieve the value using the get method as String is wrapped into an option.


scala> welcome.get


Similarly, you can also set the None object to the variable as shown below.


scala> val welcome: Option[String] = None


Since there is no value set for our variable, we can use getOrElse method to set a value on the fly as shown below.


scala> welcome.getOrElse(“Ernesto Lee Website”)


This is a very basic example of using Option collection. You can use it in a variety of cases like pattern matching, case class etc.
Try Monadic Collection
The Try Monadic Collection is similar to that of Either which contains two objects, namely Success and Failure objects. As the name suggests, the Success object returns the value if the condition in Try succeeds, else the Failure object is returned with the exception.


Please check the References section to learn more about Monadic Collections in Scala.


This concludes theory for this chapter.


















AIM


The aim of the following lab exercises is to start writing Spark code in Intellij to learn about Broadcast Variables and Accumulators in 1.x and 2.x as well as custom Accumulators.
The labs for this chapter include the following exercises.
* Using Accumulator Method
* Implementing Record Parser
* Implementing Counters
* Implementing Accumulators V2
* Implementing Custom Accumulators
* Using Broadcast variables


We need the following packages to perform the lab exercise: 
* Java Development Kit
* Scala
* Spark




















LAB EXERCISE 5: SHARED VARIABLES – HANDS ON
  

Accum V2


Custom Accum.
Broadcast Variables


	  


1. Using Accumulator Method
2. Implementing Record parser
3. Implementing Counters
4. Implementing Accumulators V2
5. Implementing Custom Accumulators
6. Using Broadcast Variables












  
  

Accum V2


Custom Accum.
Broadcast Variables


Let us start this exercise by looking at Accumulators.
Task 1: Using Accumulator method


In this task and the next two, we shall be looking at Accumulators API in Spark 1.x to count the number of malformed records and separate the malformed records from the good records.


Step 1: Download the ratings-malformed.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


Ratings-malformed.csv - http://bit.ly/2WuTese


Please save this file in IdeaProjects/Spark/chapter_6 folder. (Please create a folder named chapter_6 in the home folder.)


Please note that this file is different from the file we used in the previous lab exercises. This file has malformed records.


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter counters as the name and click on the dropdown for Kind and select Object. Enter the imports as shown below.


import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._


Step 3: Now write the main function along with the error log level setting as always.


  
  

Accum V2


Custom Accum.
Broadcast Variables


def main(args: Array[String]): Unit = {

 Logger.getLogger("Org").setLevel(Level.ERROR)


Also, create a SparkContext object and enter the master  as local to use all the cores and the name of the app as Counters.




val sc = new SparkContext("local[*]", "Counters")


Your code so far should look like the one in screenshot below.


  



Step 4: Now that we have the SparkContext object created, let us load our file using the textFile API.


val data = sc.textFile(“chapter_6/ratings-malformed.csv”)


  
  

Accum V2


Custom Accum.
Broadcast Variables


  



The aim of this task is to count the number of malformed records. But how do we decide which records are good and which are malformed? We need to have a look at our input file to answer this question. Open the file which you have downloaded in the step 1 for this task.


You will be able to see that the file contains four fields as explained in step 1. However, there are a bunch of records which are missing some fields as shown in the screenshot below.
































  
  

Accum V2


Custom Accum.
Broadcast Variables


  



In order to count and separate the good records from bad records we make use of Accumulators.


Step 5: The next step is to use the accumulator method in the SparkContext object and pass its arguments. The arguments are the initial value of zero (0) and the name of our accumulator as bad records.


val badRecords = sc.accumulator(0, "bad records")


You will see a warning “Symbol Accumulator is deprecated”. You may ignore this warning as this is older Accumulator API for Spark 1.x.


  
  

Accum V2


Custom Accum.
Broadcast Variables


With this we have successfully set the initial value for accumulator and named the counter as bad records. We now have to write a logic on how to determine good and bad records. We can achieve this by writing a recordParser object and defining a parse method within the object. Let us see that in the next task.


  



We shall come back to this program in Task 3, once we have figured out how to separate bad records from good records so that we can count the number of bad records.


Task 1 is complete!
Task 2: Implementing Record Parser


Step 1: To implement a record parser, we shall create yet another Scala object and name it recordParser. Please follow the steps associated with creating a new Scala object. No imports are required for this object.


You should end up with the object as shown below.


  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  





Step 2: Let us first create a case class to store all our good records as shown below.


case class records(userId: Int, movieId: Int, rating: Double, timeStamp: String)


  



Step 3: Let us now define a parse function which takes input record of type String as argument. The return type is an Either monadic collection which will either return String as Left object or a records case class as Right object.


Do not worry if you do not understand this as of now. All this makes sense when you look at the rest of the code.


def parse(record:String):Either[String, records]= {


  
  
  

Accum V2


Custom Accum.
Broadcast Variables


Next, let us declare an array variable of type String and name it fields, so that we can split the incoming records based on a comma. This way, we can access each field individually, and we can also know the number of fields each record has.


val fields: Array[String] = record.split(",")


  



Step 4: This is the step where we perform the error handling by using an if loop. The condition for if  will check if a record has 4 fields by using the length method.


If there are 4 fields in a record, we simply access each field based on its index and store it in the case class records using the Right object. If there are less than 4 fields, we pass the record as it is using the Left object.




















  
  
  

Accum V2


Custom Accum.
Broadcast Variables


if (fields.length == 4)
    {
     val UserId: Int = fields(0).toInt
     val movieId: Int = fields(1).toInt
     val rating: Double = fields(2).toDouble
     val timeStamp: String = fields(3)

        Right (records(userId: Int, movieId: Int, rating: Double, timeStamp: String))
   }
   else{
            Left(record)
   }
 }
}


Please make sure you correctly enter all the opening and closing flower brackets if you encounter an error.


  

  
  
  

Accum V2


Custom Accum.
Broadcast Variables


Let us now go back to the previous program counters and refer this object there.


Task 2 is complete!
Task 3: Implementing Counters


Step 1: Navigate back to the counters object in IDE and continue from where we left in Task 1.


Step 2: Let us now write a foreach action which parses each row through our recordParser object, which we have implemented in the previous task.


data.foreach(row => {
 val parsedRecords = recordParser.parse(row)
 if(parsedRecords.isLeft){
   badRecords += 1
 }
})


We then write an if condition to check if the record is a left object, i.e. a bad record. If it is, we simply increment the value of badRecords variable which we declared in task 1 by 1.


We can use the isLeft method to check if it is a Left object and isRight method for Right object.












  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Please see that we are using foreach function, which is an action and not a transformation. As learned in the theory section, Accumulators should always be specified in the action part and not in the transformations. This way we can be sure that our accumulator is only processed once and the value is accurate.


At this point, we have successfully implemented counters based on our requirement. All we need to do now is to simply get the final accumulated value.


Step 3: Let us use the println function to print the number of bad records in our input dataset.


println("The number of bad records in the input are  " + badRecords.value)
 }

}
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


To retrieve the value from our accumulator which is badRecords, we use the value  method. You cannot directly retrieve the value just by using the badRecords variable when it comes to Accumulators.


  



Step 4: Finally, let us run our code and check the output. You should see the number of bad records as shown in the screenshot below.


  



With this we have successfully implemented Accumulators using Spark 1.x API.


Step 5: Remember, we have also wrapped all the good records in the Right object. We can use it in our if condition with else object here as well.


  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


else {
 val goodRecords = parsedRecords.right.map(x => (x.userId, x.movieId, x.rating, x.timeStamp))
 goodRecords.foreach(println)
}


We are declaring a new variable called goodRecords and simply extracting (map) the fields from the Right object using the right method. Finally, we can print them out to the console in the next line.


  

Step 6: Let us now run it and see the good records as shown in the screenshot below.












  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



We have successfully implemented Accumulators and also separated good records from bad records.


Task 3 is complete!
Task 4: Implementing Accumulators V2


Please see that this is implementation of Accumulators in Spark 2.x. There will be some code related to dataframes, which we have not covered yet. But worry not. Just look at the implementation of Accumulators V2 and after we cover Dataframes, it all makes sense. 


Step 1: We shall be using the same file we used in the Task 1 for this task as well, since we are accumulating the bad records in the input data.




  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter countersV2 as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession


Since we are working on Spark 2.x, we will have to import a SparkSession object instead of a SparkContext object. The SparkContext object is wrapped within the SparkSession object. The SparkSession object will be used to read the data.


  



Step 3: Let us now write our main method and create a SparkSession object so that we can access Spark functionality. Please note that we haven’t covered this topic but just think of this as if we are creating a SparkContext object.


def main(args: Array[String]) {


val sparkSession = SparkSession.builder
.master("local[*]")
.appName("Bad record counter V2")
.getOrCreate()


  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


We have created a SparkSession object using the SparkSession.builder method, specified the execution environment as local using all the cores in our CPU and finally named our application as Bad record counter V2. The getOrCreate method gets an instance of SparkSession if it is already available or it creates one.


  





Step 4: Let us now declare our Accumulator object. The Accumulator object in Spark 2.x is a bit different from what we have seen in Spark 1.x. There are two types of Accumulators we can use in 2.x. They are the longAccumulator and doubleAccumulator. As their names suggest, a longAccumulator is used for Long data type and doubleAccumulator for Double data type. 


We shall be using longAccumulator in our code as we only need the count of type Long. 


val badRecords = sparkSession.sparkContext.longAccumulator("Bad Records")






  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



The longAccumulator object is wrapped in the sparkContext object which in turn is wrapped with the sparkSession object. The initial value for longAccumulator is set to zero (0) by default. We need not initiate it as we did in Spark 1.x API for Accumulator. All we need to do is set a name for our Accumulator. We have named it Bad Records. You are free to use any name as you like.


Step 5: The next step is to read the input data. Please note that this is a different API in Spark 2.x to read the data and we have not covered it yet. For now, just think of this as a way to read the input data. We will also need an import for implicits.


import sparkSession.implicits._


val options = Map("header" -> "false", "InferSchema" -> "true")


val data = sparkSession.read.text(“chapter_6/ratings-malformed.csv”).as[String]












  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 5: Now let us write a foreach statement to take each record from the input data, split the records by a comma and test it against a condition. If the records do not contain four fields, we increment the badRecords variable by one using the add method as shown below.


Since we know that our records contain 4 fields and if there are less than four fields in a record, we treat it as a bad record.


data.foreach(record => {
 val fields = record.split(",")

 if (fields.size != 4)
        badRecords.add(1)
})


  



  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables
Step 6: Let us now write a print statement to print out the number of bad records in our input data.


println("The number of bad records in the input are  " + badRecords.value)
 }
}


  



With this we have successfully implemented Accumulators for Spark 2.x.


Step 7: Finally, let us run our code. You should see the results as shown in the screenshot below.


  



Task 4 is complete!
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables
Task 5: Implementing Custom Accumulators V2


In the previous tasks, we have seen how to use the Spark’s built-in Accumulators. Let us now look at the steps to implement custom Accumulator in Spark 2.x.


Let us implement a Custom Accumulator which counts the number of times each movie has been rated. We can achieve the same using the map transformation and then applying reduceByKey action. But then this will have the data shuffled across the nodes of the cluster. However, when we use Accumulators, the data is not shuffled across the clusters as each executor processes data locally and has its own local accumulator. The only data shuffled across the cluster will be the count from each local accumulator, which will only be a few bytes. The local count from all the executors is aggregated by the global accumulator in the driver, thus providing the final result.


Step 1: Download the ratings_head.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


Ratings_head.csv - http://bit.ly/2X3r2wb


Please save this file in IdeaProjects/Spark/chapter_6 folder.


Step 2: Open IDE if not already, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter CountByMovie as the name of the Class. Please see that this is a class we are creating for this task and not an object. Enter the imports as shown below.


import org.apache.spark.util.AccumulatorV2
import scala.collection.mutable.HashMap
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


The first import is version two of Accumulator. The second import is a mutable HashMap as we will be storing our movies and total number of ratings as key and value respectively. We need to explicitly import the HashMap collection or else we would end up having an immutable HashMap when we declare one.


  



Step 3: We now have to extend our class to inherit AccumulatorV2 and then specify the input and output. The input to our Accumulator would be a tuple (movieId and count), processed by each task (local accumulator) on executors,  and the output is a HashMap which will be aggregated by the global accumulator on driver.


class CountByMovie extends  AccumulatorV2[(Int, Int), HashMap[Int, Int]]{


You may ignore the red error asking to implement merge method under the class name for now. This error will be gone once we implement all the methods in the code.








  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 4: Let us now declare a private HashMap variable called movieCount which will hold the final count of our CountByMovie Accumulator.


private val movieCount = new HashMap[Int, Int]()


We have to implement a reset method available in the AccumulatorV2 class to reset the accumulator value to zero.


def reset() = {
 movieCount.clear()
}


  



Step 5: We now have to implement the add method to specify the aggregation logic for local accumulators, i.e., the tasks which run on executors. All the tasks running on executors will run the method to aggregate data locally.
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


def add(tuple: (Int, Int)): Unit = {
 val movieId = tuple._1
 val updatedCount = tuple._2 + movieCount.getOrElse (movieId, 0)

 movieCount += ((movieId, updatedCount))
}


The add method takes two arguments as key and value. The key, which is the first argument, is the movieId, and the second argument, the count of the movieId, is value. We simply extract them into their respective variables and add them to the movieCount Hashmap. The getOrElse method is used to get the value of count, if it exists, or set a value for that movie as zero, and add them with the current count and previous count to get the updated count.


  



Step 6: The next step is to implement the merge method which actually aggregates all the values from executors and provides with the final count.




  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


def merge(tuples: AccumulatorV2[(Int, Int), HashMap[Int, Int]]): Unit = {

 tuples.value.foreach(add)

}


def value() =  movieCount




When all the tasks complete executing, the final results from all the executors are then sent to the driver where the merging happens. The merge method takes an AccumulatorV2 as an argument which takes a tuple and returns a HashMap as output. The merge method is called on all the local accumulators from the tasks processed in the executors. Therefore, we use the  add method inside the foreach function.


Since we declared the HashMap as private, we can only access it through the value method. The value method is used to simply get the current value in our accumulator.


To summarize, the merge method takes an accumulator as an argument and merges all the local accumulators, which were processed in the executors by tasks, based on the logic in add method, into the global accumulator. The value method is used to get the current value of the HashMap variable movieCount.
















  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 7: Next, there are a couple of methods required to complete our implementation of custom accumulator. They are the isZero method and the copy method.


def isZero(): Boolean = {
 movieCount.isEmpty
}


def copy() = new CountByMovie


The isZero method returns a Boolean by checking if the accumulator value is zero or not. The copy method is used to create a new copy of our accumulator object.


These are the abstract methods which must be implemented in our code as they are defined in the base class. These methods will be used while aggregating the value in the accumulator.








  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



The error for the class name should be gone now. With this we have successfully implemented our Accumulator V2. We now have to use this custom accumulator in our main program.


Step 8: Create a new object by right-clicking the training package which you have created in previous exercises and hover over New and then click on Scala Class. When prompted, enter countByMovieMain as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession


Let us first create a case class with all our fields for input data outside the object as shown in the screenshot below.


case class Movies(userId: Int, movieId: Int, rating : Double, timeStamp: String)










  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 9: Let us now write our main function and create a SparkSession object. 


def main(args: Array[String]) {

 val sparkSession = SparkSession.builder.
   master("local[*]")
   .appName("Count By movieId")
   .getOrCreate()


Next, let us create the Accumulator object and register it using the register method as shown below. We have to register our Accumulator since it is custom accumulator. You will not have to register for the built-in accumulators.


val countByMovie = new CountByMovie()
sparkSession.sparkContext.register(countByMovie)
















  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 10: Let us now write some code to read the input data. We will also need to import the implicits.


import sparkSession.implicits._


val options = Map("header" -> "true", "inferSchema" -> "true")


val data = sparkSession.read.format("com.databricks.spark.csv")
.options(cvsOptions)
.load(input)
.as[User]


Do not worry if this code doesn’t make sense. Just think this as a way to read the input data, as we used to do with SparkContext object in the previous exercises. Everything will start to make sense once we cover the next couple of chapters.


  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 11: Let us now apply our custom accumulator in the foreach higher order function and print the result to console.


data.foreach(record => {
     countByMovie.add((record.location, 1))
   })

   println(countByMovie.value.toList)

 }
}


  



  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


Here, we are passing our data through the foreach function, where our custom accumulator countByMovie is applied with the add method. We specify the movieId as the field for which the aggregations has to be done on. Finally, we can access the result by calling value method on our custom accumulator and convert it to a List.


Step 12: Let us now run the program. You should see the output with count for each movie in a List collection as shown in the screenshot below.


  



Please be careful while using accumulators. If the output generated from the accumulator is huge data, you should not use the accumulators. Instead, you should use the transformations as required. In this case, the result of accumulator is just movies and their counts. It is not a huge data. We have achieved our result without shuffling the data across the network, which is usually the case with transformations.


Task 5 is complete!












  
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables
Task 6: Using Broadcast Variables


Let us now look at another type of Distributed Shared Variable called the Broadcast variable. Let us use our movies dataset which we have been using throughout this course and find out the number of ratings for each movie. We shall be using the movies.csv file and ratings.csv file during this task. We shall broadcast the movies.csv file to look up with the movie Id in ratings.csv file.


Step 1: We will be needing two files for this lab exercise. Please download and save these files to IdeaProjects/Spark/chapter_6 folder, if not saved already.


ratings.csv - http://bit.ly/2QmnAH9


This file contains four columns: userId, movieID, rating and timestamp.


movies.csv - http://bit.ly/2EJj0Os


This file contains three columns: movieID, movieName and genre.


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter ratingsByMovies as the name and click on the dropdown for Kind and select Object. We will be needing the following imports for our Spark App.


import org.apache.spark.SparkContext
import scala.io.Source


The first import is as we know to create the SparkContext object. The second is Scala specific import which helps us read the movies.csv file.
  
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


Let us now define a function which would load the movie names to a Map object.


def loadMovieNames(): Map[Int, String] = {

 var movieNames: Map[Int, String] = Map()

 val data = Source.fromFile("chapter_6/movies.csv").getLines()
 for (record <- data) {
   val fields = record.split(',')
   if (fields.length > 1) {
     movieNames += (fields(0).toInt -> fields(1))
   }
 }
 movieNames
}


We are defining a function called loadMovieNames which does not take any arguments and returns a Map object which maps Int to String. We then declare a movieNames variable of type Map which maps Int to String and initialize it as an empty map.


Next, we load the data from our file using Source.fromFile method and call getlines method to get each line based on /n character. Next, we iterate through each record in our input data using the for comprehension, and split each field based on comma as we know our fields are delimited by a comma. Next, we check if each record has two fields and finally map movie Id with the movie name, by adding the fields to movieNames Map object. Finally, return the Map object as required by our function.








  
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 3: Let us now write our main function, create a SparkContext object and declare our broadcast variable.


def main(args: Array[String]): Unit = {


 val sc = new SparkContext("local[*]", "Ratings By movie ")

 val broadNames = sc.broadcast(loadMovieNames)


We can create a broadcast variable by simply calling the broadcast method on our SparkContext object. We then pass our loadMovieNames function as parameter to the broadcast method since loadMovieNames function returns a Map object, we will have the Map object broadcasted to all the nodes of the cluster.




  
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables


  



Step 4: Let us now load our ratings.csv file and create an RDD. Next, we split the records based on comma and extract the movieId field. We then use the map function to create a pairedRDD of movie Id and its count by  1, so that we can count the number of times each movie is rated in the next step.


val data = sc.textFile("chapter_6/ratings.csv")
val records = data.map(x => (x.split(",")(1).toInt, 1))




  

  
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables
Step 5: Now that we have a pairedRDD, we can use the reduceByKey function to count the number of times each movie has been rated as shown below.


val count = records.reduceByKey((x,y) => x + y)


Let us now sort the count, i.e. second field in descending order so that the highest number of rated movie will be on the top of our result.


val sorted = count.sortBy(-_._2)


  



Step 6: Finally, let us use our broadcast variable to look up the name of movie based on its movie Id. We use the ‘-‘ symbol to sort in descending order.


val sortedMoviesWithNames = sorted.map(x => (broadNames.value(x._1), x._2))


We are using the map higher order function to look up the value of the second field from the broadcast variable, which is the movie name, and the second field in our sorted RDD, which is the count. We use the value method to get the value in the broadcast variable. The compiler will look up the movie Id with its movie name, and provides us with the name of the movie as first field in the result and count as second field.
  
  
  
  
  
  
  

Accum V2


Custom Accum.
Broadcast Variables
Let us print out the result to the console.


sortedMoviesWithNames.collect.foreach(println)


  



Step 7: Let us now run our program and check the results. You should see the output as shown below.


  



Task 6 is complete!
SUMMARY


In this chapter we have looked at the Distributed Shared Variables in detail. We have learned about Accumulators, Accumulators V2, Custom Accumulators and Broadcast Variables. We have also learned about Scala’s Monadic collections for error handling.
In the labs, we have had our hands on Accumulators, Accumulators V2, Custom Accumulators and Broadcast Variables.


























REFERENCES


* https://spark.apache.org/
* https://spark.apache.org/docs/latest/api/java/org/apache/spark/util/AccumulatorV2.html
* https://dzone.com/articles/simplifying-monads-in-scala
































CHAPTER 7 : SPARK SQL
Theory


So far in this book, we have been working with RDDs, which are the low level APIs and basic data structures of Spark. Let us now look at Spark SQL and understand what it is and why it is used in Spark.
Types of Data
Before we dig into Spark SQL, let us first look at different types of big data available. The big data can be classified in three types.


Structured Data
	The structured data is the type of data which has a schema associated with it. All the attributes such as data type, data size etc related to the data are already available. This data is usually stored in tables with table names and their respective column headers. An example of unstructured data is Relational Database Management System, also known as RDBMS. The database systems include Oracle, MySQL, IBM DB2, Microsoft Access etc.
	Semi Structured Data
	Semi structured data is the type of data where the schema associated with that data is not completely available. This type of data does not qualify to be stored in a database as there is no datatype for the fields, but will still make sense with appropriate delimiters, which differentiate between lines and fields. An example of semi structured data is CSV, TSV, XML, JSON etc.
	Unstructured Data
	The Unstructured data is the type of data which does not have any kind of schema associated with it. The examples of unstructured data are presentations, word processing documents, audio and video files, webpages etc. Since there is no structure associated, it gets really hard to analyze such data. Artificial Intelligence and Machine Learning are used to analyze unstructured data.
	

Spark SQL treats structured and semi structured data as structured data. This is different behavior when compared to traditional database management systems like MySQL, Postgress etc.
What is Spark SQL?
Spark SQL is a library or module of Spark, which provides SQL style programming to process structured data. Spark SQL runs on top of Spark by wrapping all the Spark core APIs into a high level abstraction. Spark SQL also provides optimizations to run the jobs faster which lacks in Spark core, making Spark even more efficient. Since Spark SQL is syntactically similar to SQL, it is easier for developers, who already work on SQL, to become productive faster with less efforts. Spark SQL was implemented to overcome the disadvantages of running Apache Hive on top of Spark.
Why Spark SQL?
The following are the advantages of using Spark SQL.


* Spark SQL is popular because it provides developers with easy to use APIs with support to various data sources. Spark SQL provides interfaces for programming languages and query languages which include SQL and HiveQL, helping developers to get productive in no time.


* A wide variety of file formats such as csv, Avro, Json, Parquet, ORC etc are supported by Spark SQL. It also supports almost all the relational databases with JDBC connectivity, which include MySQL, Postgress, Oracle to name a few. NoSQL datastores such as Hbase, Cassandra, EasticSearch are also supported with Spark SQL.


* Spark SQL can also be easily integrated to other Spark libraries, which include Spark ML, graphX and Spark Streaming.


* Spark SQL efficiently processes structured data by advanced optimization techniques such as cost based optimizer, in-memory columnar caching, code generation and reduced disk IO. 
Spark SQL Architecture
The Spark SQL Architecture consists of the following components.


DataSource API
	The Data Source API is the universalAPI to load and store structured data. This is similar to the textFile, binaryFile, Sequence File APIs in Spark core (RDD). Instead of so many different APIs for different formats of data, we have Data Source API which can load and store structured data. Data Source API has built-in support for JDBC, Hive, Avro, JSON, Parquet, etc. Data Source API can automatically infer schema without the user explicitly mentioning the schema. We can also specify the schema using Data Source API.
	Data Frames
	Data Frames are like advanced version of RDDs. Data Frames are distributed collection of data represented in the form of rows and named columns. All the features of RDDs also apply to Data Frames. They are distributed, lazy, can be cached and are immutable. In other words, Data Frames are similar to that of tables in RDBMS but with more advanced capabilities. Since Data Frames are similar to that of RDBMS tables, we can simply run SQL like queries on our Data Frames, and have the data processed on our Spark cluster in distributed manner. 
	SQL Interpreter & Optimizer
	The queries on Data Frames are run in SQL which is a high level language. So, we need an SQL Interpreter or an SQL Parser which will interpret or parse our SQL queries.  The Optimizer in Spark SQL is called Catalyst. The Catalyst optimizer works on the SQL Data Structure trees and transforms the logical plan to an optimized plan, which inturn will be transformed to a physical plan. In simple terms, this component helps us process our big data in an efficient and optimized way.


Please check the link in References section to learn more about SQL’s Catalyst Optimizer.
	Spark SQL Thrift Server
	The Spark SQL Thrift server is used as an abstraction layer to connect Spark SQL with various Business Intelligence (BI) tools. The Spark Thrift Server is similar to that of Hive Thrift Server. So, instead of running queries from BI tools via Hive Thrift server as Map Reduce jobs, we can use the Spark Thrift Server and use Spark’s features. Since Spark is faster than Hadoop Map Reduce, we can have our queries processes faster in an efficient manner. The BI tools can be connected with Spark using the JDBC or ODBC drivers. For example, Tableau can be connected to Spark SQL using the ODBC driver and then run the queries from Tableau on Spark.
	Tungsten
	Tungsten is a Spark SQL component which helps in memory tuning and optimization of Spark jobs. Tungsten’s memory management is developed to address the drawbacks in JVM’s memory management. With Catalyst and Tungsten the Spark SQL jobs are much faster and efficient when compared to RDDs.
	

That’s all in theory for this chapter. Let us now proceed to our lab exercises and get our hands dirty with Spark SQL.
























AIM


The aim of the following lab exercises is to start writing Spark SQL code in Intellij to learn about Data Frames.
The labs for this chapter include the following exercises.
* Creating Data Frame using DataSource API
* Creating Data Frame from an RDD
* Creating Data Frame using StructType
* Querying data using Spark SQL
* Joins using Spark SQL
* Operations using DataFrame API


We need the following packages to perform the lab exercises: 
* Java Development Kit
* Scala
* Spark




















LAB EXERCISE 6: SPARK SQL – HANDS ON
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


	  


1. Creating Data Frame using DataSource API
2. Creating Data Frame from an RDD
3. Creating Data Frame using StructType
4. Querying Data using Spark SQL
5. Joins using Spark SQL
6. Operations using DataFrame API














  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Let us start this exercise by loading a file using Data Source API.
Task 1: Creating Data Frame using Data Source API


Step 1: Download the us-500.csv file from the URL below. This file contains twelve columns: first_name, last_name, company_name, address, city, country, state, zip, phone1, phone2, email and web.


us-500.csv - http://bit.ly/2LmgDW2


Please save this file in IdeaProjects/Spark/chapter_7 folder. (Please create a folder named chapter_7 in the home folder.)


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter users as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession


Since we are using SparkSession object to start our Spark Session, we need not import the SparkContext object as we did in the previous exercises. The SparkContext object is wrapped within the SparkSession object in Spark 2.x version.


  

  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


Step 3: Let us now write the main function for our program and create a SparkSession object as shown below.


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .appName("Users")
   .master("local[*]")
   .getOrCreate()


We are calling the builder method on Sparksession object to build a Spark Session. Next, the appName and master methods are used to specify the name of our app and mode of execution (local or cluster), as we used to while creating a SparkContext object. Finally, we use the getOrCreate method to get a SparkSession if there is one already or create a new SparkSession if it does not exist.


  



  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Step 4: Let us now load our file using the code as shown below.


val users = spark.read
 .format("csv")
 .options(Map("inferSchema" -> "true", "header" -> "true"))
 .load("chapter_7/us-500.csv")


We call the read method on our SparkSession object spark, which we created in the previous step, and specify the csv as format for our file using the format method. Next, we use a Map object to specify that our input file contains a header and also ask Spark SQL to infer schema. Since Map contains key value pairs, the keys are to specify properties and the values are true for both. Finally, we specify the path of our file using the load method. Please see that we use options method (plural) to specify more than one option using a Map object. If you only want to specify one option, you should use option method (singular). 


  

  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


Step 5: We now have successfully created a dataFrame named users. Let us now print it to console along with the schema.


   users.printSchema()

  users.show()
}


}


We call printSchema method to display the inferred schema and show method to display our dataFrame. Please note that when you use show method, only first 20 records in the dataFrame are shown. You can pass an integer for number of records in the show method. For example, to show 40 records you can use something like this users.show(40)


  



The show method is an action, and so this is the point where the DAG is actually executed. 


Step 6: Let us run this program and check the output. You should see the schema as shown below.




  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



As you can see, the schema been correctly discovered by Spark for each and every column in the dataFrame. Please note that if a column has values of more than one data type, Spark will infer it as String.
The output of dataFrame users is as shown below.


  







  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



As you can see from the screenshot above, the header is displayed correctly along with the records.


Step 7: We can also select only one column or more than one column from the dataFrame and have it shown using the code below.


users.select("last_name").show()


  



The output is as shown in the screenshot below.






  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
  



Selecting multiple columns…


users.select("first_name”, “last_name").show()


  

  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


We simply call the select method on users dataFrame and pass the required columns as arguments. Then we call the show method as usual.


We can load data to Spark using this same method for any format like json, parqet, ORC, Avro, etc. Please check the link in References section for more info.


Task 1 is complete!
Task 2: Creating Data Frame from an RDD


We can also create a Data Frame from an RDD. Let us see how to achieve this.


Step 1: Download the mlb_players.csv file from the URL below. This file contains six columns: name, team, position, height, weight, age.


mlb_players.csv - http://bit.ly/2JhzVJj


Please save this file in IdeaProjects/Spark/chapter_7 folder. 


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter rddToDf as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession










  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 3: Let us now create a case class so that we can define schema to our dataFrame. The names which we specify for attributes of case class object will get mapped as column names for our dataFrame. This will make sense when we run the program.


case class Players(player_name: String, team: String, position: String, height: Int, weight: Int, age: Double)


Step 4: Let us now write the main function for our program and create a SparkSession object as shown below.


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .appName("RDD to DataFrame")
   .master("local[*]")
   .getOrCreate()










  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 5: Since our aim is to convert an RDD to a DataFrame, we must use the textFile API in the SparkContext object to read the file and create an RDD.


val input = ss.sparkContext.textFile("chapter_7/mlb_players.csv")


We now have an RDD created. But the file contains a header with column names. We must first remove the header. We can achieve that by calling the first method on our RDD and then remove it using the filter method as shown below.


val header = input.first()
val records = input.filter(x => x != header)


The first line of code takes the first record from the RDD, which in our case is the header or column names and then we simply filter out the header from input RDD. Now, we just have the records RDD without the column names.














  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 6: The next step is to split the fields based on a comma so that we can assign each individual field to its appropriate case class field.


val fields = records.map(record => record.split(","))


Now that we can access individual fields by their position, let us assign them to the case class Players, using the map function as shown below.


val structRecords = fields.map(field => Players(field(0).trim, field(1).trim, field(2).trim, field(3).trim.toInt, field(4).trim.toInt, field(5).trim.toDouble))


We call trim method on all the fields to remove leading and trailing white spaces, and also cast height, weight and age fields to Int, Int and Double respectively.














  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 7: We now have our data in structured columns with named records. We can now simply convert it to a dataFrame using toDF method. 


But before we can use the toDF method, we need to import the implicits as shown below.


import ss.implicits._

val recordsDf = structRecords.toDF()


We now have our dataFrame recordsDf created from RDD.


  





  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


Step 8: Let us now call the show method on our dataFrame and run the program.


recordsDf.show()

 }

}


The output is as shown in the screenshot below.


  



Task 2 is complete!










  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Task 3: Creating Data Frame using StructType


In the previous task, we have created a dataFrame from an RDD. We have used a case class and toDF method to achieve the same. However, there are some limitations using case class method. The case class cannot have more than 22 arguments. If our data has more than 22 fields, it becomes hard to create a dataFrame from RDD using case class and toDF method.


To overcome this limitation, we have a createDataFrame method, which takes an RDD and schema as parameters to create a dataFrame. Let us create a dataFrame using createDataFrame method.


We shall be using the same input file mlb_players.csv for this task as well. However, you are highly encouraged to use another input file and play around.


Step 1: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter createDf as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{DoubleType, IntegerType, StringType, StructField, StructType}


  

  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Step 2: Let us now write the main function and create SparkSession as we already did in previous tasks.


def main(args: Array[String]): Unit = {

 val ss = SparkSession
   .builder()
   .appName("Rdd to DataFrame")
   .master("local[*]")
   .getOrCreate()


  



Step 3: The next step is similar to what we have done in the previous task. We load the input file using the textFile API, extract the header and filter it out using the filter method. Next, we split the fields based on comma delimiter.


val input = ss.sparkContext.textFile("chapter_7/mlb_players.csv")
val header = input.first()
val records = input.filter(x => x != header)
val fields = records.map(record => record.split(","))
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 4: Now, instead of mapping the fields to a case class as in previous task, we map the fields to a Row object. This Row object is an ordered collection of fields, which can be accessed by index or position. It is similar to a row in a table.


val structRecords = fields.map(field => Row(field(0).trim, field(1).trim, field(2).trim, field(3).trim.toInt, field(4).trim.toInt, field(5).trim.toDouble))


We now have our fields as rows. All the fields are assigned and casted as we have in the previous task.


Step 5: Now that we have rows, let us create a schema. We can create schema using the instance of StructType object. The StructType object contains StructField objects which take parameters as name of the column, type of the column and an optional boolean specifying if the column contains null values. Also, the data type must be defined as StringType, IntegerType, DoubleType etc.




  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


val schema = StructType(List(
 StructField("player_name", StringType, false),
 StructField("team", StringType, false),
 StructField("position", StringType, false),
 StructField("height", IntegerType, false),
 StructField("weight", IntegerType, false),
 StructField("age", DoubleType, false)
))


We have specified the StructFields as a List inside the StructType object.


  



Step 6: Finally, we can use our RDD which is a structRecords and schema as parameters for createDataFrame method to create a dataFrame.


val recordsDf = ss.sqlContext.createDataFrame(structRecords, schema)


Since createDataFrame is a method of sqlContext object, we call sqlContect on our SparkSession object and then call createDataFrame method.




  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Let us use the show method to check the created dataFrame using createDataFrame method.


recordsDf.show()

 }

}


  



Step 7: Let us finally run our program and check the output. The output is as shown in the screenshot below.


  

  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


We have successfully created our dataFrame using the createDataFrame method.


Task 3 is complete!
Task 4: Querying data using Spark SQL


With Spark SQL, we can process data in two ways. The first way is to use the Spark SQL data manipulation language, which are essentially the traditional SQL queries we use in RDBMS. We can simply perform operations using SQL queries and obtain the result. The second way is to use the DataFrame API which is DataFrame Data Manipulation language. The DataFrame API is programmatical kind of language. 
You are free to use any way to process the data based on your level of comfortability. Both the APIs are optimized and are efficient. Let us first use SQL queries to process data in this task and look at few operations we can perform.


Step 1: We shall be using the file us-500.csv for this task as input source. Please create a new object and name it sqlQueries. Perform all the steps you performed in Task 1 and come back here. Your program should look something like the screenshot shown below.


















  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 2: Let us first assign a view for our dataFrame so that we can run queries against it. In simple words, we are just creating a table for our dataFrame so that we can reference it while we run SQL queries against it. 


users.createOrReplaceTempView("users")


We are using the createOrReplaceTempView method to create a temporary table named users if it doesn’t exist, or replace a view if it already exists with the same name. This temporary table is available till the SparkSession is active. Once the session ends, the table will not be available anymore. Hence the name temp view. You can also persist the table using saveAsTable method.






  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 3: Let us now run some queries. First, let us run a basic query to select all the users from table who belong to the state Florida.


val foridaUsers = spark.sql("SELECT * FROM users WHERE  state = \”FL\”")


We use the sql method in our SparkSession object which is spark and enter the following query. We have simply entered a query to select all the records from our users table who belong to state FL. Since the values of State are String, we have to enclose them in double quotes and use the escape character ‘\’.


Next, we can simply call show method on floridaUsers dataFrame to check the results.


results.floridaUsers.show()














  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Run the program and you should have the result as shown in the screenshot below with the top 20 users who belong to State ‘FL’.


  



You can also use the collect, foreach and print to print all the records as we used to in the previous exercises.


Step 4: Let us now run a query to check the count of total users who belong to state “New Jersey”.


val totalUsersNJ = spark.sql("SELECT count(*) AS NJ_Count FROM users WHERE state = \”NJ\”")
totalUsersNJ.show()


In the query above, we are simply using count function with a WHERE clause to get the count of users who belong to NJ. We use the AS clause to name the column as shown in the screenshot below. Then, we use the show method to display the results.
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


The output should be as shown in the screenshot below.


  



Step 5: Let us now look at the count of users by state and also arrange them in descending order of their count.


Note: You may comment out the previous queries to avoid processing them again.
val userCountByState = spark.sql("SELECT state, count(*) AS count FROM users GROUP BY state ORDER BY count DESC)
totalUsersNJ.show()


In the query above, we have used GROUP BY to group by state and then ORDER BY to sort the count in descending order.


The result is as shown in the screenshot below.
















  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
  



Step 6: Instead of using the show method, let us now see how we can save this file. You may comment out or remove the line with the show method from the previous step.


Before we save the file, there is one important thing we need to know about how SparkSQL creates tasks. For the second stage of DAG, the number of tasks created to process the data are 200 by default. But in Spark core, the number of tasks in second stage is always equal to number of tasks in first stage. For example, consider we have two stages stage 0 and stage 1. For stage 0, the number of tasks created will be equal to the number of input splits. Next, in Spark core, the number of tasks in stage 1 are equal to the number of tasks in stage 0. However, in SparkSQL, the number of tasks in stage 1 will be 200 by default.


The 200 tasks in SparkSQL is a good starting point if there is huge data to process. We can configure the number for optimization if required. However, our file is just a sample of small data and there is no requirement of 200 tasks to be created. So, using the following property, we can set the total number of tasks as 1. If we run with the default value of 200 and save the file, there will be multiple partitions of output for small set of data. You are free to check how the output looks like without setting this property.
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


spark.conf.set("spark.sql.shuffle.partitions", "1")


Since this is a configuration, we are calling conf on our SparkSession object and setting the property to use only one task using the set method. This will only create one task and we will be left with only one output file. 


Let us now save the file using the code below.


 userCountByState.write
   .format("csv")
   .save("chapter_7/output")


Similar to reading the file using read and load methods, we use write and save methods to save the file to file system.


  



After you run the program, the process should finish with an exit code of 0 indicating successful processing and the output should be available in the file system with only one file in the saved location.










  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Open the file and you should have the values as shown below.


  



This way you can perform any operations using the SQL data manipulation language. 


Task 4 is complete!










  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Task 5: Joins using Spark SQL


Let us now use Spark SQL to join two dataFrames.


Step 1: Download the ratings.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


ratings-head.csv - http://bit.ly/2FPdhHE


Please save this file in IdeaProjects/Spark/chapter_7 folder if not saved already in the previous tasks.


Step 2: Download the movies.csv file from the URL below. This file contains three columns: movieID, movieName and genre.


movies-head.csv - http://bit.ly/2RTg72N


Please save this file in IdeaProjects/Spark/chapter_7 folder. 


We shall join these datasets based on the movieID.


Step 3: Create a new Scala object and name it sqlJoins. Then import the following:


import org.apache.spark.sql.SparkSession


Then write the main function for our program and create a SparkSession object as shown below.






  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .appName("SQL Joins")
   .master("local[*]")
   .getOrCreate()


  

















  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


Step 4: Let us now read both the files as shown below.


val movies = spark.read
 .format("csv")
 .options(Map("inferSchema" -> "true", "header" -> "true"))
 .load("chapter_7/movies-head.csv")
val ratings = spark.read

 .format("csv")
 .options(Map("inferSchema" -> "true", "header" -> "true"))
 .load("chapter_7/ratings-head.csv")


We now have two dataFrames for each of our input files.


  



Step 4: Now that we have our dataFrames, let us create a temp view so that we can run our join query against them.


movies.createOrReplaceTempView("movies")
ratings.createOrReplaceTempView("ratings")
  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Step 5: We now have our views. All we need to do now is to perform the join. We can join this using the SQL query as shown below.


val joinedDf = spark.sql("SELECT * FROM movies JOIN ratings ON movies.movieId = ratings.movieId”


Finally, let us call the show method on our joinedDf dataFrame and run the program.


joinedDf.show()


You should see the joined table as shown in the screenshot below.


  



Task 5 is complete!
  
  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API
Task 6: Operations using DataFrame API


Step 1: The initial few steps are similar to what we have done in the previous tasks. Create a new Scala object and name it dfOps. Specify the required imports and create a SparkSession object as in the previous tasks. Finally load the us-500.csv file.


Your program at this point of time should look like the one in the screenshot below.


  



Step 2: Now that we have the dataFrame created, let us perform an operation to select all the users from Florida using the DataFrame API.


val foridaUsers = users.select(“*”).where(“state = \”FL\”")
  
  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


This is similar to the SQL query which we have performed in the task earlier. We have methods here which look more like programming style. In the code above, we have used select method to select all the columns of our dataFrame and then where method to specify our condition.


Please see that we need not create a temp view as we did earlier. It is only required when we are working with SQL queries.


Let us now call the show method on our dataFrame to view the results on the console.


floridaUsers.show()


  



The output should look something like the table shown in the screenshot.


  



Step 3: Let us now run a query to check the count of total users who belong to state “New York”.
  
  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


val nyUserCount = users.groupBy("state")
   .agg(("state", "count"))
   .where("state = \"NY\"")


This is a bit different than what we have done in the SQL query. In the code above, we are first grouping by state and then applying the agg method. The agg method takes the column as first parameter and then the type of aggregation as second parameter. We specify the second parameter as count since we want to count the number of users from New York State. Then, we specify the condition using the where method.


Let us now call the show method on our dataFrame to view the results.


nyUserCount.show()


  



The output is as shown in the screenshot below.


  

  
  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


Step 4: Let us now write some code to get the count for all the users for each state. We first need to import the implicits as shown below.


import spark.implicits._


val userCountByState = users.groupBy("state")
   .agg(("state", "count"))
   .orderBy($"count(state".desc)


As you can see in the query above, we use the orderBy method to order the result by the count of state in a descending order.


Let us call the show method.


userCountByState.show()


  



The output is as shown in the screenshot below.










  
  
  
  
  
  
  

Spark SQL


Spark SQL Joins
Ops. Using DF API


  



Task 6 is complete!




























SUMMARY


In this chapter we have looked at basics of Spark SQL. We have learned what Spark SQL is, why it is required and its architecture in detail.
In the labs, we have had our hands on creating a dataFrame, converting an RDD to dataFrame using the toDF method and StructType. We have then used SQL queries and dataFrame API to process data.


























REFERENCES


* https://spark.apache.org/
* https://spark.apache.org/docs/latest/sql-data-sources.html
* https://spark.apache.org/docs/latest/api/sql/index.html


































CHAPTER 8 : DATASETS
Theory


We have been working with RDDs and DataFrames in the previous chapters, and understood what they are and why we need them. Let us look at the Datasets API which is the most advanced API in Spark. We shall see in detail what Datasets are and why do we need them. To understand Datasets better, let us first understand the differences between an RDD and a DataFrame.
RDD vs DataFrame


RDD is a low level API.
	DataFrame is High level API.
	RDDs are written in functional style of programming.
	DataFrames are more of relational style of programming.
	RDDs are type safe. Type safe guarantees that the variable of a particular data type declared, holds the exact same type of data within it. Type checking happens at the compile time and not at the runtime. For example, if you declare a variable as Int and assign a String, an error will be shown at compile time itself.
	DataFrames are not type safe. The type error is only shown at runtime and not at the compile time. It is possible to declare an Int and assign a String, but this error is only detected at runtime.
	RDDs are slower when compared to DataFrames. There is no optimizer available for RDDs. 
	DataFrames are faster and efficient. DataFrames are optimized with Catalyst optimizer and Tungsten.
	RDDs processes structured as well as unstructured data.
	DataFrames on the other hand only process structured and semi-structured data.
	RDDs cannot infer schema for the ingested data. RDDs requires the users to explicitly specify the schema.
	DataFrames have the ability to infer schema automatically for the ingested data. The user can also explicitly specify the schema.
	As we can see from the comparison above, there are clearly some advantages and disadvantages for both the APIs. Wouldn’t it be great if we could somehow take the advantages and discard the disadvantages? Well, turns out we could!
What are Datasets?
Dataset is the most advanced API in Spark. Datasets are an extension of DataFrames, which overcome all the disadvantages of both RDDs and DataFrames. The Dataset API provides developers with type safe mechanism and functional style programming, while retaining the relational type of programming in DataFrames and performance optimizations. Hence it is called as an extension of DataFrames. Datasets were introduced in Spark from the 1.6 version.


Datasets use Encoders to serialize and deserialize the Spark SQL representation to JVM objects. The serialization and deserialization with encoders is significantly fast when compared to Java serialization. In simple words, datasets use encoders to convert the data between JVM objects and Spark SQL representation of tabular objects.
Why Datasets?
The following are the advantages of using datasets.


* Datasets are a combination of RDDs and DataFrames. Datasets help you code like you would with RDDs and process them through performance optimization engines.


* Datasets have all the other features as RDD and DataFrames. They are lazy, immutable, distributed and can be cached.


* Datasets assures type safety similar to that of RDD. Any type errors are flagged at compile time rather than being notified at runtime.


* Data processing with datasets is optimized using the Catalyst optimizer and Tungsten similar to that of DataFrames. This ensures very fast and efficient processing of data.


* Dataset API provides the developers with a functional style of programming as well as relational style of programming.


* Datasets can process structured as well as unstructured data. Datasets can also automatically infer schema.


* Datasets can be converted from RDD and DataFrames. 


We shall be working with datasets later in the lab exercises. 


That’s all in theory for this chapter. Let us now proceed to our lab exercises.
















































AIM


The aim of the following lab exercises is to start writing Spark SQL code in Intellij to learn about Datasets and built-in functions.
The labs for this chapter include the following exercises.
* Creating Dataset using DataSource API
* Creating Dataset from an RDD
* Aggregate and Collection Functions
* Date/Time Functions
* Math and String Functions
* Window Functions


We need the following packages to perform the lab exercises: 
* Java Development Kit
* Scala
* Spark




















LAB EXERCISE 7: DATASETS & FUNCTIONS
  

Date/Time


Math & String
Window Functions


	  


1. Creating Dataset using DataSource API
2. Creating Dataset from an RDD
3. Aggregate and Collection Functions
4. Date/Time Functions
5. Math and String Functions
6. Window Functions














  
  

Date/Time


Math & String
Window Functions
Task 1: Creating Dataset using Data Source API


Creating a Dataset is similar to that of a DataFrame with some minor changes.


Step 1: Download the ratings.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


ratings-head.csv - http://bit.ly/2FPdhHE


Please save this file in IdeaProjects/Spark/chapter_8 by creating a new folder.


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter createDS as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession


Next, we need to write a case class so that we can specify the schema for our fields. This case class is what makes a Dataset and differentiates from DataFrame. While loading the file, we simply refer to this case class object to create a dataset.


private case class Movies(userId: Int, movieId: Int, rating: Double, timestamp: String)


We have created a case class and named it Movies by specifying the fields and its types.






  
  

Date/Time


Math & String
Window Functions


If there is an error saying that the case class Movies is already defined, change the name of class case to anything else and refer to it accordingly throughout the program.


Step 3: Now, write the main function and create the SparkSession object as shown below.


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .appName("Creating a Dataset")
   .master("local[*]")
   .getOrCreate()


  



Step 4: Before we load the file using the DataSource API, we need to import the implicits. This import is required to create a Dataset.


import spark.implicits._


This import is available in our SparkSession object. Hence we refer it with the SparkSession object (spark) which we created in the previous step.
  
  

Date/Time


Math & String
Window Functions


Now, we load the file as we usually do while creating the DataFrame except for the as method at the end. The as method refers to the case class object Movies creating a Dataset object.


val movies = spark
 .read
 .format("csv")
 .options(Map("header" -> "true", "inferSchema" -> "true"))
 .load("chapter_8/ratings-head.csv")
 .as[Movies]
 .cache()


  



We use the cache method to cache our Dataset in memory so that it does get created everytime an action is called. You now have your Dataset created.


Step 5: We can now use the show method as well as printSchema method to check the first 20 records in our Dataset and the schema associated with the Dataset.


movies.printSchema()

movies.show()


  
  

Date/Time


Math & String
Window Functions


The following schema is shown when we run the program.


  



And the result as shown below.


  



Step 6: Let us now perform some operations with our dataset. The following code below is used to find the total count of each rating.


val ratingCount = movies.groupBy("rating").count()
  
  

Date/Time


Math & String
Window Functions


We simply use the groupBy function to group all the ratings and then count the number of ratings using the count function. 


Next, let us call the show method to display the result on the console.


ratingCount.show()


Once the program has finished running, the following result should be shown as output.


  



But this was also achieved using DataFrames. How are Datasets special? Well, we can also write functional style programming with Datasets.


Step 7: Let us now write some functional style programming using our Dataset. First, we shall extract the two columns userId and rating from the movies dataset using the map function.


val users = movies.map(x => (x.userId, x.rating))


As you can see, we have used functional style programming the code above which we cannot use with DataFrames. Also, we are able to refer to the column by its name.
  
  

Date/Time


Math & String
Window Functions


Next, let us convert our Dataset to RDD and write some code to find out average rating for each user.


val count = users.rdd.mapValues(x => (x,1))
 .reduceByKey((x,y) => (x._1 + y._1, x._2 + y._2))
 .mapValues(x => x._1/x._2)
 .sortByKey(false)


We have simply used the rdd method to convert our dataset to RDD. The rest of the code is the familiar functional programming style.


Let us now use the collect method as we used to in the previous exercises and print out the result to the console.


count.collect.foreach(println)
        }
}


  



The output should be shown as below.


  
  

Date/Time


Math & String
Window Functions


  



Task 1 is complete!
Task 2: Creating Dataset from an RDD


As we did in the previous lab exercise to create a DataFrame from an RDD, we shall also look at creating a Dataset from an RDD.


Step 1: Download the mlb_players.csv file from the URL below. This file contains six columns: name, team, position, height, weight, age.


mlb_players.csv - http://bit.ly/2JhzVJj


Please save this file in IdeaProjects/Spark/chapter_8 folder. 


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter rddToDs as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.
  
  
  

Date/Time


Math & String
Window Functions


import org.apache.spark.sql.SparkSession


Step 3: Let us now create a case class so that we can define schema to our dataset as we did with DataFrame in the previous exercise. The names which we specify for attributes of case class object will get mapped as column names for our dataset. 


case class Players(player_name: String, team: String, position: String, height: Int, weight: Int, age: Double)


Step 4: Let us now write the main function for our program and create a SparkSession object as shown below.


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .appName("RDD to Dataset")
   .master("local[*]")
   .getOrCreate()


  





  
  
  

Date/Time


Math & String
Window Functions


Step 5: Since our aim is to convert an RDD to a Dataset, we must use the textFile API in the SparkContext object to read the file and create an RDD.


val input = ss.sparkContext.textFile("chapter_8/mlb_players.csv")


We now have an RDD created. But the file contains a header with column names. We must first remove the header. Instead of using the first method and then the filter method, let us remove the header using an efficient approach.


val removeHeader = input.mapPartitionsWithIndex((index, itr) => {
 if (index == 0) itr.drop(1) else itr
})


In the previous approach to remove the header, we were using filter method to filter out the first record from the RDD. This approach works well when there is small data. But, Spark isn’t developed for small data. We will be using huge data in realtime environment.


Consider a scenario where we have a billion records in our RDD. When we use the filter function, the filter condition is tested for each and every record of RDD. Just to filter out one record, we end up testing the condition for all the billion records. To overcome this, we can use the mapPartitionWithIndex function. This higher order function provides us with an entire partition of RDD with index, unlike map function, which provides us with each record. We can then pass our logic which is to drop the first record in the partition if the partition index is 0, else return the partition as is. So, the condition inside if statement is only verified per partition basis instead of per record. This condition is only tested based on the number of partitions and not on the number of records, making it efficient.
  
  
  

Date/Time


Math & String
Window Functions


  



Step 6: The next step is similar to what we have done in the previous exercise while creating a DataFrame from RDD. We split the fields based on a comma, so that we can assign each individual field to its appropriate case class field.


val fields = removeHeader.map(record => record.split(","))


Now that we can access individual fields by their position, let us assign them to the case class Players, using the map function as shown below.


val structRecords = fields.map(field => Players(field(0).trim, field(1).trim, field(2).trim, field(3).trim.toInt, field(4).trim.toInt, field(5).trim.toDouble))


We call trim method on all the fields to remove leading and trailing white spaces, and also cast height, weight and age fields to Int, Int and Double respectively.








  
  
  

Date/Time


Math & String
Window Functions


  



Step 7: We now have our data in structured columns with named records. We can now simply convert it to a dataset using toDS method. 


But before we can use the toDS method, we need to import the implicits as shown below.


import ss.implicits._

val recordsDs = structRecords.toDS()


We now have our dataset recordsDs created from RDD.


  



Step 8: Let us now call the show method on our dataset and run the program.




  
  
  

Date/Time


Math & String
Window Functions


recordsDs.show()
 }
}


The output is as shown in the screenshot below.


  

The only difference between creating a DataFrame and a Dataset is the method which we call at the end. We use the toDF method to create a DataFrame and toDS method to create a dataset.


We can also use the programmatical schema to create a dataset as we did with the DataFrame in Task 3 of previous exercise. Please try it out and create a dataset by programmatically creating a schema.


Task 2 is complete!










  
  
  
  

Date/Time


Math & String
Window Functions
Task 3: Aggregate and Collection Functions


Before we look at the user-defined functions, let us look at few of the built-in functions which are available in Spark in the next two tasks. 
Aggregate Functions


Step 1: Please perform the steps 1 – 4 from task 1 in this exercise. Make sure you name the Scala object as builtInFunctions instead of createDS. Also, make sure you rename the case class to Rating so that it doesn’t conflict with classes from other objects. You should be having a screen as shown below.


  



Step 2: Let us first use the built-in aggregation functions. In the following piece of code, we are performing multiple aggregations at once.


  
  
  
  

Date/Time


Math & String
Window Functions


Before we use the functions we need to have the following import as shown below.


import org.apache.spark.sql.functions._


Next, we can simply use the select method and perform the multiple aggregations at once.


val agg = movies.select(
   avg("rating")
 , min("userId")
 , max("movieId")
 , sum("userId")
 , mean("rating")
)


  



We have used multiple aggregations such as avg, min, max etc on various columns. As the names of the functions suggest, avg computes the average, min and max compute the lowest and the highest values in the column, sum and mean compute the sum and mean of all the values in the columns respectively.
  
  
  
  

Date/Time


Math & String
Window Functions
Step 3: Let us call the show method and run the program.


agg.show()


The following result should be shown.


  



Step 4: As we can see from the screenshot above, the column names are not so nice. We can assign our own column names using the as method.


val aggAlias = movies.select(
 avg("rating").as("avgRating")
 , min("userId").as("lowestUserId")
 , max("movieId").as("highestMovieId")
 , sum("userId").as("sumOfUserId")
 , mean("rating").as("meanRating")
)


All we are doing is assigning an alias (column name) to each and every field using the as method.














  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 5: Let us call the show method and run the program.


aggAlias.show()


The following result should be shown.




  



As you can see from the screenshot above, the column names appear as we have specified.


Step 6: Let us now use the groupBy method and perform the aggregations.


  
  
  
  

Date/Time


Math & String
Window Functions


val byUser = movies.groupBy("userId")
 .agg(countDistinct("rating").as("distinctCount")
   , sumDistinct("rating").as("distinctSum")
   , count("movieId").as("movieCount"))


In the code above, we have used the groupBy method and performed the aggregations over the group. We have used the countDistinct function to count the number of distinct ratings for userId. Similarly, the sumDistinct function to only sum the distinct ratings. Finally, count function, as you know, is used to count all the movie ids.


  



Step 7: Let us call the show method and run the program.


byUser.show()


The following result should be shown.








  
  
  
  

Date/Time


Math & String
Window Functions


  

Collection Functions


So far, we have been working with IDE and learned how to create objects and run the programs. However, for this task where we learn about collection functions, let us work with the Spark shell so that we can quickly check the output on the fly, instead of running the program everytime we use a function. You may choose to work with IDE and it is perfectly fine. You will then have to create the SparkSession object and specify the required imports.


Step 1: Open the terminal and fire up the Spark shell. You should be having the scala prompt as shown in the screenshot below.










  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 2: We need to import the implicits and functions to be able to work with the functions.


import spark.implicits._
import org.apache.spark.sql.functions._


Let us now declare a Seq collection as shown below.


val num = Seq(Seq(1,2,3), Seq(4, 5, 6), Seq(7,8,9))
















  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 3: Next, let us convert the collection to dataset using the toDS method and rename the column as numbers using the withColumnRenamed method. The default column name when you create a dataset is value. Hence we change the default column name to numbers.


val numDS = num.toDS().withColumnRenamed("value", "numbers").cache()


  



Step 4: now that we have our dataset, let us apply some of the collection functions. First, let us use the array_contains function to check if the collection contains the element we require.
  
  
  
  

Date/Time


Math & String
Window Functions


val contains = numDS.where(array_contains($"numbers", 5))


We use the array_contains function inside the where method to check if the column numbers contains the number 5. This function takes the column name as the first argument and the element as second. We can also pass more than one element as second argument enclosed in a collection as shown in the example below.


val eg = numDS.where(array_contains($"numbers", Array(7,8))


Let us check the result using the show method.


contains.show()


The result is as shown in the screenshot below. Since we used the where method, we are only shown the collection which contains the number 5.


  



Step 4: Let us now use explode function. The explode function takes each element in the collection and generates a new row. 


val exploded = numDS.select($"numbers", explode($"numbers").as("exploded"))
  
  
  
  

Date/Time


Math & String
Window Functions


In the code above, we have used the select method to select the numbers column, and then the result of using explode function on numbers column as second column. We have specified the second column name as exploded.


After running the show method, the following result is shown as in the screenshot.


exploded.show()


  



As you can see from the output, each element inside the collection is a new row.


Similarly, we also have posexplode function, which also provides us with the position of each row.


val posExploded = numDS.select(posexplode($"numbers"))






  
  
  
  

Date/Time


Math & String
Window Functions


When posexplode function is used, two columns are created. One is the column with exploded values as seen above and the other is with the position of each exploded value.


After running the show method, the following result is shown as in the screenshot.


posExploded.show()


  



Since we haven’t used the as method to specify the names of the columns, the default column names pos and col are created for us.


Step 5: Let us now use size function to check the size of each of the collections in our datasets. The size function simply returns the size of the collection similar to the length method.




  
  
  
  

Date/Time


Math & String
Window Functions


val sizeDS = numDS.select($"numbers", size($"numbers").as("size"))


We have used the select method to select the numbers column as first column and size function on numbers column as second column. We have then used the as method to rename the column as size.


After running the show method, the following result is shown as in the screenshot.


sizeDS.show()


  



The size is shown as 3 since there are 3 elements in each collection.


Step 6: Let us now use the sort_array function to sort the collection in ascending or descending order.


val sorted = numDS.select($"numbers", sort_array($"numbers", false).as("sorted"))


The sort_array function takes the column on which we want to perform the sort on as first argument and boolean as second argument. We specify true if we want to sort in the ascending order or false, otherwise.
  
  
  
  

Date/Time


Math & String
Window Functions


After running the show method, the following result is shown as in the screenshot.


sorted.show()


  



There are many other functions which can be applied. Please check the link in references section and try to practice with all the functions.


Task 3 is complete!
Task 4: Date/Time Functions
Let us now look at Date/Time functions to manipulate, extract and perform arithmetic operations on date and time. As in collection functions, we shall be using the Spark shell to demonstrate Date/Time functions as well. Please feel free to use IDE if you prefer it over Spark Shell.


Step 1: Let us first create a collection with data as shown below. Please make sure you have the imports from the previous section already imported. You will have to import them again if you have closed the Spark Session.


  
  
  
  
  

Date/Time


Math & String
Window Functions




val dates = Seq(
 (1, "Ernesto", "2015-09-24"),
 (2, "Lee", "1985-05-16"),
 (3, "John", "2012-07-16"),
 (4, "Doe", "1914-08-02")
)


Next, let us convert the collection to dataset using the toDS method and rename the column as shown below using the withColumnRenamed method. The default column names for dataset are monotonically increasing numbers like _1, _2, _3 etc.


val datesDS = dates.toDS()
   .withColumnRenamed("_1", "id")
   .withColumnRenamed("_2", "name")
   .withColumnRenamed("_3", "date")




  

  
  
  
  
  

Date/Time


Math & String
Window Functions


Let us check the schema using the printSchema method, so that we can compare the datatype for date column in the next step.


datesDS.printSchema()


  



As you can see from the screenshot above, the date is of type String.


Step 2: Let us cast the date column and convert it to date type using the cast function as shown below.


val casted = datesDS.select($"id", $"name", $"date".cast("date")).cache()


Let us print the schema to check if we were able to successfully convert the date column from String type to Date type. Let us also use the show function to view the dataset.


casted.printSchema()
casted.show()








  
  
  
  
  

Date/Time


Math & String
Window Functions


  



As you can see from the screenshot above, we have successfully casted the date column as date type.


Step 3: Let us now extract the individual attributes from the date object such as day, month, year etc. We shall be using various functions to add columns for each function using the withColumn method.


val extracted = casted
  .withColumn("year", year($"date"))
 .withColumn("month", month($"date"))
 .withColumn("dayOfYear", dayofyear($"date"))
 .withColumn("quarter", quarter($"date"))
 .withColumn("weekOfYear", weekofyear($"date"))


  
  
  
  
  

Date/Time


Math & String
Window Functions
We have used the year, month and dayofyear functions to extract the extract the individual attributes from the date column. We have also used the quarter function to get which quarter the date is from and weekofyear function to get the week which the date belongs to.


The following output is shown when we use the show method.


extracted.show()


  



Step 4: Let us now use the arithmetic functions to manipulate the date.


val arithmetic = casted
 .withColumn("ageInDays", datediff(current_date(), $"date"))
 .withColumn("addedDays", date_add($"date", 25))
 .withColumn("subtrDays", date_sub($"date", 16))
 .withColumn("addedMonths", add_months($"date", 4))
 .withColumn("lastDay", last_day($"date"))
 .withColumn("nextDay", next_day($"date", "tuesday"))
 .withColumn("monthsBetween", months_between(current_date(), $"date", true))




  
  
  
  
  

Date/Time


Math & String
Window Functions


* The datediff function is used to calculate the date difference between two dates. Here we have used the current_date method to get the present date and get the difference from the date in date column.


* The date_add and date_sub functions are used to add and subtract the number of days from the date in date column. The function takes the date column and the number of days  as arguments.




* The add_months function is used to add number of months to the date in date column. The function takes the date column and the number of months  as arguments.


* The last_day and next_day functions are used to get the last day of the month and next day of the month for the day of the week for the date in date column respectively. The next_day function takes date column and the day of week as arguments.


* The months_between function is used to get the number of months between two days. We have used the present date using current_date function and date column as the arguments.


 The following output is shown when we use the show method.


arithmetic.show()










  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 5: Next, let us use the timestamp functions. Since we have only created a date type in the previous dataset, let us create a timestamp type instead of date type. First, let us create the dataset and rename the columns as shown below.


val timeStamp = spark.createDataset(Seq(
 (1, "Ernesto", "2015-09-24 00:01:12"),
 (2, "Lee", "1985-05-16 03:04:15"),
 (3, "John", "2012-07-16 06:07:18"),
 (4, "Doe", "1914-08-02 09:10:20")
))

val timeStampDS = timeStamp
 .withColumnRenamed("_1", "id")
 .withColumnRenamed("_2", "name")
 .withColumnRenamed("_3", "timeStamp")


Let us print the schema so that we can compare it with the timestamp type in the next step.


timeStampDS.printSchema()








  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 6: Let us now convert the timestamp which is of String type to timestamp type.


val castedTimeStamp = timeStampDS.select($"id", $"name", $"timeStamp".cast("timestamp")).cache()


Let us now print the schema and the dataset to check the casting.


castedTimeStamp.printSchema()
castedTimeStamp.show()


As you can see from the screenshot below, we have successfully casted the timestamp column from String type to timestamp type. 














  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 7: Let us now extract the attributes from timestamp column as we did for the date column couple of steps ago.


val extractedTs = timeStampDS
 .withColumn("second", second($"timeStamp"))
 .withColumn("minute", minute($"timeStamp"))
 .withColumn("hour", hour($"timeStamp"))


The functions used above are self explanatory.


The following output is shown when we use the show method.


extractedTs.show()










  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 8: Finally, let us use couple of conversion functions to convert the dates into different formats.


val conversions = timeStampDS
   .withColumn("unixTime", unix_timestamp($"timeStamp"))
   .withColumn("fromUnix", from_unixtime($"unixTime"))


* The unix_timestamp function is used to convert the timestamp to unix timestamp.


* The from_unixtime function is used to convert the unix time which we obtained above.


The following output is shown when we use the show method.


conversions.show()










  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Task 4 is complete!
Task 5: Math and String Functions
Math Functions
There are a number of math functions which can be applied to columns with numbers. Let us now look at few of them.


Step 1: Let us first create a collection with data as shown below. Please make sure you have the imports from the previous section already imported. You will have to import them again if you have closed the Spark Session.


val numbers = List(5, 4, 9.4, 25, 8, 7.7, 6, 52)


Step 2: Next, let us convert the collection to dataset using the toDS method and rename the column as numbers using the withColumnRenamed method. The default column name when you create a dataset is value. Hence we change the default column name to numbers.






  
  
  
  
  
  

Date/Time


Math & String
Window Functions


val numbersDS = numbers.toDS()
 .withColumnRenamed("value", "numbers")
 .cache()


The dataset should now be created with the renamed column.


  



Step 3: Let us now perform various math functions on the dataset. All these functions are self explanatory.


val mathFuncs1 = numbersDS.select(abs($"numbers"), ceil($"numbers"), exp($"numbers"), cos($"numbers"))


* The abs function returns the absolute value of the number.


* The ceil function returns the number of double type greater than or equal to the nearest rounded integer. 


* The exp function returns Eulers E raised to power of double value.


* The cos function returns the trigonometric cosine of an angle.


Let us check the result using the show method.


mathFuncs1.show()


The following result is shown.


  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 4: Let us now use some more math functions.


val mathFuncs2 = numbersDS.select(factorial($"numbers"), floor($"numbers"), hex($"numbers"), log($"numbers"))


* The factorial function returns the factorial of the number.


* The floor function is opposite to the ceil function which returns the number of double type lesser than or equal to the nearest rounded integer.


* The hex function returns a hex value.


* The log function returns the natural logarithm (base e) of a double value as a parameter.


Let us check the result using the show method.


mathFuncs2.show()
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


The following result is shown.


  



Step 5: Let us now use even more math functions.


val mathFuncs3 = numbersDS.select(pow($"numbers", 2), round($"numbers"), sin($"numbers"), tan($"numbers"))


* The pow function returns the number raised to the power of some other number. It takes two arguments. The first argument is the column with numbers and the second argument is number which power has to be calculated.


* The round function returns the rounded value to its nearest decimal.


* The sin and tan functions return the sine and tangent trigonometric angle respectively.


Let us check the result using the show method.


  
  
  
  
  
  

Date/Time


Math & String
Window Functions


mathFuncs3.show()


The following result is shown.


  



Step 6: Let us finally conclude math functions with a couple more of them.


val mathFuncs4 = numbersDS.select(sqrt($"numbers"), log10($"numbers"), $"numbers" + Math.PI)


* The sqrt function returns the square root of the given numbers in the column.


* The log10 function returns the base 10 logarithm of a double value.


* In the third column, we have simply added the value of PI by using the Math.PI expression.


Let us check the result using the show method.


mathFuncs4.show()
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


The following result is shown.


  



There are many other math functions that can be applied. Please check the references section for link to all the functions.
String Functions
There are a plethora of String functions available in Spark. Let us look at few of them now.


Step 1: As usual, let us first create the List and create a dataset from it. Please make sure to specify imports again if you have closed the Spark session.


val quote = List("I have no special talent.",
 "I am only passionately curious.",
 "I have a dream.",
 "I came, I saw, I conquered.")


val quoteDS = quote.toDS().cache()


Let us use the show method to display the dataset as shown below.
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


quoteDS.show()


  



Step 2: Let us first use the split method to split the strings by space as below.


val splitted = quoteDS.select(split($"value", " ").as("splits"))


The split method takes two arguments. The first argument is the name of the column and the second method is the pattern to which the string should be split on.


Let us use the show method to display the dataset as shown below.


splitted.show()










  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



As you can see from the screenshot above, the rows have now been splitted by whitespace.


Step 3: Next, let us use the explode function which we have learned in this exercise to create column for each element in the collection.


val exploded = splitted.select(explode($"splits").as("explode"))


Let us use the show method to display the dataset as shown below.


exploded.show()


















  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Now that we have each word as a row, let us apply some string functions.


The first function we use is to find out the length of each word using the length function as shown below.


val strLen = exploded.select($"explode", length($"explode")).as("length")


Let us use the show method to display the dataset as shown below.


strLen.show()


















  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 4: Let us now use string functions to convert strings from lower case and upper case.


val upCase = quoteDS.select(upper($"value").as("upperCase"))
val lowCase = upCase.select(lower($"upperCase").as("lowerCase"))


Let us use the show method to display the datasets as shown below.


upCase.show()
lowCase.show()


















  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 5: Finally, let us look at substring and trim functions to extract a part of string and trim the whitespaces before and after the string respectively.


val sub = quoteDS.select(substring($"value", 0, 2).as("firstWord"))


val trimmed = sub.select(trim($"firstWord"))


Let us use the show method to display the datasets as shown below.


sub.show()
trimmed.show()


The substring function takes three arguments. The first is the column name from which the sub string to be extracted from. Second and third are the start and end positions from which we extract the string.
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Here we have extracted the first word and space after it using the substring function. Since our first word is only a letter, we start from 0 position and end at 2nd position which is the whitespace. Next, we use the trim function to trim the whitespaces before and after. Since there are no whitespaces before, the function will simply trim the whitespace after. You can also use rtrim function to trim only the whitespaces at the end and ltrim function to trim only the whitespaces at the beginning.


These are only a few string functions and there are many available out there. Please practice with as many functions as you can.


Task 5 is complete!






  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions
Task 6: Window Functions
Let us look at window functions. Window functions are little different when compared to other built-in functions. These are the functions which are applied over a window i.e., a set of rows rather than each row. For example, we can find out the salary for each employee by department and rank them from high to low or vice versa.


Let us perform this task in IntelliJ IDE. However, you may choose to perform it in the Spark shell as well.


Step 1: Download the employee.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


employee.csv - http://bit.ly/2Z3atOx


Please save this file in IdeaProjects/Spark/chapter_8 by creating a new folder.


Step 2: Open IDE, right-click the training package which you have created in previous exercise and hover over New and then click on Scala Class. When prompted, enter window as the name and click on the dropdown for Kind and select Object. Enter the import as shown below.


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._


Next, we need to write a case class so that we can specify the schema for our fields. 


case class Employee(name: String, number: Int, dept: String, pay: Double, manager: String)


  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions
We have created a case class and named it Employee by specifying the fields and its types.


Step 3: Now, write the main function and create the SparkSession object as shown below.


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .appName("Creating a Dataset")
   .master("local[*]")
   .getOrCreate()


Next import the implicits and load the file as shown below.


import spark.implicits._

val employeeDS = spark.read
 .format("csv")
 .options(Map("header" -> "true", "inferSchema" -> "true"))
 .load("chapter_8/employee.csv")
 .as[Employee]


Your program should look something like the one shown in screenshot.


















  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 4: Now that we have loaded our file, let us first create a window. We shall create a window to partition by the department and order by pay in descending order.


val window = Window.partitionBy($"dept").orderBy($"pay".desc)


We have called the partitionBy and orderBy method on the Window object to create a window. The partitionBy method creates partitions for each department withing the window and orderBy method will order the rows by pay in descending order.


Step 5: Let us first use the rank function to get the pay for each employee by department in desceinding order. 


val ranked = rank().over(window)


  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


We have simply called the rank function over the window we created in the previous step using the over method. The rank function allocates increasing integer values to rows based on the column we specified in the orderBy method.


Let us now call the show method on our dataset and add the rank column using the select function.


employeeDS.select($"*", ranked.as("rank")).show()


You should have the following output when you run this program.


  



Step 6: Let us now find the third highest salary from each department.


val rankedDS =  employeeDS.select($"*", ranked.as("rank"))
val second = rankedDS.where("rank = 3")


Let us now call the show method on our dataset.


second.show()
  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



As you can see we have obtained the third highest salary of employees from each department. Since Admin Offices department had two employees with the same pay, they both are ranked as three.


Step 7: Let us now look at dense_rank. Both these functions are similar and provide the increasing integer value based on the ordering of rows. However, there is a difference when it comes to dealing with duplicate values. If there are two duplicate values, the rank function allocates the same rank for both the rows and skips the next rank for next row and allocates next incremental rank for the next row instead. For example, if rank 1 is allocated to top two rows due to presence of duplicate values, rank 2 will be skipped and instead rank 3 will be allocated to the next row.


However, the dense_rank function doesn’t skip the ranks when it encounters duplicate values. In the example above, the dense_rank will allocate same rank i.e., rank 1 for duplicate rows and then rank 2 for the next row. Let us look at this now.


val denseRanked = dense_rank().over(window)


Let us now call the show method on our dataset and add the rank column using the select function.


  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


employeeDS.select($"*", denseRanked.as("dense_rank")).show()


The following output is shown when we run the program.


  



As a challenge, please find out the second highest salary from each department using the dense_rank function.


Step 7: Similarly, let us look at the row_number and percent_rank.


The row_number function, as the name suggests, provides a row number for each row starting from 1 irrespective of duplicates. The numbers are neither skipped nor are repeated.


val rowNumber = row_number().over(window)
employeeDS.select($"*", rowNumber.as("rowNumber")).show()


The following output is shown when we run the program.










  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



The percent_rank calculates the percent rank of a given row based on the following formula. 


(rank - 1) / (the number of rows in the window or partition - 1)


For example, if the rank of a row is 11 and there are 101 rows in the partition, the rank will be 11-1/101-1 or 10/100 or 0.1. The percent rank ranges from 0 to 1. The first row will always have a percent rank of 0.


val percentRank = percent_rank().over(window)
employeeDS.select($"*", percentRank.as("percentRank")).show()


The following output is shown when we run the program.
















  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions


  



Step 8: Let us finally look at lead and lag functions. The lead and lag functions are used to find how much the value of next row is leading or lagging when compared to the current row.


val leads = lead($"pay", 1, 0).over(window)
employeeDS.select($"*", leads.as("lead")).show()


The lead function takes three arguments. The first is the column name, second is the offset value, which determines the number of rows preceding/succeeding the current row and the third is the default to specify the default value if the offset is outside the scope of the window.


The following output is shown when we run the program.


  



  
  
  
  
  
  
  

Date/Time


Math & String
Window Functions
Similarly, there is lag function which calculates the lag.


val lags = lag($"pay", 1, 0).over(window)
employeeDS.select($"*", lags.as("lag")).show()


The following output is shown when we run the program.


  



Task 6 is complete!


























SUMMARY


In this chapter we have looked at Datasets, which is the advanced API of Spark. We have learned what Datasets are, why they are required and how they are beneficial over RDD and dataframes.
In the labs, we have had our hands on creating a dataset, converting an RDD to dataset using the toDF method. We have then used built-in functions to process data.






















































REFERENCES


* https://spark.apache.org/
* https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html
* https://spark.apache.org/docs/latest/api/sql/index.html






























































CHAPTER 9 : USER DEFINED FUNCTIONS
Theory


We have learned about the built-in functions in the previous chapter. Let us now look at User-Defined Functions.
Why User-Defined Functions?
Spark SQL has tons of built-in functions available to process data. These functions include aggregations, date-time functions, Math functions, String functions, collection functions and window functions. These built-in functions come handy and can be applied most of the time. But there can be scenarios which do not get satisfied with the usage of built-in functions. For such scenarios we can write our own functions called User-Defined Functions. We simply have to write these custom functions and register them with Spark SQL to be able to use them in our code.


We can classify such custom written functions in two types.


* User-Defined Functions (UDFs)
* User-Defined Aggregate Functions (UADFs)


The difference between UDF and UADF is that, the UDF performs the user-defined operation on one row at a time and returns the result for each row, while UADF performs the user-defined operation on group of rows on a column. An example of aggregate function is sum, count, average etc. The aggregate functions are followed by groupBy function most of the time.
Steps to implement User Defined function
User defined functions (UDF & UDAF) can be implemented by following these three steps.


* Writing a User Defined Function.
* Registering the User Defined Function in Spark Application.
* Using the User Defined Function in Spark SQL or in the DataFrame API.


The steps above are similar to implement a user defined function in Apache Hive and Apache Pig.
UDAF types
The user defined aggregate functions (UDAF) can be further classified into two types. 


* Typed user defined aggregate function
* Untyped user defined aggregate function


The Typed user defined aggregate function is applied to Datasets where we get the structure (schema) of rows using the case class, while the Untyped user defined aggregate function is applied to DataFrames.


Please use the built-in functions over UDFs whenever you can. The built-in functions are an efficient way to perform operations. UDFs should only be used when they are absolutely required.


Do not worry if this doesn’t make any sense right now. We shall be looking at UDF and UDAF in our lab exercise to understand this better. The UDFs in Spark SQL are not so hard to implement. We can also use the Hive UDFs within Spark SQL.


Before we jump into our lab exercises, let us look at some Scala concepts which are required while writing our custom functions.
Function currying in Scala
Function currying in Scala is used to create partially applied functions. The curried functions are declared with multiple parameter groups with each group enclosed in parenthesis.


For example,


Scala> def sum(a: Int) (b:Int): Int = {
> a + b
> }


There can also be multiple parameters in each parameter group. We can then use this curried function and create partially applied functions.


Partially applied functions in Scala
The partially applied functions, as the name suggests, are applied partially by only passing some parameters while holding back other parameters. The partially applied functions are used when you do not want to pass all the parameters at once, but instead you can pass some parameters first and then the other parameters at later time.


For example, we can partially apply the function which we created above using currying.


Scala> val part = sum(54)_


This will return us a function object called part.


We can now pass the parameter which we held back as shown below.


Scala> part(6)


The result here will be 60, which is the sum. We shall look at these concepts in our lab exercise to understand better.


That’s all in theory for this chapter. Let us now proceed to our lab exercises.






























AIM


The aim of the following lab exercises is to write and use the user defined functions in Spark applications.
The labs for this chapter include the following exercises.
* Defining Currying functions
* Using partially applied functions
* Writing User Defined Function
* Writing Untyped UDAF
* Using Untyped UDAF
* Typed UDAF


We need the following packages to perform the lab exercises: 
* Java Development Kit
* Scala
* Spark




















LAB EXERCISE 8: USER DEFINED FUNCTIONS
  

Writing UUDAF


Using UUDAF
Typed UDAF


	  


1. Defining Currying functions
2. Using partially applied functions
3. Writing User Defined Function
4. Writing Untyped UDAF
5. Using Untyped UDAF
6. Typed UDAF














  
  

Writing UUDAF


Using UUDAF
Typed UDAF
Task 1: Defining Currying Functions


Before we start writing custom functions, let us look at currying functions. We shall be working in the Scala shell for this and the next task.


Step 1: Open the terminal and fire up the Scala shell by simply typing in Scala in the terminal. You should see the Scala prompt as shown in the screenshot below.


$ scala


  



Step 2: Let us now define a simple currying function with two parameter groups as shown below to understand the concept of curried functions.


scala> def sum(x: Int) (y: Int): Int = {
| x + y
| }


  

  
  

Writing UUDAF


Using UUDAF
Typed UDAF


We have defined a function called sum which adds two numbers. Instead of passing the parameters as one group, we have curried the parameters in two parameter groups. This will help us with partially applied functions.


Step 3: We can also define currying functions with multiple parameters inside each parameter group as shown below.


scala> def sumProd(a: Int, x: Int) (b: Int, y: Int): Int = {
        | a * b + x * y
        | }


  



Step 4: We can also define a currying function in such a way that we can transform a function which takes two or more parameters into a function that takes only one parameter.


scala> def prod(a: Int) = (b: Int) => a * b


  

  
  

Writing UUDAF


Using UUDAF
Typed UDAF


As you can see from the screenshot above, we have declared a prod function which only takes one parameter a and returns another function which in turn takes another parameter b and returns the result.


Step 5: We can simply pass the arguments with each argument inside a paranthesis as shown below.




scala> prod(54)(22)


  



Task 1 is complete!
Task 2: Using partially applied functions


Let us now see how we can use the curried functions and apply them partially.


Step 1: We have created a sum function in step 1 of previous exercise. Let us use that function and partially apply the parameters for that function.


scala> val sumObj = sum(6)_ 


This will return us a function object as shown in the screenshot below.
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  



The _ is used as a placeholder for the parameter we are holding back. It indicates the compiler that we are partially applying a function. 


Step 2: We can then use the function object later to pass the parameter which we held back as shown below.




scala> sumObj(5)


  





Step 3: Similarly, let us partially apply the sumProd function which we created in the step 2 of previous task.


scala> val sumProdObj = sumProd(5, 6)_
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  



We can then pass the held back parameters at a later time as shown below.


scala> sumProdObj(7, 8)


  



The result is as shown in the screenshot above.


Step 4: We have also created a prod function in the previous task. You can also apply partial functions on that function as well. Please complete this step by yourself as a lab challenge.


Task 2 is complete!










  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF
Task 3: Writing User Defined Function


Now that we have learned about curried and partially applied functions, let us look how to write a user defined function. We will be writing a user defined function to decrease the rating of each movie by 0.5. We will be calling the UDF by using both Spark SQL and Dataframe APIs.


Step 1: Download the ratings_head.csv file from the URL below. This file contains four columns: userId, movieID, rating and timestamp.


ratings_head.csv - http://bit.ly/2X3r2wb


Please save this file in IdeaProjects/Spark/chapter_9 folder.


Step 2: Open IntelliJ IDE, create a new Scala object and name it decrRatingUDF. We shall be using the following imports in our application.


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._


Next, let us define our user defined function using val keyword instead of using def keyword.


val decrUDF = udf((input: Double) => input - 0.5)


The syntax to define a function using val is a bit different than what we have been using so far with def function. Here we are simply assigning a function literal to an immutable variable. Also, we haven’t specified the return type for the function as we can make use of Scala type inference to take care of the return type.




  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


 We are then passing the function literal inside the udf function. The udf  function takes a column as parameter and returns a column. Since we will be passing the entire column of our dataset Ratings as input to the decrUDF function in the application, we are using this udf function.


There are not many differences when it comes between val and def keyword to define functions. Please check the link in references section to learn more about val vs def.  


The program at this point of time should look like the screenshot as shown below.


  



Step 3: Let us now declare a case class for the dataset we are about to load as shown below.


case class Ratings(userId: Int, movieID: Int, rating: Double, timeStamp: String)


Next, define the main function, create a Spark Session and load the file as dataset as shown below.




  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


def main(args: Array[String]): Unit = {

 val spark = SparkSession
   .builder()
   .master("local[*]")
   .appName("Ratings Decrement UDF")
   .getOrCreate()


Make sure to import the implicits before you load the file as dataset.




import spark.implicits._

val ratings = spark
 .read
 .format("csv")
 .options(Map("InferSchema" -> "true", "header" -> "true"))
 .load("chapter_9/ratings_head.csv")
 .as[Ratings]


The program should now look something like the screenshot below.
























  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  
 


Step 4: Let us call the UDF decrUDF using the DataFrame API using the select method, as shown below.


val ratingDecDf = ratings.select($"*", decrUDF($"rating").as("ratingDec"))


Here, we are selecting all the columns from our dataset and then adding one more column called ratingDec, which is obtained by applying our UDF on rating column.


Let us finally call the show method to check the output.


ratingDecDf.show()




  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


The output should be displayed as shown in the screenshot below, when you run the program.


  



Please see that we need not register the UDF using the udf.register method when we have declared a function literal within the udf function. However, we must register our UDF when he have not used the function literal and defined the UDF using the def keyword. We shall look at this in the next few steps.


Step 5: Let us now apply UDF inside the Spark SQL query. You may either choose to create a new Scala object to apply the UDF inside Spark SQL query or within this section. 


We have used the same Scala object to apply the UDF using Spark SQL.


Step 6: Let us now define a function to decrease the rating by 0.5 using the def keyword. Please define this function outside the main function as shown in the screenshot below
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


def decrUDF2(input: Double): Double = {

 input - 0.5
}


  

Step 7: Let us now register this new UDF by using the partially applied function as shown below. We have used the underscore placeholder to specify that this is a partially applied function for which the parameter will be passed later in the program.


spark.udf.register("decrUDF2", decrUDF2 _)


Let us also create a temporary table using createOrReplaceTempView function as shown below. We can then run our SQL queries over this table.


ratings.createOrReplaceTempView("ratings")


val ratingDecDf = spark.sql("select *, decrUDF2(rating) as ratingDec from ratings")


The program should look something like the screenshot shown below.




  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  



Step 8: Let us finally call the show method and check the output.


ratingDecDf.show()


The following output is shown.


  

With this we have written, registered and used a UDF.


Task 3 is complete!
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF
Task 4: Writing Untyped UDAF


In the previous task, we have implemented a UDF. In this task let us implement an Untyped UDAF. The Untyped UDAF is used with DataFrames. UDAFs are a kind of bit complex when compared to UDFs. Let us write a UADF function to calculate the average. The average function is a built-in function available in Spark, but let us see this just to understand the process of writing a UADF.


While we write a UADF, we need to implement few methods similar to what we did in Lab exercise 5, Task 5 Implementing Custom Accumulators.


Step 1: We shall be using the same file ratings_head.csv which we have used in the previous task. Let us use the average UDAF which we are about to implement to calculate the average rating per user.


ratings_head.csv - http://bit.ly/2X3r2wb


Please make sure this file is available in IdeaProjects/Spark/chapter_9 folder.


Step 2: Create a new Scala object and name it averageUDAF. Next, we will need the following imports to implement a UADF. We haven’t used some of these imports so far. We shall learn about them as we continue with our program.


import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SparkSession}


  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF
Now that we have the required imports, we need to extend our object to inherit UserDefinedAggregateFunction as shown below.


object averageUDAF extends UserDefinedAggregateFunction {


We have used the import org.apache.spark.sql.expressions. UserDefinedAggregateFunction so that we can inherit its methods in our program.


The program at this point should now look like the screenshot below.


  



Please ignore the error you see for object name. This will go away once we implement all the required methods for UDAF.


Step 3: The first method is to specify the inputSchema i.e., the data types of input parameters we will be passing for this UDAF.


def inputSchema: StructType = StructType(Array(StructField("inputColumn", DoubleType)))






  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF
The inputSchema method returns a StructType. The inputColumn is of DoubleType as the ratings column is of type Double and is enclosed in an array of StructField, which is in turn enclosed in StructType. We are assigning schema to the input by using the StructType and StructField methods as this is an Untyped UDAF.


Step 4: Next, we have to specify the bufferSchema method. The bufferSchema tells how the data is being aggregated when the tasks are working inside the executors.


To understand this better, consider we have ratings column with four rows as shown below.


Ratings
2.5
5.0
3.5
4.5


When we perform the average, we first have to compute the sum of all the rows in a column and their count. The sum divided by count gives the average. So, the task which processes the average of rows of the column has to store the intermediate sum and the count of records in a buffer.


The buffer is first initialized as (0, 0) implying the sum and count of ratings respectively. So, the task reads the first row and updates the buffer values as (2.5, 1) by adding the initial values where 2.5 is sum and 1 is the count. Next, when the task reads second row of ratings column, the buffer will update the values as (7.5, 2). The buffer will then be updated as (11.0, 3) for third row and (15.5, 4) for fourth row. Therefore, we need to provide the data type for the count and sum of buffer. We do this using the bufferSchema method as shown below.
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


def bufferSchema = StructType(Array(StructField("sum", DoubleType),StructField("count", LongType)))


We are specifying the bufferSchema as we did with the inputSchema using StructType and StructField. The sum is of DoubleType and count is of LongType.


Step 5: Let us now specify the data type of return value using the dataType method. Since the average which returns is of type Double, we specify the DoubleType as shown below.


def dataType: DataType = DoubleType


Step 6: Next, we need to specify if the function always returns the same output on identical input using deterministic method. This is true  by default.


def deterministic: Boolean = true


The program at this point should look like the screenshot shown below.


  



Step 7: Now that we have set our input, buffer and output schema, we have to initialize our buffer (0, 0) using the initialize method.
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


def initialize(buffer: MutableAggregationBuffer): Unit = {
 buffer(0) = 0.00
 buffer(1) = 0L


The initialize method takes buffer of type MutableAggregationBuffer and returns nothing. The MutableAggregationBuffer as the name suggests is mutable and is used for aggregation purposes. The two columns in the buffer sum and count are initialized to 0. The first buffer is of type Double and second is of type Long.


Step 8: At this point of time, we have specified schema for input, buffer and output. We have also initialized the buffer with zero. We now have to write the logic so that tasks know how to update the buffer.


We do this using the update method as shown below.


def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
 if (!input.isNullAt(0)) {
   buffer(0) = buffer.getDouble(0) + input.getDouble(0)
   buffer(1) = buffer.getLong(1) + 1
 }
}


The update method takes two parameters. One is the buffer which we have initialized and the actual input (which is ratings in our case). The first buffer simply adds the ratings and the second buffer increments the count by 1 until all the records are processed. This update method is applied to every task processing this job. After the update method completes processing, there will be a final output buffer for each task. All we have to do is to merge the output of each task, which is what we are going to do in the next step.


  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


Step 9: To merge the output of all the tasks, we use the merge method as shown below.


def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
 buffer1(0) = buffer1.getDouble(0) + buffer2.getDouble(0)

 buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
}


The merge method takes two buffers i.e., two outputs from each task. It simply adds two outputs and stores them back in buffer1. This is performed for all the buffers.


Step 10: At this point of time, we have one single output from merge method. It contains the sum of all records and their count. All we have to do is write a logic to find out the average. The average is nothing but the sum divided by total count. This is implemented using the evaluate method.


def evaluate(buffer: Row): Double = buffer.getDouble(0) / buffer.getLong(1)
With this we have completed writing our UDAF. The error below the object name should be gone by now.  It should look like the screenshot below.












  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  



Task 4 is complete!
























  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF
Task 5: Using Untyped UDAF


Now that we have successfully written our Untyped UDAF, let us use it to find the average rating per user.


Step 1: Let us create a new object avgRatingUDAF within the UDAF which we have written in the previous task. Next, write the main function and also create a Spark Session object as shown below.


object avgRatingUDAF {

 def main(args: Array[String]) {

   val sparkSession = SparkSession.builder
     .master("local[*]")
     .appName("Average Rating UDAF")
     .getOrCreate()


The program should now look like the screenshot as shown below.




















  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  



Step 2: Let us now register the UDAF with Spark.


sparkSession.udf.register("averageUDAF", averageUDAF)


  



Step 3: Let us now read the file and create a temporary view on the DataFrame to write SQL queries.
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


val ratings = sparkSession.read
        .format("csv")
        .options(Map("InferSchema" -> "true", "header" -> "true"))
 .load("chapter_9/ratings_head.csv")

ratings.createOrReplaceTempView("ratings")


  



Step 4: Let us finally use the UDAF in our SQL query as shown below.


val average = sparkSession.sql("SELECT userId,  averageUDAF(rating) AS avgRating FROM ratings GROUP BY userId")


Let us check the result using the show method as shown below


average.show()


The following output is shown.
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  

We have successfully written a UDAF, registered and used it in the Spark application.


Task 5 is complete!
Task 6: Typed UDAF


Finally, let us write and use a Typed UDAF. The Typed UDAF is used on Datasets where we assign the schema using the case class making it type safe. Let us write a Typed UDAF which calculates the average of ratings as we did in the previous couple of tasks. The methods which we have used in the Untyped UDAF are different than the methods in Typed UDAF.


Step 1: We shall be using the same file ratings_head.csv which we have used in the previous task. Let us use the average UDAF which we are about to implement to calculate the average of all the ratings.
  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


ratings_head.csv - http://bit.ly/2X3r2wb


Please make sure this file is available in IdeaProjects/Spark/chapter_9 folder.


Step 2: Create a new Scala object and name it averageTypedUDAF. Next, we will need the following imports to implement a UADF. We haven’t used some of these imports so far. We shall learn about them as we continue with our program.


import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.{Encoder, Encoders, SparkSession}


Next we have to declare the case classes to specify schema for both input and buffer. However, we need not use StructType here as we did with Untyped UDAF. As we will be loading the input as Dataset and not DataFrame, the schema is associated with the case class.


case class Ratings(userId: Int, movieID: Int, rating: Double, timeStamp: String)
case class Average(var sum: Double, var count: Long)


The first case class Ratings specifies the input schema and the second case class Average specifies the buffer schema. Please note that we have used var keyword to define mutable fields in Average buffer as the buffer keeps on updateing when task process each row, as explained in previous tasks.


Now that we have the required imports and case classes defined, we need to extend our object to inherit Aggregator abstract class as shown below.


object MyAverageAggregator extends Aggregator[Ratings, Average, Double] {


  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


The Aggregator abstract class takes three parameters. They are the input, buffer and output type. The input is Ratings, the buffer is Average and the output type is Double.


The program should now look like the one as shown in the screenshot.


  



Please ignore the error you see for object name. This will go away once we implement all the required methods for UDAF as in the previous tasks.


Step 3: Let us now implement the methods from the Aggregator abstract class. The first method is the zero method which initializes the buffer to zero. This is similar to initialize method which we have used in the previous task.


def zero: Average = Average(0, 0L)


Step 4: Next, we have to implement the reduce method which is similar to update method used in the previous task. The reduce method provides the logic to specify how the tasks should process the rows and columns of dataset.


  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


def reduce(buffer: Average, rat: Ratings): Average = {
 buffer.sum += rat.rating
 buffer.count += 1
 buffer
}


The reduce method takes the buffer which is of type Average and the input ratings. The buffer is then updated for each row in the rating column along with the count. Once all the records are processed, the final value of the buffer for each task is returned.


At this point, we have buffer outputs of each task. We now have to merge them to get the final sum and count of all the records.


Step 5: We now have to implement the merge function so that the buffer outputs from all the tasks are merged.


def merge(b1: Average, b2: Average): Average = {
 b1.sum += b2.sum
 b1.count += b2.count
 b1
}


This function simply adds the sum and counts of all the buffers and returns back the buffer.


The program at this point should look like in the screenshot below.






  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


  



Step 6: Next, similar to the evaluate method in the previous task, we have to implement the finish method. The finish method contains the logic to compute the average i.e., dividing the sum with count.


def finish(reduction: Average): Double = reduction.sum / reduction.count


Step 7: We now have to implement the encoders for buffer and output values using the bufferEncoder and outputEncoder. These encoders are required for serialization purposes to translate between the Scala and Spark types.


def bufferEncoder: Encoder[Average] = Encoders.product
def outputEncoder: Encoder[Double] = Encoders.scalaDouble
  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


The error below the object name should have been gone by now as we have implemented all the methods required to create a typed UDAF.


The program should now look like the one shown in the screenshot.


  



Step 8: Let us now use this typed UDAF. Create a new object within the program as shown in the screenshot and name it avgTypedUDAF. Define the main function and also create the Spark Session.




  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF


object avgTypedUDAF {

 def main(args: Array[String]) {

   val sparkSession = SparkSession.builder
     .master("local[*]")
     .appName("Average ratings Typed UDAF")
     .getOrCreate()


Next, load the file.


val ds = sparkSession.read
        .format("csv")
        .options(Map("InferSchema" -> "true", "header" -> "true"))
         .load("chapter_9/ratings_head.csv")
        .as[Ratings]
The program should look like the screenshot below.


  



  
  
  
  
  
  
  

Writing UUDAF


Using UUDAF
Typed UDAF
Step 9: Let us now use of UDAF we wrote in this task. We have to call the toColumn method on our UDAF and give it a name using the name method as shown below.


val averageRating = averageTypedUDAF.toColumn.name("averageRating")


Let us now use the select method for the UDAF as shown below.


val avg = ds.select(averageRating)


Finally, let us call the show method and run the program.


avg.show()


The output which calculates average of all the ratings is as shown in the screenshot below.
 
  



This completes the Typed UADF task.


Task 6 is complete!




SUMMARY


In this chapter we have looked at User Defined Functions, which is the custom functions Spark. We have learned what UDFs and UDAFs are, why they are required and when they are used. We have also learned Scala programming concepts called function currying and partially applied functions.
In the labs, we have had our hands on Scala function currying and partially applied functions. We have then used UDFs and two types of UDAFs to process data.


















































REFERENCES


* https://spark.apache.org/
* https://alvinalexander.com/scala/fp-book-diffs-val-def-scala-functions
































































CHAPTER 10 : FILE FORMATS
Theory


In this chapter, let us learn about the various file formats we can use with Spark. There are a wide variety of file formats supported by Spark and each of them have their own advantages and disadvantages based on our requirement. We have been dealing with csv files so far in the course of this book to load and store data. Let us now shift the gears and learn more about the different types of file formats and how we can use them to our advantage.


We can read and write files of various formats to spark using Spark APIs. They are the RDD APIs and DataSource (DataFrame or Dataset) APIs. Let us look at the various file formats we can use with Spark and then in the lab exercises, let us see how we can read and write these file formats using the Spark APIs.
DataSource API
Before we see the various file formats we can process with Spark, let us look at the DataSource read and write API structure in detail.
Reading Data
We can easily read data to Spark from file systems such as HDFS, Amazon S3, local file system etc by specifying the path to the file(s). All the methods to read various files are implemented within the DataFrameReader class. We can access this class by calling read on sparkSession object. Next, we need to specify the file format using format method if our input format is other than parquet, as it is the default format in Spark. We can then optionally specify options using option method or options method with multiple options inside a Map object. We can also optionally specify the schema using the schema method. Finally, using the load method we specify the input path for the file. The syntax to read data is as shown below.


sparkSession.read
.format(“_”)
.option(“key”, “value”)
.schema(“_”)
.load(“_”)


The _ in the syntax represents the appropriate values. For example, format method takes the format of the file like csv, json etc.


Let us now look at the different read modes we can use as options while reading data.
Read Modes
While we read data, there could be times where we encounter malformed or corrupt records. We can use the read modes within the option method to specify how we would like to deal with them. There are three types of read modes.


permissive
	This mode sets all fields to nulls when it encounters a malformed record and all such records are placed in a new field in a column called columnNameOfCorruptRecord. This is the default mode.
	dropMalformed
	This mode simply drops or removes the entire row when it encounters malformed records. 
	failFast
	This mode fails the job and throws an exception as soon as it encounters malformed records.
	

We can set this mode using the option method as shown below.


sparkSession.read
.format(“csv”)
.option(“mode”, “dropMalformed”)
.option(“inferSchema”, “true”)
.option(“header”, “true”)
.schema(newSchema)
.load(“/usr/local/some.csv”)
Writing Data
Similar to reading data, we can also write data to a filesystem. All the methods to write various files are implemented within the DataFrameWriter class. We can access this class by calling write on the DataFrame we would like to save. We then have to specify the format using the format method. Next, as we specified options with reading data, we can also specify the options using option or options method. Finally we use the save method to save the file by providing a path.


The syntax to read data is as shown below.


dataFrame.write
.format(“_”)
.option(“key”, “value”)
.save(“_”)


Let us now look at the different save modes we can use as options while writing data.
Save Modes
Unlike Read modes, save modes do not deal with malformed records, but instead these modes provide us with options on what to do if there is already data available in the path where we are intending to save data. There are four save modes available to write the data.


append
	This mode simply appends the output file to the list of files that are already available in the save location.
	overwrite
	When overwrite mode is used, Spark will overwrite any existing data available in the save location. Please use caution while using this mode. Only use it if the data already available is not required.
	errorIfExists
	This mode throws an error and terminates the write job if there is any data present in the save location. This is the default mode used by Spark when writing data to a filesystem.
	ignore
	This mode simply ignores any data that already exists in the save location. The data will not be saved and nothing will be changed or removed in the existing data.
	

This is the structure associated with reading and writing data using the DataSource API. Let us now look at the file formats.


Text Files
Text files can be read and saved using Spark’s DataSource API. There are two methods available in the DataSource API to read text files. They are the text and textFile method. Both the methods are used to read text files but the only difference is that the text method returns a DataFrame with partition columns, if any, while textFile method returns a Dataset ignoring partition columns. An example of reading a text file is as shown below.


spark.read.text(“/usr/local/files/sample.txt”)


or


spark.read.textFile(“/usr/local/files/sample.txt”)


We can also read a text file by specifying the format in the format method as shown below.


spark.read.format(“text”).load(“/usr/files/sample.txt”)


We can also save the DataFrame as text using the DataFrameWriter class. However, we must make sure that there is only one column of type String while saving, or else the write will fail. Each row in the DataFrame becomes a line in the output file. An example of writing a text file is as shown below.


df.write.text(“usr/local/output/out.txt”)


We can also write a text file by specifying the format in the format method as shown below.


df.write.format(“text”).save(“/usr/local/out.text”)
CSV Files
Similar to the text files we can also save the CSV files using the DataSource API. Each line in the CSV file represents a single record. CSV contains a large number of options when it comes to processing them. With these options we can efficiently handle tricky scenarios such as corrupt records etc. 


CSV files support compression, are human readable and are splittable. However, CSV files are not nestable and cannot hold complex data structures. The following is an example to read a CSV file.


spark.read
.format(“csv”)
.option(“inferSchema” -> “true”)
.option(“header” -> “false”)
.load(“/usr/local/files/sample.csv”)


Writing to CSV files from a DataFrame is also straight forward as shown below.


df.write
.format(“csv”)
.mode(“errorIfExists”)
.save(“usr/local/output/out.csv”)
JSON
JSON, which stands for JavaScrit Object Notation, is also one of the popular file formats around. JSON is compressable, splittable and human readable. It is also nested and supports complex data structures. With Spark, we can load a single line JSON and also a multi-line JSON. All we need to do is specify an option for multi-line JSON. However, it is recommended to use single line JSON whenever possible. This makes it easier for appends when you are writing it back to JSON format. Similar to CSV there are a wide variety of options to read and write data.


The following is an example of loading a JSON.


spark.read
.format(“json”)
.option(“multiLine” -> “true”)
.schema(“userSchema”)
.load(“/usr/local/files/sample.json”)
        
We can also write to a JSON as shown below.




df.write
.format(“json”)
.mode(“overWrite”)
.save(“usr/local/output/out.json”)
Parquet Files
Parquet is a widely used file format in Spark because of the optimizations it provides for analytical processing and is considered as one of the most efficient file formats for Spark. Parquet is column-oriented data structure and is the default file format in Spark. When no format is specified, Spark automatically processes them as Parquet. Parquet is compressable, splittable but is not human readable. Parquet supports complex data structures. It can be easily processed when columns are of struct, map or array type. This is not possible with JSON and CSV files.


Parquet has only a couple of options while reading and writing the data. The following is an example to read a Parquet file.


spark.read
.format(“parquet”)
.load(“/usr/local/files/sample.parquet”)


We can also read without specifying the schema as shown below.


spark.read
.load(“/usr/local/files/sample.parquet”)


The following is an example to write data.


df.write
.format(“orc”)
.mode(“errorIfExists”)
.save(“usr/local/output/out.parquet”)
ORC Files
ORC is yet another columnar file format which stands for Optimized Row columnar. There are no options for ORC files because Spark processes ORC file format very efficiently. Both ORC and Parquet are similar, but Parquet is very much optimized for Spark and ORC is optimized for Hive. 


ORC supports complex data structures, is splittable and can be compressed. ORC is not human readable. The following shows an example of reading an ORC file.


spark.read
.format(“orc”)
.load(“/usr/local/files/sample.orc”)


These are the major file formats used by Spark. Let us look at these in action during the lab exercises.
RDD API
Let us now look at reading data using RDD API.
Text Files
We can easily read and write plain-text files using Spark’s RDD API. The text files can be compressed, splittable and are human readable. However, text files are not very efficient when compared to other file formats. Please note that the splitting might not always be possible when certain compression codecs are used.


In the RDD API, we use the textFile method on SparkContext object to read the text file and saveAsTextFile method to save the output as text file. We can save the text files using a compression codec by simply passing the name of the compression codec as a parameter to saveAsTextFile method.


When a text file is read using textFile method, each line becomes a record in that RDD. We can also read multiple text files in a directory as a single RDD using wholeTextFiles method. This creates a paired RDD with the names of individual files as keys and entire content of those individual files as values. This is very useful in scenarios where there are many small files. We can read them all in one go using wholeTextFiles method. 


The syntax to read a text file is


sparkContext.textFile(“/path/to/text/file.txt”)


We can read multiple text files as shown below.


sparkContext.wholeTextFiles(“path/to/files”)


Similarly, we can save an RDD as text file as shown below.


rdd.saveAsTextFile(“/path/to/save”)
Sequence Files
Sequence file is the popular Hadoop file format which contains keys and values in binary form. We can read Sequence files using the sequenceFile method on the SparkContext object, and write them using the saveAsSequenceFile method on the RDD.


The syntax to read the file is


sparkContext.sequenceFile(“/path/to/seq/file”, classOf[keyDataType], classOf[valueDataType])


As you can see in the syntax above, we also have to specify the data types of keys and values of the Sequence file using classOf[] object.


The syntax to write the file is


rdd.saveAsSequenceFile(“/path/to/seq/output”)


Please note that we cannot directly read or save a Sequence file using the DataSource API. We can however convert the RDD loaded from Sequence file to a DataFrame using the toDF method. To save a DataFrame to Sequence File, we must create a paired RDD from a DataFrame and then use the saveAsSequenceFile method.
Hadoop Files
We can also read the output of Hadoop MapReduce jobs to Spark using the hadoopFile and newAPIHadoopFile methods. The hadoopFile method is the old Hadoop API while newAPIHadoopFile method is the new Hadoop API.


The syntax to read a Hadoop file using the old Hadoop API is as follows.


sparkContext.hadoopFile[keyDataType, valueDataType, inputFormatClass](“/path/to/file”)


As you can see from the syntax above we have to specify the data types of both keys and values and also the Hadoop input format class for the input file.


An example of this is as follows.


sparkContext.hadoopFile[Text, LongWritable, TextInputFormat](“/usr/local/files/out/part-00000”)


Please note that we should import the required classes for data types and input formats as required. 


The syntax to read a Hadoop file using the new Hadoop API is as follows.


sparkContext.newAPIHadoopFile(“/path/to/file”, classOf[inputFormatClass], classOf[keyDataType], classOf[valueDataType], conf)


In the new Hadoop API, we provide the path to input file or comma separated list of files, Hadoop input format class, key and value data types and finally the conf object. We can use the conf object to specify properties such as delimiters.


An example of this is as follows.


sparkContext.newAPIHadoopFile(“/usr/local/output/part-0000”. classOf[TextInputFormat], class[LongWritable], class[Text], conf)


Please note that we should import the required classes for data types and input formats as required. 


Do not worry if this doesn’t make sense now. We shall look at this in our lab exercise and everything will start to make sense.




AIM


The aim of the following lab exercises is to read and write various file formats in Spark applications.
The labs for this chapter include the following exercises.
* Text Files
* CSV Files
* JSON Files
* Parquet Files
* ORC Files
* Hadoop and Sequence Files


We need the following packages to perform the lab exercises: 
* Java Development Kit
* Scala
* Spark




















LAB EXERCISE 9: USING FILE FORMATS
  

Parquet Files


ORC Files
Hadoop & Seq Files


	  


1. Text Files
2. CSV Files
3. JSON Files
4. Parquet Files
5. ORC Files
6. Hadoop and Sequence Files














  
  

Parquet Files


ORC Files
Hadoop & Seq Files
Task 1: Text Files


Let us begin this lab exercise by reading and writing text files in Spark using both DataSource API and RDD API. For ease and simplicity let us perform these tasks in Spark Shell.
RDD API
Let us first read and write text files to Spark using the RDD API.


Step 1: Download a text file from the URL below and save it in the path IdeaProjects/Spark/chapter_10/. Please create new directories as required. The IdeaProjects folder is present in your Home folder.


treasure_island.txt - http://bit.ly/2LBFLtt


Step 2: Open the terminal and fire up the Spark shell. Let us load the text file using the code below.


scala> val textData = sc.textFile(“IdeaProjects/Spark/chapter_10/treasure_island.txt”)


This will read the data and create an RDD[String] as shown below. We can read data from any filesystem such as HDFS, AWS, Azure etc, this way just by providing the complete path or fully qualified URL of that particular filesystem. We can then perform all the RDD operations or convert to a DataFrame or Dataset as required.


Step 2: Let us now write this back to the file system as shown below.




  
  

Parquet Files


ORC Files
Hadoop & Seq Files
scala> textData.saveAsTextFile(“IdeaProjects/Spark/chapter_10/output”)


  



Let us now check if the save was successful. For that open a new terminal and check the contents using the command below.


$ cat IdeaProjects/Spark/chapter_10/output/part-00000


  



Step 3: Now, let us see how we can load multiple files in a directory using the wholeTextFiles method. For this please download the files available in the URL below.


books - http://bit.ly/2kupo5v
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


The folder should contain a total of 6 files. Please save them all in the IdeaProjects/Spark/chapter_10/ folder. You should now have the 6 files in this path IdeaProjects/Spark/chapter_10/books.


Step 4: Let us read these files using the wholeTextFiles method. This will read all the files present in books folder. Please switch back to Spark-shell and read the files using the code below.


scala> val textFiles = sc.wholeTextFiles(“IdeaProjects/Spark/chapter_10/books”)


This will return you a RDD[String, String] which is a paired RDD as shown below.


  



This paired RDD contains the name of the files as keys and the entire content of files as values.


Step 5: Let us simply print the file names by using the keys method on textFiles RDD as shown below.


scala> textFiles.keys.collect.foreach(println)












  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



We can also get the values by using the values method. We can also perform all the operations which you can apply on paired RDDs such as mapValues, reduceByKey, sortByKey etc.


This paired RDD can again be saved to filesystem using the saveAsTextFile method as usual.


Let us now use text files with the DataSource API.
DataSource API
Let us now load and save text files using the DataSource API.


Step 1: Download a text file from the URL below and save it in the path IdeaProjects/Spark/chapter_10/. Please create new directories as required. The IdeaProjects folder is present in your Home folder.


ratings.txt - http://bit.ly/2lJcCQF


Step 2: Let us now load this file to Spark using the Spark Shell with the following code.


scala> val ratings = spark
.read
.text(“IdeaProjects/Spark/chapter_10/ratings.txt”)
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


Let us now check if the read was successful by calling the show method on the ratings dataframe.


scala> ratings.show()


  



We can also use the textFile method as shown below.


scala> val ratings = spark
.read
.textFile(“IdeaProjects/Spark/chapter_10/ratings.txt”)


Using textFile ignores the partition directory names.


Step 3: Let us write this back to the filesystem as shown below.


scala> ratings
.write
.text(“IdeaProjects/Spark/chapter_10/output1”)


Please make sure that you only have one string column while you save the text file successfully. Also, make sure the output directory (in this case, output1) doesn’t exist before you perform the write action.
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


Step 4: Use the following command to check if the save was successful. You will have to use the new terminal to run this command, as this won’t be executed in Spark shell.


$ cat IdeaProjects/Spark/chapter_10/output1/part*


You should see the file saved as shown below.


  





Task 1 is complete!
















  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files
Task 2: CSV Files
Let us now look at reading and writing CSV files to Spark. We have been reading and writing CSV files in the previous chapters. However, let us also see some of many options that can be used while reading and writing CSV files.


Step 1: Download the file ratings.csv from the URL below and save it to the IdeaProjects/Spark/chapter_10 folder.


ratings.csv - http://bit.ly/2L8IEBS


Each line of this file represents one rating of one movie by one user, and has the following format: userId, movieId, rating, timestamp.


Step 2: Let us now read this file to Spark from Spark shell using few options.


scala> val data = spark
.read
.format(“csv”)
.option(“InferSchema”, “true”)
.option(“header”, “false”)
.option(“nullValue”, “Null”)
.load(“IdeaProjects/Spark/chapter_10/ratings.csv”)


We have used a new option here which is called NullValue. This will replace all the null values with the provided string, which is Null in this case. The default is “”. Please check the references section for all the options that can be used while reading or writing CSV files. All the options can be used in this way or inside a map object.


We can then call the show method as shown in the screenshot below to check if it was successful.
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



Step 3: We can also use the modes we have learned in our theory. Let us see an example.


scala> val dataNew = spark
.read
.format(“csv”)
.options(Map(“InferSchema” -> “true”
, “header” -> “false”
, “nullValue” -> “Null”
, “mode” -> “FAILFAST”))
.load(“IdeaProjects/Spark/chapter_10/ratings.csv”)


scala> dataNew.show()
















  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



Step 4: Let us now write this dataframe back to the filesystem in CSV format.


scala> dataNew.write
.format(“csv”)
.option(“sep”, “|”)
.save(“IdeaProjects/Spark/chapter_10/output2”)


Here, we have used an option called sep which replaces the delimiter from comma to a pipe.


Step 5: Let us check if the save was successful as we desired.


$ cat IdeaProjects/Spark/chapter_10/output2/part*














  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



Task 2 is complete!
Task 3: JSON Files
Similar to previous tasks, let us read and write JSON files. We shall be reading two kinds of JSON files. One is a single line JSON and other is the multi line JSON.


Step 1: Download the file example_1.json from the URL below and save it to the IdeaProjects/Spark/chapter_10 folder.


example_1.json - http://bit.ly/2lRFI06


Step 2: The following code is used to read the single line JSON file.


scala> val jsonData = spark.read
.format(“json”)
.option(“multiLine”, “false”)
.load(“IdeaProjects/Spark/chapter_10/example_1.json”)




Step 3: Let us check if we were able to load the JSON file successfully.


scala> jsonData.show()
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



Step 4: Let us now load the multi line JSON file. Download the file example_2.json from the URL below and save it to the IdeaProjects/Spark/chapter_10 folder.


example_2.json - http://bit.ly/2lL3IST


Step 5: The following code is used to read the single line JSON file.


scala> val multiJson = spark.read
.format(“json”)
.option(“multiLine”, “true”)
.load(“IdeaProjects/Spark/chapter_10/example_2.json”)


  



  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


Step 6: Let us now write this dataframe to the filesystem.


scala> multiJson.write
.format(“json”)
.save(“IdeaProjects/Spark/chapter_10/output3”)


  



You can check the output by running the following command from a new terminal.


$ cat IdeaProjects/Spark/chapter_10/output3/part*


  



Task 3 is complete!


Task 4: Parquet Files
Parquet is Spark’s default file format. Let us read and write Parquet files in Spark.


Step 1: Download the file userdata1.parquet from the URL below and save it to the IdeaProjects/Spark/chapter_10 folder.


userdata1.parquet - http://bit.ly/2kfIhJ4


  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


Step 2: Let us now read this Parquet file to Spark using the code below.


 scala> val parquetData = spark
.read
.load(“IdeaProjects/Spark/chapter_10/userdata1.parquet”)


Please see that we need not mention the format here as Parquet is default file format in Spark. However, you may explicitly mention the format as we did in the previous tasks if you desire so.
You should see the following output when you call the show method on the dataframe.


  





Step 3: Let us write this back to the filesystem in Parquet format.


scala> parquetData
.write
.save(“IdeaProjects/Spark/chapter_10/output4”)
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


We can check if the save was successful by simply running the cat command from a new terminal as shown below. However, you will not be able to read the file correctly as it is not human readable.


$ cat IdeaProjects/Spark/chapter_10/output4/part*


  



Step 4: We can also save a parquet file using compression as shown below.


scala> parquetData
.write
.option(“codec”, “gzip”)
.save(“IdeaProjects/Spark/chapter_10/output5”)


Task 4 is complete!
Task 5: ORC Files
Step 1: Download the file userdata1_orc from the URL below and save it to the IdeaProjects/Spark/chapter_10 folder.


userdata1.orc - http://bit.ly/2kfQi0J




  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files
Step 2: Reading an ORC file is similar to what we have been doing so far through out this exercise.


scala> val orcData = spark
.read
.format(“orc”)
.load(“IdeaProjects/Spark/chapter_10/userdata1_orc”)


You should see the following output when you call the show method on the dataframe.


  



Step 3: We can now simply write to an ORC format similar to what we have been doing with other file formats so far.


scala> orcData
.write
.format(“orc”)
.save(“IdeaProjects/Spark/chapter_10/output5”)


  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


Similar to Parquet, ORC is also not human readable and you will only see gibberish data when used the cat command as shown below.


$ cat IdeaProjects/Spark/chapter_10/output5/part*


  



Task 5 is complete!
Task 6: Hadoop and Sequence Files


Let us now work with Hadoop and Sequence Files. These files are popular file formats with Hadoop MapReduce framework. These files contain key value pairs in binary format. Let us first create and write a Sequence file and then read the same sequence file.






  
  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files
Sequence Files
Step 1: Let us first create an RDD using the parallelize method as shown below.


scala> val seqRDD = sc.parallelize(List((“Ernesto”, 2000), (“Learning”, 4500), (“Lee”, “8000”)))
 
This will create an RDD[(String, Int)] as shown below.


  



Step 2: Let us now write the RDD to Sequence file format using the saveAsSequenceFile method as shown below.


scala> seqRDD.saveAsSequenceFile(“IdeaProjects/Spark/chapter_10/seqOut”)


You may run a cat command from another terminal to check if the save was successful, but the file will not be human readable as shown in the screenshot below.


$ cat IdeaProjects/Spark/chapter_10/seqOut/part*


  

  
  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files
We know that the save was successful by looking at SEQ at the beginning of the file. We can also see that the key type is of Text and the value type is of IntWritable.


Step 3: Let us now read this Sequence file we just saved. Reading Sequence files is a bit different from what we have been doing so far. While reading the Sequence file, we need to specify the key and value data types also.


scala> val seqData = sc
.SequenceFile(“IdeaProjects/Spark/chapter_10/seqOut/part-00000”
,classOf[org.apache.hadoop.io.Text]
,classOf[org.apache.hadoop.io.IntWritable])


  



Since this is a Hadoop file format, we need to specify the data types in Hadoop. We have specified the Text and IntWritable types as the types for keys and values, since our keys are of String and values are of Int.


Step 4: However, since these are Hadoop data types, we cannot access the keys directly. We need to convert them to Java data types as shown below. The job will fail if we do not convert the data types and collect.


scala> val newRDD = seqData.map
{
        case (x, y) => (x.toString, y.get())
}
  
  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  
 


As you can see from the screenshot above, we now have the RDD[(String, Int)]. We can now simply perform all the operations we usually do on RDDs. We have to use the toString method when converting from Hadoop’s Text type and the get method for other data types.


Step 5: Let us now collect the RDD and check out the results.


scala> newRDD.collect()


  



With this we have successfully written and read the Sequence files.
Hadoop Files


Hadoop files are the output of Hadoop MapReduce jobs. We can read Hadoop files with Spark and do further processing using Spark. We shall now look at both the old Hadoop API to read the output from MapReduce jobs to Spark.
  
  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


Step 1: Download the file part-r-00000 from the URL below and save it to the IdeaProjects/Spark/chapter_10 folder.


part-r-00000 - http://bit.ly/2lSqdFy


This file is the output of a Word Count MapReduce job. It contains words as keys and values as the count separated by tab.


Step 2: Before we read the file, we first need the following imports. We need to import the datatypes for both keys and values and also the input format. The keys are of type Text, values are Text and the input format is KeyValueTextInputFormat.


scala> import org.apache.hadoop.io.Text
scala> import org.apache.hadoop.mapred.KeyValueTextInputFormat
  



Step 3: Let us now read the file using the hadoopFile API as shown below. This is the old Hadoop API.


scala> val hadoopData = sc.hadoopFile[Text, IntWritable, KeyValueTextInputFormat](“/IdeaProjects/Spark/chapter_10/part-r-00000”)








  
  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



We now have an RDD from Hadoop MapReduce output. However, in order to access the key value pairs, we have to first convert them to the Java datatypes as we did with the Sequence files.


Step 4: Convert the data types from Hadoop types as shown below.


scala> val hadoopRDD = hadoopData.map
{
        case (x, y) => (x.toString, y.toString)
}


  



Step 5: Finally let us call the collect method and check the output from the RDD.


scala> hadoopRDD.collect()
















  
  
  
  
  
  
  

Parquet Files


ORC Files
Hadoop & Seq Files


  



As you can see from the screenshot above, we were successfully able to read the key value pairs from the MapReduce output.


Please try to read the data using new hadoop API as a lab challenge.


Task 6 is complete!


































SUMMARY


In this chapter we have looked at various file formats we can process using Spark. We have covered both the RDD API as well as DataSource API to read and write files from and to Spark.
In the labs, we have had our hands on using the various file formats we have learned using both the RDD API as well as DataSource API to read and write files from and to Spark.






















































REFERENCES


* https://spark.apache.org/
* https://spark.apache.org/docs/latest/sql-data-sources.html
* https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html
* https://docs.databricks.com/spark/latest/data-sources/read-csv.html
* https://docs.databricks.com/spark/latest/data-sources/read-json.html




















































CHAPTER 11 : SPARK CONFIGURATIONS & OPTIMIZATIONS
Theory


Throughout the course of this book, we have been looking at Spark features and developing Spark applications. In this chapter, we shall look at important Spark configurations and optimization techniques to process data faster with maximum efficiency. Spark is build in a view to help developers start developing and run Spark applications with the default settings as soon as Spark is installed. However, there are few scenarios where we have to modify configurations, fine tune settings and apply optimization techniques to run Spark applications faster with more efficiency.


Let us first look into Spark configurations and then at the performance optimization techniques.
Spark Configurations
There are three options through which Spark can be configured.


* Spark configuration properties
* Environmental variables
* Logging


Let us now look at them in detail and understand how they help us in configuring Spark.
Spark Configuration Properties
Almost every Spark configuration can be configured by using the SparkConf class. Whenever a new SparkContext object is created, a SparkConf object is required. These configurations can be set in three ways.


* Configure Programatically
* Set dynamically during Runtime
* Configure the default spark-default.conf file


We have been so far using configurations programatically to set application name, master etc. However, it is possible to set during runtime via command line and also set it in the spark-default.conf file. To specify configurations during runtime, we have to make a jar file of our application and refer to it using spark-sumbit tool. We can also use a Spark configuration file within the spark-sumbit tool. We shall look at this in the lab exercises and also in our next chapter while we look at running Spark in the cluster.


With all these methods to set configurations, there is a precedence structure defined in Spark that determines which method has priority. The top priority is for the configurations set programatically. Second priority is for the configurations defined using the spark-submit tool. The next priority is for the configurations in the properties file and the last priority is for the default values.


There are tons of configurations that can be set within the Spark application. Let us look at few of the mostly used configurations in Spark and understand them better. For all the Spark configurations, please check the link in References section.


Configuration Name
	Default
	Description
	spark.app.name
	None
	This property is used to specify the name of the Spark application. This will reflect in the Spark UI as well as logs.
	spark.driver.memory
	1g
	This property is used to specify the amount of available memory for driver process. The default being 1 GB. The command line syntax for this property is --driver-memory.
	spark.executor.memory
	1g
	This property is used to specify the amount of available memory for driver process. The command line syntax for this property is --executor-memory.
	spark.driver.cores
	1
	This property is used to specify the number of cores for the driver process . The command line syntax for this property is --driver-cores.
	spark.executor.cores
	1
	This property is used to specify the number of cores for each executor . The command line syntax for this property is --executor-cores.
	spark.master
	None
	This property is used to specify the cluster manager for Spark.
	spark.serializer
	org
.apache
.spark
.serializer
.JavaSerializer
	This property specifies the class that is used for serializing objects.
	spark.eventLog.enabled
	false
	This property when set to true will enable the event log which allows to view completed Spark jobs in the history server.
	

Environment Variables
The environment variables for spark can be set through conf/spark-env.sh file. The environment variables in Spark helps you set certain settings such as java location. This spark-env.sh file is not available immediately after installing Spark but can be copied from spark-env.sh.template file. 


Please check the link in references section to check all the available environmental variables that can be modified within the spark-env.sh file. As a developer, you rarely change these settings. This section is included just for information purposes. 
Logging
Log4j is used for logging in Spark. All the configuration for logging can be done using the conf/log4j.properties file. Similar to the environment variable file, log4j.properties file is also not available out of the box but can be copied using the log4j.properties.template.
Performance Optimization
As a Spark developer, we must always strive to run Spark applications as fast as possible with maximum efficiency. Although Spark is build with speed and efficiency as top priority, there are certain scenarios where we must tune the default settings and use some best practices to optimize our applications. Let us now look at few of these performance optimization techniques. These are only a few techniques to optimize your Spark application. These might not always work in every scenario. Keep experimenting by fine tuning the settings until you find the best configuration that works.
Using Datasets extensively
With what we have learned throughout this book, it is clear that Dataset APIs provide the most efficient, faster and better error handling measures. Datasets provide the best qualities of both RDDs and DataFrame APIs and so, it is recommended to use Datasets when possible. Datasets (and Dataframes) are efficient and provide increased performance using the Tungsten and Catalyst optimizations.


However, there could be senarios where we have to use RDD APIs to achieve something not possible via higher level APIs. In such cases, it is recommended to develop code in Scala or Java as the serialization here is much more optimized.
Avoiding UDF and UDAF
In order to better utilize these optimizations, it is  recommended to use UDFs and UDAFs only when they are absolutely required. Also, do not develop UDFs and UDAFs that perform more than one transformation or an action in a single UDF and UDAF. The goal should be to use built-in functions as much as possible so that we can utilize the power of Spark to the fullest.


It is also recommeded to develop UDFs and UDAFs in Scala or Java.
Data Serialization
Data serialization plays an important role in the performance of a Spark application. While the default Java serialization provided by Spark is efficient for almost all the cases, there are few cases where it is required to define a serialization for custom data types. In cases where custom data types are used, we have to define them using the Kryo serializer. Let us look at two of the Serializers provided by Spark in detail.


Java Serializer
	Java serializer is the default serialier in Spark. The objects in Spark are serialized by using the ObjectOutputStream framework. The Java Serialization works with any class that implements java.io.Serializable. It is also possible to tune the performance by extending java.io.Externalizable class. Although this serializer work with almost all the data types, it is quite slow and results in large serialized formats in many classes.
	Kryo Serializer
	Kryo Serializer is much more faster and compact when compared to the Java Serializer. The Kryo Serializer can be used with Spark to serialize custom data types. However, it does not support all the Serializable types and cannot be used unless the classes that are used in the application are registered first.


	 
Spark Memory Tuning
The memory of executors is allocated into two categories.


* Storage Memory
* Execution Memory


Storage Memory
	The storage memory has a default allocation of 60% (0.6) of the total memory. The remaining 40% of space is reserved for user data, internal Spark metadata and prevention of Out Of Memory (OOM) errors that may arise from very large datasets. This memory is used to cache or persist the datasets in memory when we call the cache() or the persist() methods. The amount of memory allocated for caching can be altered using the spark.memory.fraction setting. The Storage memory may occupy all the memory when there is no execution memory being used and vice versa. The execution memory may also evict storage memory but only to a certain extent as described below.
	Execution Memory
	The execution memory is used to store the intermediate buffers while performing shuffles, joins, sorts and aggregation operations on the data. The amount of memory, execution may evict from the Storage memory allocation can be altered using the spark.memory.StorageFraction setting. The default is 0.5 of the total allocated Storage memory. This setting ensures that the amount of blocks which are reserved for storage are never evicted.
	

This allocation of storage and execution memories make sure that when there is no caching required by the applications, the entire memory can be used by execution and if the applications require caching, the required amount of space can be reserved so that they are not evicted by the execution process. This dynamic allocation of memory helps utilize the memory efficiently in turn providing faster results. This also makes sure that the users need not worry about altering the memory allocations everytime when they require more storage or caching memory. These default settings work most of the time but there might be scenarios where you might want to change these values as per the requirement.
The following methods help tune the memory usage.


* Check the memory consumption of dataset by creating an RDD of the dataset and then look at the Storage page in the webUI. This will show the total consumption of memory by that RDD.


* The size of an object can be found using the estimate method available in SizeEstimator class. This is useful for determining the amount of heap space a broadcast variable will occupy on each executor or the amount of space each object will take when caching objects in deserialized form. This is not the same as the serialized size of the object, which will typically be much smaller.


* Use Serialization while caching the dataset in memory. Please check the Caching topic in chapter 5 for more information.


* Measure the impact of Garbage Collection by checking how often the garbage collection runs and the amount of time spent. This can be achieved by adding -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps to the Java options property in Spark Configuration. 


For example, spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps


Once this property is set and submit a Spark job, you will see the messages printed in worker’s log each time a garbage collection occurs.  Please check the Spark documentation in References for more information on garbage collection.
Level of Parallelism
Parallelism denotes the number of tasks which run parallelly on a single CPU core. It is recommened to start with three tasks per CPU core and then optimize the number, if required. The default level of parallelism can be changed using the property spark.default.parallelism.


With certain type of aggregation operations such as sortByKey, groupByKey, reduceByKey, join, etc, you might be greeted by a OutOfMemoryError. This is not because of low memory but because the working set of one of your tasks, such as one of the reduce tasks in groupByKey, was too large. By increasing the level of parallelism, we can have smaller working set of each task and thereby overcoming the OutOfMemoryError.
Levels of Data Locality
Data locality is a concept which refers to the distance between the node which contains the data and the node which contains the code to compute that data. The jobs run more faster if the data and the computation happens on the same JVM or node. It is always faster to move code to data than to move data to the code as code is smaller when compared to data.


The following are the levels of data locality from closest to farthest.


* PROCESS_LOCAL: This is the closest data locality possible where the data is available within the same JVM/Process/Executor as of code. 


* NODE_LOCAL: At this level, the data and code are present within the same node but a different process (executor). The data has to be moved between processes at this level and therefore there is a little overhead.


* RACK_LOCAL: The data and code are present in same rack but on different nodes. The data has to be moved through network at this level.


* ANY: The data at this level is not an the same rack as of the code and could be elsewhere on a different rack/cluster/data center.


* NO_PREF: This level has no preference to data locality and the data is accessed from anywhere.


Spark thrives to launch the tasks at the best data locality level with following priority: local -> node -> rack -> any. However, it is not always possible as the CPU might be busy processing another task and it has to wait sometime before it can acquire data at any other level before giving up. We can control the wait time for data locality by using the property that starts with spark.locality.wait and setting the value in time units. We can specify the wait time at each level using the following properties.


spark.locality.wait.process - This property is used to set the wait time for process locality. The default value is 3 seconds.


spark.locality.wait.node – This property is used to set the wait time for node locality. The default value is 3 seconds. 


spark.locality.wait.rack - This property is used to set the wait time for rack locality. The default value is 3 seconds.


We can simply set 0 to skip waiting and immediately move to next level for any of these properties above. Please adjust the time for these settings as per the requirement to increase the speed and efficieny of Spark application.
Use Broadcast Variables
We have covered Broadcast variables in Chapter 6. Broadcast variables are the best way to increase performance of a Spark job. Broadcast variables are the shared variables, which allow Spark to send large values efficiently in an immutable (read-only) state to all the worker nodes. These variables can be used one or more times during Spark operations. The broadcast variables are sent to the worker nodes only once and are then cached to the worker nodes’ memory in deserialized form. These variables are very useful when the Spark job consists of multiple stages and multiple tasks in those stages require the same variable. Broadcast Variables overcome the inefficiency of shipping the variables every time to executors.


It is recommended to use broadcast variables whenever possible to increase the overall performance of a Spark application.
Filter Data as soon as possible
To increase performance, filter data as soon as possible. Always make sure you have a filter operation before a operation which would cause in shuffling data over the network. For example, if you prefer to perform a join of two datasets and then filter the data, the shuffling of unnecessary data across the network is huge. But if you filter the data first and then perform the join, lesser data is shuffled across the network.


Spark has in-built performance optimization for such cases called predicate pushdown. Spark automatically pushes the filter conditions as first operation in cases where shuffling of data across network is involved.
Logs
Logs are the best way to get information on the performance of the Spark job. The logs are generated by driver and executor processes providing more insights on the job run. Logs have the information of errors and exceptions occurred during the job run and, are useful in troubleshooting or to apply performance optimization techniques.


The amount of information the logs generate can be controlled using the log4j properties file.
More Power
After trying most of these performance techniques you may still observe low performance. You may increase more power by adding additional nodes to the cluster.


These performance optimization techniques can help improve the performance of Spark applications if used wisely. These techniques might not always work in every scenarios. Please use your best judgement on applying these techniques. 


That’s all in theory for this chapter. Let us now proceed to our lab exercises.




























AIM


The aim of the following lab exercises is to configure Spark and learn performance optimization techniques.
The labs for this chapter include the following exercises.
* Spark Configuration File
* Using spark-submit tool
* Environment Variables File
* Logging Properties File
* Checking Log Files


We need the following packages to perform the lab exercises: 
* Java Development Kit
* Scala
* Spark
















LAB EXERCISE 10: SPARK CONFIGURATIONS & OPTIMIZATIONS
  

Log prop.


Checking Log Files


	  


1. Spark Configuration Files
2. Using spark-submit tool
3. Environment Variables File
4. Logging Properties File
5. Checking Log Files












  
  

Log prop.


Checking Log Files
Task 1: Spark Configuration File


Let is start this chapter by locating the Spark configuration file and modifying few of its properties.


Spark Configuration file is the file with all the default Spark configurations. These configurations can be overridden by passing configurations at runtime or programatically.


Step 1: The Spark configuartion file is found in the Spark installation directory inside the conf folder. Open the terminal in your machine and enter the following command to chane the working directory to the Spark conf directory in the terminal.


$ cd $SPARK_HOME/conf


You should see the full path of the directory as shown below. However, if you have not followed the lab exercises to install Spark from this book, you might have named the SPARK_HOME as something else. Please check Lab exercise 1 Step 4 for more information.


  



Step 2: Let us now check the contents of the conf directory using the following command.
  
  

Log prop.


Checking Log Files
$ ls


You should see the following files.


  



As you can see from the screenshot above, we have a file called spark-defaults.conf.template.


To be able to use the Spark configurations, we should either create a new file and name it spark-defaults.conf or rename the spark-defaults.conf.template to spark-defaults.conf.


Step 3: Run the following command from terminal to rename the spark-defaults.conf.template to spark-defaults.conf.


$cp spark-defaults.conf.template spark-defaults.conf


Next, run the ls command again and you should see the file spark-defaults.conf which we have renamed as shown in the screenshot below.


$ls








  
  

Log prop.


Checking Log Files


  



Step 4: Let us now check the contents of Spark configuration file by running the following command. Please note that you can use any text editor to view this configuration file. We are using the gedit text editor as shown below


$gedit spark-defaults.conf


You should now see the configuration file open in the text editor as shown below.


  

  
  

Log prop.


Checking Log Files


Scroll down a bit and you should see some example properties commented out with # at the start of the line as shown below.


  



Step 5: Let us now add some properties by removing # at the start of each property and also add new ones.  Remove the # for the first property which is spark.master. Then change its value from spark://master:7077 to local as shown in the screenshot below.


  

  
  

Log prop.


Checking Log Files
We have now set the default master for Spark as local in the Spark configuration file.


Next, let us add a new property to specify the Spark executor memory. To do this, go to the end of the configuration file and enter the following property and it’s value.


spark.executor.memory 4g


We have specified the memory for executors in Spark as 4 GB as shown in the screenshot below.


  



You can set the default onfiguration values for Spark by simply specifying the name of the property and its value separated by a white space. 


Step 6: Once you have specified the required default configurations, save the file and close it. These values will be effective for the next Spark job you run. Please remember that these values can either be overridden dynamically or programatically as explained in theory for this chapter.








  
  

Log prop.


Checking Log Files


Step 7: If you have followed the steps in this task and modified the configurations values, please delete the configurations file using the command below. Please make sure you delete the .conf file only and not the .template file.


$ rm spark-defaults.conf


  



Task 1 is complete!
Task 2: Using spark-submit Tool


So far we have been submitting jobs to Spark from the IDE by simply clicking the run button. This works well in the development phase but when it comes to later stages of software development cycles such as production, it is not possible to trigger Spark jobs using an IDE. We have to create a jar file of our application, move it to a node (usually the edge node) of the Spark cluster and then trigger the job using the spark-submit tool via CLI.


Let us now look how to trigger a job using spark-submit tool. You can also specify configuration properties here dynamically. Let us use one of the Spark programs we have written from the previous lab exercises. We shall be using avgRatings.scala program for this task.




  
  
  

Log prop.


Checking Log Files


Step 1: First, we need to make sure that there are no hard coded storage paths in our Spark program. This way we can supply the paths as arguments from the spark-submit tool. The paths are highlighted as shown below. 


  



You can specify them as arguments which can then be provided at runtime using the spark-submit tool. We shall look at this in detail in the next chapter. For now, we shall be using the program as is without changing any of the paths since we are running the Spark application in local mode only.




  
  
  

Log prop.


Checking Log Files


Step 2: Next we need to export the jar file so that we can run it using the spark-submit tool. We shall be using the Scala Build tool (SBT) to create a jar file which contains our programs and all the dependenies needed as one file. and From the IDE, click on Terminal as shown in the screenshot below.


  



You should now see a command promt as you would have in a traditional terminal at OS level.


Step 3: Navigate to the project’s folder with all the programs present using the command below.


$ cd IdeaProjects/Spark/src/main/scala/training


Now, either create a build.sbt file here in this location by adding all the necessary dependencies, as we did while installing Spark or simply copy the build.sbt file from IdeaProjects/Spark/ folder.


We are simply copying the file here for simplicity. However, you are free to create a new build file.
  
  
  

Log prop.


Checking Log Files


$ cp /home/{username}/IdeaProjects/Spark/build.sbt .


Please replace {username} with your username.


  



Step 4: Let us now use sbt to compile and then package a jar. To compile, run the following command.


$ sbt compile


Wait for few minutes for the compilation to finish. Once the compile is finished, you should see the success as shown below.


  



Now run the following command to generate a jar file.


  
  
  

Log prop.


Checking Log Files
$ sbt package


This will take some time to process and generate the jar file as required. Once it is finished, you should see a success message as shown in the screenshot below.


  



Step 5: Let us now verify if the jar has been successfully generated. You should now have two folders target and project. Inside the target folder, you will have a scala folder with its version and inside the scala folder there will be a jar file created as shown below.


$ ls target/scala*


  



Now that we have successfully generated the jar file. We can copy it to any location or  edge node (on Spark cluster) and run it via CLI using the spark-submit tool.


For ease and simplicity let us copy the jar file and the required file path which is hard coded in the program i.e., chapter_5/ratings.csv to the home folder.


  
  
  

Log prop.


Checking Log Files
$ cd


$ cp IdeaProjects/Spark/src/main/scala/training/target/scala*/spark* .


$ cp -r IdeaProjects/Spark/chapter_5 .


  



All the files we require for spark-submit are now available. We can now run the spark-submit tool.


Step 6: Open a terminal from your desktop. You may close the IDE at this point of time. Run the following command to run the spark-submit tool.


$ spark-submit --class training.avgRatings spark_2.12-0.1.jar


Wait for a while for the job to finish processing.


  



  
  
  

Log prop.


Checking Log Files
Once it is done, you should see the result on the console as shown below.


  



While running the spark-submit tool, it is mandatory to specify the class of the Spark application using the --class switch and then specifying the path to the jar file. We can also specify other properties with the spark-submit tool. For example, we can specify the master and other configuration properties as shown below.


 $ spark-submit \
--class training.avgRatings \
--master spark://85.67.45.54:7077 \
--deploy-mode cluster \
--executor-memory 50g
--num-executors 85
/path/to/spark_2.12-0.1.jar


We shall be looking at this again in the next chapter.


Task 2 is complete!
  
  
  
  

Log prop.


Checking Log Files
Task 3: Environment Variables File


The environment variables file in Spark is used to configure environmnt settings. The environment variables are the variables that are set outside of the Spark programs at the OS level. Let us now look at the environment variables file in Spark.


Step 1: Open the terminal and navigate to Spark’s conf directory. You should see the files as shown in the screenshot.


  



The file we are intersted at the moment is spark-env.sh.template.


Step 2: To be able to use the Spark environment variables file, we should either create a new file and name it spark-env.sh or rename the spark-env.sh.template to spark-env.sh.


$ cp spark-env.sh.template spark-env.sh


Next, use any of the text editors to open the file.


$ gedit spark-env.sh


You should see the file open as shown in the screenshot below.
  
  
  
  

Log prop.


Checking Log Files
  



Step 3: Scroll down the file and you will be able to see all the environment variables commented with #. You can uncomment by removing the # before the variables and set the values. The screenshot below shows few of the options you can set as environment variables.


  

  
  
  
  

Log prop.


Checking Log Files


Step 4: You may modify the settings by reading the description of what each setting does. After you are done, save the file and the settings will be ready for the next Spark job.


Please consider deleting the environmnt variables file after you have saved it as some settings might effect the framework adversely.


Task 3 is complete!
Task 4: Logging Properties File


Step 1: The log properties file is also found in the same conf directory of Spark installation. The file is called log4j.properties.template. As done previously, we need to rename the file to remove .template at the end. Please follow the steps from the previous exercises to achieve the same.


Step 2: Once you have the log4j.properties file, please use any of the text editors to view and edit the file. You should see the logging options as shown in the screenshot below.


  



  
  
  
  
  

Log prop.


Checking Log Files


Step 3: Let us set the log properties to save the logs to the disk. We can achieve this by adding the following properties to the log4j.properties file.


log4j.rootCategory=INFO,FILE
log4j.appender.FILE=org.apache.log4j.FileAppender
log4j.appender.FILE.File=IdeaProjects/Spark/logs log4j.appender.FILE.MaxFileSize=10MB
log4j.appender.FILE.MaxBackupIndex=10
log4j.appender.FILE.layout=org.apache.log4j.PatternLayout
log4j.appender.FILE.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L – %m%n


The log4j.properties file will look as shown below.


  



The above logging properties are used to enable logs to a file by using the appropriate classes. We specify the path where the log files have to be saved along with its size and the layout. 


Step 4: Save the file and these properties will be available when we run our next job.
  
  
  
  
  

Log prop.


Checking Log Files


As opposed to what we have been doing so far with the configuration files, please so not delete this file as we shall check the logs after running a Spark job in next task in this exercise.


Task 4 is complete!
Task 5: Checking Log Files


For this task, let us run a Spark App and check if the log settings we made in the previous task work.


Step 1: Let is use the spark-submit tool to run a Spark application. This time we shall run the ratingsByMovies.scala program. First, let us copy the required input files to our home folder using the following command.


$ cp -r IdeaProjects/Spark/chapter_6 .


Also make sure to copy the jar we generated in previous task to the home directory if not already available.


Step 2: Let us now trigger the Spark application using the spark-submit tool as shown below.


$ spark-submit --class training.ratingsByMovies spark_2.12-0.1.jar


The job should run and you should see the output as shown below.








  
  
  
  
  
  

Log prop.


Checking Log Files


  



Step 3: Now that we have successfully ran the Spark job, let us check the logs file we have configured in the logs4j.properties file. To do this change the current directory to Spark folder and open the logs file via any text editor.


$ cd gedit IdeaProjects/Spark/logs


You should see the logs persisted to a file as shown in the screenshot below.












  
  
  
  
  
  

Log prop.


Checking Log Files


  



This way we can check the logs to troubleshoot any errors or simply look for areas where we can increase in performance.


Task 5 is complete!




















SUMMARY


In this chapter we have looked at various Spark Configurations we can configure Spark. We have covered Spark configurations which include the Spark configuration properties, Environment Variables and Logging. We have then looked at various performance optimization techniques which can help increase the efficiency and speed of a Spark application
In the labs, we have had our hands on setting up various configuration files and also learned how to trigget a Spark job using the spark-submit tool.


















































REFERENCES


* https://spark.apache.org/
* http://spark.apache.org/docs/latest/configuration.html
* http://spark.apache.org/docs/latest/submitting-applications.html


























































Table of Contents
CHAPTER 1: INTRODUCTION TO APACHE SPARK        1
Theory        1
An Overview of BigData        1
Quick Introduction to Hadoop        1
Why Hadoop?        2
Quick Introduction to Hadoop Distributed File System        3
Block Placement in HDFS        4
HDFS Architecture        4
Introduction to MapReduce        6
Architecture of MapReduce        6
Processing Data with MapReduce        8
3V’s of Hadoop        12
Introduction to Spark        13
What is Spark?        13
Why Spark?        14
Components of Spark        16
Spark Data Storage        19
Various Spark Versions        19
LAB EXERCISE        20
SUMMARY        21
REFERENCES        22
CHAPTER 2: PROGRAMMING WITH SCALA        23
Theory        23
What is Scala?        23
Why Scala?        23
Data Types in Scala        25
Functions in Scala        26
Collections in Scala        27
Coding Scala        28
Conclusion        28
AIM        30
LAB EXERCISE 1: PROGRAMMING WITH SCALA        31
Task 1: Download and Install JDK        32
Task 2: Download and Install Scala        34
Task 3: Scala Basics        36
Task 4: Loops        44
Task 5: Functions        54
Task 6: Collections        59
LAB CHALLENGE        67
SUMMARY        68
REFERENCES        69
CHAPTER 3 : HANDS ON SPARK        70
Theory        70
Introduction to RDD        70
Architecture of Spark        71
Job Workflow in Spark        73
AIM        75
LAB EXERCISE 2: HANDS ON SPARK        76
Task 1: Download and Install Spark        77
Task 2: Installing Spark on Multi-Node Cluster        83
Task 3: Creating RDDs from Spark-Shell        87
Task 4: Basic RDD operations        90
Task 5: Download and Install IntelliJ IDEA        97
Task 6: Configuring Intellij IDEA        104
SUMMARY        111
REFERENCES        112
CHAPTER 4 : INTERNALS OF SPARK        113
Theory        113
Characteristics of RDD        113
RDD Operations        114
Lineage Graph        115
Directed Acyclic Graph        116
AIM        120
LAB EXERCISE 3: SPARK PROGRAM        121
Task 1: Creating a new package in IntelliJ IDEA        122
Task 2: Spark Program – Loading Data        126
Task 3: Spark Program – Performing Operations        132
Task 4: Spark Program – Saving Data        138
Task 5: Spark Program – Lineage Graph        140
Task 6: Spark Web Interface        142
SUMMARY        147
REFERENCES        148
CHAPTER 5 : RDD KEY-VALUE PAIRS & CACHING        149
Theory        149
Paired RDD        149
RDD Caching and Persistence        152
AIM        156
LAB EXERCISE 4: PAIRED RDD – HANDS ON        157
Task 1: Creating a Tuple        158
Task 2: Creating a Paired RDD        161
Task 3: Performing Operations on Paired RDD        164
Task 4: Performing more Operations on Paired RDD        171
Task 5: Performing Joins on Paired RDDs        177
Task 6: Performing Actions on Paired RDDs        181
LAB CHALLENGE        185
SUMMARY        186
REFERENCES        187
CHAPTER 6 : SHARED VARIABLES        188
Theory        188
What are Shared Variables?        188
Why Shared Variables?        188
Broadcast Variables        189
Accumulators        190
Scala Monadic Collections        191
AIM        193
LAB EXERCISE 5: SHARED VARIABLES – HANDS ON        194
Task 1: Using Accumulator method        195
Task 2: Implementing Record Parser        199
Task 3: Implementing Counters        203
Task 4: Implementing Accumulators V2        207
Task 5: Implementing Custom Accumulators V2        213
Task 6: Using Broadcast Variables        224
SUMMARY        230
REFERENCES        231
CHAPTER 7 : SPARK SQL        232
Theory        232
Types of Data        232
What is Spark SQL?        233
Why Spark SQL?        233
Spark SQL Architecture        234
AIM        236
LAB EXERCISE 6: SPARK SQL – HANDS ON        237
Task 1: Creating Data Frame using Data Source API        238
Task 2: Creating Data Frame from an RDD        245
Task 3: Creating Data Frame using StructType        251
Task 4: Querying data using Spark SQL        256
Task 5: Joins using Spark SQL        264
Task 6: Operations using DataFrame API        268
SUMMARY        273
REFERENCES        274
CHAPTER 8 : DATASETS        275
Theory        275
RDD vs DataFrame        275
What are Datasets?        276
Why Datasets?        276
AIM        278
LAB EXERCISE 7: DATASETS & FUNCTIONS        279
Task 1: Creating Dataset using Data Source API        280
Task 2: Creating Dataset from an RDD        286
Task 3: Aggregate and Collection Functions        292
Task 4: Date/Time Functions        304
Task 5: Math and String Functions        314
Task 6: Window Functions        326
SUMMARY        335
REFERENCES        336
CHAPTER 9 : USER DEFINED FUNCTIONS        337
Theory        337
Why User-Defined Functions?        337
Steps to implement User Defined function        337
UDAF types        338
Function currying in Scala        338
Partially applied functions in Scala        339
AIM        340
LAB EXERCISE 8: USER DEFINED FUNCTIONS        341
Task 1: Defining Currying Functions        342
Task 2: Using partially applied functions        344
Task 3: Writing User Defined Function        347
Task 4: Writing Untyped UDAF        354
Task 5: Using Untyped UDAF        361
Task 6: Typed UDAF        364
SUMMARY        372
REFERENCES        373
CHAPTER 10 : FILE FORMATS        374
Theory        374
DataSource API        374
RDD API        380
AIM        383
LAB EXERCISE 9: USING FILE FORMATS        384
Task 1: Text Files        385
Task 2: CSV Files        391
Task 3: JSON Files        394
Task 4: Parquet Files        396
Task 5: ORC Files        398
Task 6: Hadoop and Sequence Files        400
SUMMARY        407
REFERENCES        408
CHAPTER 11 : SPARK CONFIGURATIONS & OPTIMIZATIONS        409
Theory        409
Spark Configurations        409
Performance Optimization        412
AIM        419
LAB EXERCISE 10: SPARK CONFIGURATIONS & OPTIMIZATIONS        420
Task 1: Spark Configuration File        421
Task 2: Using spark-submit Tool        426
Task 3: Environment Variables File        433
Task 4: Logging Properties File        435
Task 5: Checking Log Files        437
SUMMARY        440
REFERENCES        441
Table of Contents        442














Apache Spark