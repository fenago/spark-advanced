{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run this code to make sure that the spark session, spark context and scala are working\n",
    "// you can install the scala/spark kernal following these directions:  https://www.datacamp.com/community/tutorials/beginners-guide-to-scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object hello\n",
    "{\n",
    "    def main(args: Array[String])\n",
    "    {\n",
    "        print(\"\\n\\n>>>>> START OF PROGRAM <<<<<\\n\\n\");\n",
    "        \n",
    "        println(\"Hello World.\")\n",
    "        \n",
    "        print(\"\\n\\n>>>>> END OF PROGRAM <<<<<\\n\\n\");\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//  Run this in the shell to get the data:  wget https://raw.githubusercontent.com/fenago/data/master/ErnestoSparkBook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 1: Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = sc.textFile(\"./ErnestoSparkBook.txt\") // read the file into the cluster\n",
    "lines.take(10).mkString(\"\\n\") // display first 10 lines in the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 2: Inspect the number of partitions (workers) used to store the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numPartitions = lines.partitions.length    // get the number of partitions\n",
    "println(s\"Number of partitions (workers) storing the dataset = $numPartitions\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 3: Split each line into a list of words separated by a space from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = lines.flatMap(x => x.split(' ')) // split each line into a list of words \n",
    "words.take(10).mkString(\"\\n\") // display the first 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 4: Filter the list of words to exclude common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stopWords = Seq(\"\",\"a\",\"*\",\"and\",\"is\",\"of\",\"the\",\"a\") // define the list of stop words \n",
    "val filteredWords = words.filter(x => !stopWords.contains(x.toLowerCase())) // filter the words \n",
    "filteredWords.take(10).mkString(\"\\n\") // display the first 10 filtered words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 5: Cache the filtered dataset in memory to speed up future actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredWords.cache() // cache filtered dataset into memory across the cluster worker nodes \n",
    "filteredWords.count() // materialize the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 6: Transform filtered words into list of (word,1) tuples for WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val word1Tuples = filteredWords.map(x => (x, 1)) // map the words into (word,1) tuples \n",
    "word1Tuples.take(10).mkString(\"\\n\") // display the (word,1) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 7: Aggregate the (word,1) tuples into (word,count) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordCountTuples = word1Tuples.reduceByKey{case (x, y) => x + y} // aggregate counts for each word \n",
    "wordCountTuples.take(10).mkString(\"\\n\") // display the first 10 (word,count) tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 8: Display the top 10 (word,count) tuples by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sortedWordCountTuples = wordCountTuples.top(10)(Ordering.by(tuple => tuple._2)).mkString(\"\\n\") // top 10 (word,count) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 9: Create a table from the (word,count) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class WordCount(word: String, count: Int) // create a case class to name the tuple elements \n",
    "\n",
    "val wordCountRows = wordCountTuples.map(x => WordCount(x._1,x._2)) // tuples -> WordCount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountRows.toDF.createOrReplaceTempView(\"word_count\") // convert RDDs to DataFrames and register a temp table for querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Step 10: Use SQL to visualize the words with count >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wc = spark.sql(\"select word, count from word_count\").show(true)\n",
    "//spark.sql(\"select word, count from word_count where count >=2\").show(true)\n",
    "//spark.sql(\"select word, count from word_count where count >=2 ORDER BY count DESC --use SQL to query words with count >= 2 in descending order\").show(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
